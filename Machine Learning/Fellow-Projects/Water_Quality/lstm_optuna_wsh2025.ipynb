{"cells":[{"cell_type":"markdown","id":"9ccae883-1904-48a6-81b3-d0228caeab1f","metadata":{"id":"9ccae883-1904-48a6-81b3-d0228caeab1f"},"source":["### Multivariate time series prediction using MLP with hyperparameter optimization\n","\n","In this notebook, we use **Optuna** to find the optimum values of hyperparameters. Optuna is a package for optimizing hyperparameters. In this notebook we specifically optimize the values of **learning rate** and **weight decay**.\n","\n","Optuna is a python package specifially designed for hyperparameter tuning. We need to define a range of possible values for each of the hyperparameters. And optuna will try different parameter values with the model to minimize the validation loss after for specified number of experiments.\n"]},{"cell_type":"code","execution_count":16,"id":"4117e9d0-7925-4788-bbe1-28c60c80bb56","metadata":{"id":"4117e9d0-7925-4788-bbe1-28c60c80bb56","outputId":"0039405e-e038-4cfc-c034-91d8a7891d8e","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1754593626158,"user_tz":300,"elapsed":6303,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.4.0)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.4)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.42)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n"]}],"source":["import torch\n","import numpy as np\n","\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","!pip install optuna\n","import optuna"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yP5yamUpdFt0","executionInfo":{"status":"ok","timestamp":1754593628410,"user_tz":300,"elapsed":2240,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}},"outputId":"bad66a66-f504-4a98-a3a9-0efed282bc04"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"id":"yP5yamUpdFt0"},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/wsh_2025/Project Codes/input datafiles/')"],"metadata":{"id":"ejI65QqBd-0l","executionInfo":{"status":"ok","timestamp":1754593628427,"user_tz":300,"elapsed":13,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}}},"execution_count":18,"outputs":[],"id":"ejI65QqBd-0l"},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'device: {device}')\n","\n","# set seed\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","if torch.cuda.is_available():\n","  torch.cuda.manual_seed(42)\n","  torch.cuda.manual_seed_all(42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I6IugcS4eURT","executionInfo":{"status":"ok","timestamp":1754593628461,"user_tz":300,"elapsed":29,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}},"outputId":"2dcadbac-78e0-479d-bc24-49893931cdda"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["device: cpu\n"]}],"id":"I6IugcS4eURT"},{"cell_type":"code","source":["input_file = '11447650_filled_daily_data.csv'"],"metadata":{"id":"Z6BOmdq-d-U9","executionInfo":{"status":"ok","timestamp":1754593628487,"user_tz":300,"elapsed":7,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}}},"execution_count":20,"outputs":[],"id":"Z6BOmdq-d-U9"},{"cell_type":"code","source":["class RiverData(torch.utils.data.Dataset):\n","\n","  # first function is the __init__() function, which loads data from input file\n","  def __init__(self, df, target, datecol, seq_len, pred_len):\n","    self.df = df\n","    self.datecol = datecol\n","    self.target = target\n","    self.seq_len = seq_len # length of single sequence of input data\n","    self.pred_len = pred_len # length of forward-forecasting sequence\n","\n","  # second function sets the index to the date column\n","  def setIndex(self):\n","    self.df.set_index(self.datecol, inplace = True)\n","\n","  # third function defines the length of the input dataset, minus the\n","  # training sequence length minus the prediction length\n","  def __len__(self):\n","    return len(self.df) - self.seq_len - self.pred_len\n","\n","  # fourth function returns a typle of a feature and a label, which is used for\n","  # model training. for time series, the feature is the past values for\n","  # training and the label are the future values to be predicted\n","  def __getitem__(self, idx):\n","\n","    # raises warning if not enough data at index idx\n","    if len(self.df) <= (idx + self.seq_len + self.pred_len):\n","      raise IndexError(\n","          f'Index {idx} out of bounds for dataset size {len(self.df)}')\n","\n","    # pulls values for training sequence and assigns to df_piece\n","    df_piece = self.df[idx:idx + self.seq_len].values\n","\n","    # converts df_piece into a tensor data type\n","    feature = torch.tensor(df_piece, dtype = torch.float32)\n","\n","    # pulls the target data\n","    label_piece = self.df[self.target][\n","        idx + self.seq_len:\n","        idx + self.seq_len + self.pred_len].values\n","\n","    # converts label_piece to tensor data type\n","    label = torch.tensor(label_piece, dtype = torch.float32)\n","\n","    return (feature, label)"],"metadata":{"id":"BmWtWuxZfYNB","executionInfo":{"status":"ok","timestamp":1754593628528,"user_tz":300,"elapsed":37,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}}},"execution_count":21,"outputs":[],"id":"BmWtWuxZfYNB"},{"cell_type":"markdown","id":"60c24959-e72f-435d-8a14-69a648fa5ab1","metadata":{"id":"60c24959-e72f-435d-8a14-69a648fa5ab1"},"source":["Normalizing data:"]},{"cell_type":"code","source":["df = pd.read_csv(input_file)\n","df.drop('SSD_st_d', axis = 1, inplace = True) # drop ssd\n","df.drop('Precip_in', axis = 1, inplace = True) # drop wrong precip\n","\n","raw_df = df.drop('DATE', axis = 1, inplace = False)\n","scaler = MinMaxScaler()\n","\n","# apply transformations and make new df\n","df_scaled = scaler.fit_transform(raw_df)\n","\n","df_scaled = pd.DataFrame(df_scaled, columns = raw_df.columns)\n","df_scaled['DATE'] = df['DATE']\n","\n","df = df_scaled"],"metadata":{"id":"mhXNpbldllji","executionInfo":{"status":"ok","timestamp":1754593628676,"user_tz":300,"elapsed":131,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}}},"execution_count":22,"outputs":[],"id":"mhXNpbldllji"},{"cell_type":"code","source":["train_size = int(0.7 * len(df))\n","test_size = int(0.2 * len(df))\n","val_size = len(df) - train_size - test_size\n","\n","seq_len = 13\n","pred_len = 1\n","num_features = 3\n","\n","# remember: single asterix unpacks list\n","common_args = ['SSC_mg_L', 'DATE', seq_len, pred_len]\n","\n","# pulled from beginning of df to train_size\n","train_dataset = RiverData(df[:train_size], *common_args)\n","train_dataset.setIndex()\n","\n","# pulled from the end of train_size to the end of val_size\n","val_dataset = RiverData(df[train_size: train_size + val_size], *common_args)\n","val_dataset.setIndex()\n","\n","# pulled from the end of train + val to the end of the df\n","test_dataset = RiverData(df[train_size + val_size: len(df)], *common_args)\n","test_dataset.setIndex()"],"metadata":{"id":"qXmL3DPTmRKf","executionInfo":{"status":"ok","timestamp":1754593628678,"user_tz":300,"elapsed":17,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}}},"execution_count":23,"outputs":[],"id":"qXmL3DPTmRKf"},{"cell_type":"code","source":["# number of training examples used in one iteration to update model params\n","BATCH_SIZE = 492\n","\n","SHUFFLE = False # order matters for time series\n","DATA_LOAD_WORKERS = 1\n","\n","# learning rate determines step size at which params are updated while training\n","#learning_rate = 0.001\n","\n","# weight decay adds penalty to loss function based on magnitude of model\n","# weights, preventing overreliance on single parameter\n","#weight_decay = 0.0001"],"metadata":{"id":"Au5BwhIQoziW","executionInfo":{"status":"ok","timestamp":1754593628680,"user_tz":300,"elapsed":12,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}}},"execution_count":24,"outputs":[],"id":"Au5BwhIQoziW"},{"cell_type":"code","execution_count":25,"id":"ae359aec-9435-4229-8587-5f120b0370b3","metadata":{"id":"ae359aec-9435-4229-8587-5f120b0370b3","executionInfo":{"status":"ok","timestamp":1754593628703,"user_tz":300,"elapsed":3,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}}},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","# remember: double asterix unpacks dictionary\n","common_args = {'batch_size': BATCH_SIZE, 'shuffle': SHUFFLE}\n","train_loader = DataLoader(train_dataset, **common_args)\n","val_loader = DataLoader(val_dataset, **common_args)\n","test_loader = DataLoader(test_dataset, **common_args)"]},{"cell_type":"markdown","id":"bc7ac35c-8ecc-4ab7-a97c-0cb4d59bc624","metadata":{"id":"bc7ac35c-8ecc-4ab7-a97c-0cb4d59bc624"},"source":["Defining PyTorch LSTM model:"]},{"cell_type":"code","source":["class BasicLSTMNetwork(torch.nn.Module):\n","  # __init__() function sets up layers and defines model params\n","  def __init__(self, seq_len, pred_len):\n","    # call base class constructor\n","    super().__init__()\n","    self.seq_len = seq_len\n","    self.pred_len = pred_len\n","    self.num_features = num_features\n","    self.n_layers = 1\n","\n","    # define size of hidden state\n","    self.n_hidden = 128\n","\n","    # define layers for combining across time series\n","    self.lstm1 = torch.nn.LSTM(input_size = self.num_features,\n","                               hidden_size = self.n_hidden,\n","                               num_layers = self.n_layers,\n","                               batch_first = True)\n","    self.relu = torch.nn.ReLU()\n","    self.fc1 = torch.nn.Linear(self.n_hidden * self.seq_len, self.pred_len)\n","\n","  def init_hidden(self, batchsize):\n","    device = next(self.parameters()).device\n","    hidden_state = torch.zeros(self.n_layers,\n","                               batchsize,\n","                               self.n_hidden,\n","                               device = device)\n","    cell_state = torch.zeros(self.n_layers,\n","                             batchsize,\n","                             self.n_hidden,\n","                             device = device)\n","    return hidden_state, cell_state\n","\n","  # forward() function defines how forward pass computation operates\n","  # gradients are stored inside FC layer objects\n","  # each training example needs the old gradient erased\n","  def forward(self, x):\n","    batchsize, seqlen, featlen = x.size()\n","    self.hidden_states = self.init_hidden(batchsize)\n","    lstm_out, self.hidden_states = self.lstm1(x, self.hidden_states)\n","    lstm_out = lstm_out.contiguous().view(batchsize, -1)\n","    lstm_out = self.relu(lstm_out)\n","    lstm_out = self.fc1(lstm_out)\n","    return lstm_out"],"metadata":{"id":"vH85CcfGshaN","executionInfo":{"status":"ok","timestamp":1754593628716,"user_tz":300,"elapsed":14,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}}},"execution_count":26,"outputs":[],"id":"vH85CcfGshaN"},{"cell_type":"code","execution_count":27,"id":"5277794b-fe37-4595-8554-d26db5710e44","metadata":{"id":"5277794b-fe37-4595-8554-d26db5710e44","executionInfo":{"status":"ok","timestamp":1754593628719,"user_tz":300,"elapsed":2,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}}},"outputs":[],"source":["loss = torch.nn.MSELoss()"]},{"cell_type":"code","source":["# model param check 2\n","for i, (f, l) in enumerate(train_loader):\n","  print('features shape: ', f.shape)\n","  print('labels shape: ', l.shape)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LiUcsCy_w7pH","executionInfo":{"status":"ok","timestamp":1754593628850,"user_tz":300,"elapsed":123,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}},"outputId":"6445c443-5eb6-406f-92bb-7817594e81bf"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["features shape:  torch.Size([492, 13, 3])\n","labels shape:  torch.Size([492, 1])\n"]}],"id":"LiUcsCy_w7pH"},{"cell_type":"code","source":["# define metrics\n","import numpy as np\n","import matplotlib.pyplot as plt\n","epsilon = np.finfo(float).eps\n","\n","def wape_function(y, y_pred):\n","    # Weighted Average Percentage Error metric in the interval [0; 100]\n","    y = np.array(y)\n","    y_pred = np.array(y_pred)\n","    nominator = np.sum(np.abs(np.subtract(y, y_pred)))\n","    denominator = np.add(np.sum(np.abs(y)), epsilon)\n","    wape = np.divide(nominator, denominator) * 100.0\n","    return wape\n","\n","def nse_function(y, y_pred):\n","    y = np.array(y)\n","    y_pred = np.array(y_pred)\n","    return (1-(np.sum((y_pred-y)**2)/np.sum((y-np.mean(y))**2)))\n","\n","\n","def evaluate_model(model, data_loader, plot=False):\n","    # following line disables dropout and batch normalization if they\n","    # are part of the model.\n","\n","    model.eval()\n","    all_inputs = torch.empty((0, seq_len, num_features))\n","    all_labels = torch.empty(0, pred_len)\n","    for inputs, labels in data_loader:\n","        all_inputs = torch.vstack((all_inputs, inputs))\n","        all_labels = torch.vstack((all_labels, labels))\n","\n","    with torch.no_grad():\n","        all_inputs = all_inputs.to(device)\n","        outputs = model(all_inputs).detach().cpu()\n","        avg_val_loss = loss(outputs, all_labels)\n","        nse = nse_function(all_labels.numpy(), outputs.numpy())\n","        wape = wape_function(all_labels.numpy(), outputs.numpy())\n","\n","    print(f'NSE : {nse}', end=' ')\n","    print(f'WAPE : {wape}', end=' ')\n","    print(f'Validation Loss: {avg_val_loss}')\n","    model.train()\n","\n","    if plot is True:\n","        plt.figure(figsize=(16, 6))\n","        plt.plot(np.array(all_labels.cpu()[:400]), color='green', label='observations')\n","        plt.plot(np.array(outputs.cpu()[:400]), color='red', linestyle='-', label='predictions')\n","        metrics_text = f\"NSE: {nse:.3f}\\nWAPE: {wape:.3f}\"\n","        plt.text(0.28, 0.97, metrics_text, transform=plt.gca().transAxes,\n","        fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n","        plt.legend()\n","        plt.show()\n","\n","    return avg_val_loss\n","\n"],"metadata":{"id":"cQbta7DKxPg5","executionInfo":{"status":"ok","timestamp":1754593628905,"user_tz":300,"elapsed":61,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}}},"execution_count":29,"outputs":[],"id":"cQbta7DKxPg5"},{"cell_type":"code","execution_count":30,"id":"9beaf3a1-2c40-4fc6-b0f7-c352b5763231","metadata":{"id":"9beaf3a1-2c40-4fc6-b0f7-c352b5763231","outputId":"8740958a-c10b-4339-c880-f35309a8b441","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1754596710238,"user_tz":300,"elapsed":3081381,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:07:08,372] A new study created in memory with name: no-name-db8c71d1-eee5-4c75-828c-5cbe829e8850\n","/tmp/ipython-input-420348275.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n","/tmp/ipython-input-420348275.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n","  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006363313823385397 NSE : -5.6298651695251465 WAPE : 155.39746930437138 Validation Loss: 0.001434034900739789\n","Epoch 2: Training Loss: 0.005044633580837399 NSE : -11.320901870727539 WAPE : 215.37152029351066 Validation Loss: 0.002665001666173339\n","Epoch 3: Training Loss: 0.004715505107014906 NSE : -4.200301170349121 WAPE : 137.51651622855474 Validation Loss: 0.0011248211376369\n","Epoch 4: Training Loss: 0.004477015201700851 NSE : -2.203913688659668 WAPE : 106.47596256071377 Validation Loss: 0.0006930041126906872\n","Epoch 5: Training Loss: 0.004328573337261332 NSE : -3.4340620040893555 WAPE : 127.41472139417564 Validation Loss: 0.0009590843692421913\n","Epoch 6: Training Loss: 0.0041492801829008386 NSE : -3.1787524223327637 WAPE : 123.84272998269788 Validation Loss: 0.0009038609568960965\n","Epoch 7: Training Loss: 0.00399313164234627 NSE : -1.6568248271942139 WAPE : 97.31419399220363 Validation Loss: 0.000574669218622148\n","Epoch 8: Training Loss: 0.003911273626727052 NSE : -1.6347813606262207 WAPE : 97.12785641324967 Validation Loss: 0.0005699011962860823\n","Epoch 9: Training Loss: 0.00381788892991608 NSE : -1.3778681755065918 WAPE : 91.98988555585666 Validation Loss: 0.0005143310991115868\n","Epoch 10: Training Loss: 0.003744984740478685 NSE : -0.9260300397872925 WAPE : 81.89440286839967 Validation Loss: 0.00041659886483103037\n","Epoch 11: Training Loss: 0.00369557488738792 NSE : -0.9407720565795898 WAPE : 82.35130391278064 Validation Loss: 0.0004197874804958701\n","Epoch 12: Training Loss: 0.003630651677667629 NSE : -0.7801766395568848 WAPE : 78.5071063767693 Validation Loss: 0.0003850508073810488\n","Epoch 13: Training Loss: 0.003576285031158477 NSE : -0.7224197387695312 WAPE : 77.14795188759875 Validation Loss: 0.0003725580754689872\n","Epoch 14: Training Loss: 0.0035184539592592046 NSE : -0.6953228712081909 WAPE : 76.56499134893998 Validation Loss: 0.0003666970005724579\n","Epoch 15: Training Loss: 0.0034593883938214276 NSE : -0.6117701530456543 WAPE : 74.47070938692126 Validation Loss: 0.0003486246569082141\n","Epoch 16: Training Loss: 0.00340141793640214 NSE : -0.5634993314743042 WAPE : 73.27314419128224 Validation Loss: 0.00033818374504335225\n","Epoch 17: Training Loss: 0.0033395515783922747 NSE : -0.47405970096588135 WAPE : 70.91071689145525 Validation Loss: 0.0003188379923813045\n","Epoch 18: Training Loss: 0.003278246094851056 NSE : -0.4158303737640381 WAPE : 69.40970169477393 Validation Loss: 0.0003062430187128484\n","Epoch 19: Training Loss: 0.0032195785352087114 NSE : -0.33870363235473633 WAPE : 67.26640678743408 Validation Loss: 0.0002895606157835573\n","Epoch 20: Training Loss: 0.0031655134989705402 NSE : -0.3230435848236084 WAPE : 66.93981780659148 Validation Loss: 0.00028617330826818943\n","Epoch 21: Training Loss: 0.0031076193517947104 NSE : -0.26097428798675537 WAPE : 65.22989306039065 Validation Loss: 0.00027274782769382\n","Epoch 22: Training Loss: 0.0030529031828336883 NSE : -0.22596526145935059 WAPE : 64.30113193387672 Validation Loss: 0.00026517538935877383\n","Epoch 23: Training Loss: 0.003001181143190479 NSE : -0.18438851833343506 WAPE : 63.12507973567364 Validation Loss: 0.00025618227664381266\n","Epoch 24: Training Loss: 0.002954328931082273 NSE : -0.14802491664886475 WAPE : 62.08959579746096 Validation Loss: 0.00024831690825521946\n","Epoch 25: Training Loss: 0.0029108565431670286 NSE : -0.10579276084899902 WAPE : 60.83136061370411 Validation Loss: 0.00023918214719742537\n","Epoch 26: Training Loss: 0.002870560410883627 NSE : -0.06742334365844727 WAPE : 59.66435554814367 Validation Loss: 0.00023088284069672227\n","Epoch 27: Training Loss: 0.002833172760801972 NSE : -0.03417491912841797 WAPE : 58.6394446644848 Validation Loss: 0.00022369128419086337\n","Epoch 28: Training Loss: 0.0027982580086245434 NSE : -0.004975438117980957 WAPE : 57.72355798294802 Validation Loss: 0.00021737543283961713\n","Epoch 29: Training Loss: 0.0027657228092721198 NSE : 0.02176344394683838 WAPE : 56.870392528819494 Validation Loss: 0.0002115918177878484\n","Epoch 30: Training Loss: 0.0027352068609616254 NSE : 0.04764610528945923 WAPE : 56.02947614183569 Validation Loss: 0.00020599343406502157\n","Epoch 31: Training Loss: 0.00270658335830376 NSE : 0.07575148344039917 WAPE : 55.089616643388716 Validation Loss: 0.0001999142550630495\n","Epoch 32: Training Loss: 0.0026788439790834673 NSE : 0.10497176647186279 WAPE : 54.09155531466928 Validation Loss: 0.00019359395082574338\n","Epoch 33: Training Loss: 0.002652013114129659 NSE : 0.12406158447265625 WAPE : 53.46036146838715 Validation Loss: 0.00018946481577586383\n","Epoch 34: Training Loss: 0.0026262731535098283 NSE : 0.13559991121292114 WAPE : 53.11137145358654 Validation Loss: 0.00018696908955462277\n","Epoch 35: Training Loss: 0.002601518492156174 NSE : 0.14682042598724365 WAPE : 52.76653811677888 Validation Loss: 0.00018454210658092052\n","Epoch 36: Training Loss: 0.0025778499821171863 NSE : 0.16104203462600708 WAPE : 52.29540347293157 Validation Loss: 0.00018146597722079605\n","Epoch 37: Training Loss: 0.0025551896960678278 NSE : 0.17711901664733887 WAPE : 51.73920910550124 Validation Loss: 0.00017798852059058845\n","Epoch 38: Training Loss: 0.002533449172915425 NSE : 0.1922469139099121 WAPE : 51.21004773717455 Validation Loss: 0.0001747163914842531\n","Epoch 39: Training Loss: 0.002512412022042554 NSE : 0.20478254556655884 WAPE : 50.775339267474095 Validation Loss: 0.00017200494767166674\n","Epoch 40: Training Loss: 0.002491843897587387 NSE : 0.21602267026901245 WAPE : 50.385374497091995 Validation Loss: 0.00016957368643488735\n","Epoch 41: Training Loss: 0.0024716532388993073 NSE : 0.22600197792053223 WAPE : 50.03862333493152 Validation Loss: 0.0001674152008490637\n","Epoch 42: Training Loss: 0.002451648661008221 NSE : 0.2340400218963623 WAPE : 49.764124158345666 Validation Loss: 0.00016567655256949365\n","Epoch 43: Training Loss: 0.0024318838823091937 NSE : 0.24649423360824585 WAPE : 49.30605991119635 Validation Loss: 0.0001629827165743336\n","Epoch 44: Training Loss: 0.0024127615415636683 NSE : 0.2597941756248474 WAPE : 48.81107335681975 Validation Loss: 0.00016010594845283777\n","Epoch 45: Training Loss: 0.002394214017840568 NSE : 0.269392728805542 WAPE : 48.48071543224031 Validation Loss: 0.00015802981215529144\n","Epoch 46: Training Loss: 0.0023759280702506658 NSE : 0.2830269932746887 WAPE : 47.96190198244773 Validation Loss: 0.0001550807210151106\n","Epoch 47: Training Loss: 0.002358330539209419 NSE : 0.29190170764923096 WAPE : 47.628442183819395 Validation Loss: 0.00015316113422159106\n","Epoch 48: Training Loss: 0.002340945055038901 NSE : 0.3001493215560913 WAPE : 47.320122574055155 Validation Loss: 0.0001513771858299151\n","Epoch 49: Training Loss: 0.002323655904547195 NSE : 0.30967986583709717 WAPE : 46.957307539972064 Validation Loss: 0.00014931571786291897\n","Epoch 50: Training Loss: 0.002306703790964093 NSE : 0.31870025396347046 WAPE : 46.61290362927602 Validation Loss: 0.00014736462617293\n","Epoch 51: Training Loss: 0.0022900481326360023 NSE : 0.3280264735221863 WAPE : 46.251009985199396 Validation Loss: 0.00014534738147631288\n","Epoch 52: Training Loss: 0.002273605767186382 NSE : 0.337546169757843 WAPE : 45.8761334973213 Validation Loss: 0.00014328827091958374\n","Epoch 53: Training Loss: 0.0022574243557755835 NSE : 0.3463894724845886 WAPE : 45.52849429863876 Validation Loss: 0.0001413754653185606\n","Epoch 54: Training Loss: 0.0022414341638068436 NSE : 0.35487306118011475 WAPE : 45.19534718892665 Validation Loss: 0.00013954048336017877\n","Epoch 55: Training Loss: 0.0022256036445469363 NSE : 0.36335456371307373 WAPE : 44.85901065226908 Validation Loss: 0.00013770592340733856\n","Epoch 56: Training Loss: 0.002209973597928183 NSE : 0.3711703419685364 WAPE : 44.55501031873423 Validation Loss: 0.00013601539831142873\n","Epoch 57: Training Loss: 0.0021944718473605462 NSE : 0.37982505559921265 WAPE : 44.20933897563111 Validation Loss: 0.00013414336717687547\n","Epoch 58: Training Loss: 0.0021791322715216666 NSE : 0.3876562714576721 WAPE : 43.89695440995601 Validation Loss: 0.00013244946603663266\n","Epoch 59: Training Loss: 0.0021638431908286293 NSE : 0.39451855421066284 WAPE : 43.62108774050989 Validation Loss: 0.00013096518523525447\n","Epoch 60: Training Loss: 0.002148779001799994 NSE : 0.3991844654083252 WAPE : 43.448933730795694 Validation Loss: 0.00012995593715459108\n","Epoch 61: Training Loss: 0.0021337636944736005 NSE : 0.4070141911506653 WAPE : 43.124112484626124 Validation Loss: 0.00012826237070839852\n","Epoch 62: Training Loss: 0.0021190660327192745 NSE : 0.4144384264945984 WAPE : 42.81459214942361 Validation Loss: 0.00012665653775911778\n","Epoch 63: Training Loss: 0.0021046490119260852 NSE : 0.4213104844093323 WAPE : 42.530195326343 Validation Loss: 0.0001251701032742858\n","Epoch 64: Training Loss: 0.002090348682941112 NSE : 0.4290499687194824 WAPE : 42.20135498530362 Validation Loss: 0.00012349605094641447\n","Epoch 65: Training Loss: 0.0020763074353453703 NSE : 0.4354356527328491 WAPE : 41.937262095849576 Validation Loss: 0.00012211485591251403\n","Epoch 66: Training Loss: 0.0020624838725780137 NSE : 0.44144439697265625 WAPE : 41.68881199057764 Validation Loss: 0.00012081515160389245\n","Epoch 67: Training Loss: 0.0020487311985561973 NSE : 0.44848787784576416 WAPE : 41.386537699860334 Validation Loss: 0.00011929166066693142\n","Epoch 68: Training Loss: 0.0020352442461444298 NSE : 0.45434272289276123 WAPE : 41.139488440933064 Validation Loss: 0.00011802525841630995\n","Epoch 69: Training Loss: 0.0020219384732627077 NSE : 0.46002137660980225 WAPE : 40.90130287048425 Validation Loss: 0.00011679697490762919\n","Epoch 70: Training Loss: 0.0020087127113583847 NSE : 0.46693140268325806 WAPE : 40.59927873089992 Validation Loss: 0.0001153023331426084\n","Epoch 71: Training Loss: 0.0019956912628913415 NSE : 0.472989022731781 WAPE : 40.33766233766234 Validation Loss: 0.00011399207141948864\n","Epoch 72: Training Loss: 0.0019828386894005234 NSE : 0.47870898246765137 WAPE : 40.08924141668925 Validation Loss: 0.00011275486031081527\n","Epoch 73: Training Loss: 0.0019701319879459334 NSE : 0.48387813568115234 WAPE : 39.866598569969355 Validation Loss: 0.00011163676390424371\n","Epoch 74: Training Loss: 0.00195757405981567 NSE : 0.4892086982727051 WAPE : 39.63551312251152 Validation Loss: 0.00011048375745303929\n","Epoch 75: Training Loss: 0.0019451130510788062 NSE : 0.49571388959884644 WAPE : 39.34035980071293 Validation Loss: 0.00010907671094173566\n","Epoch 76: Training Loss: 0.0019328963053339976 NSE : 0.5008503198623657 WAPE : 39.11429405265681 Validation Loss: 0.00010796569404192269\n","Epoch 77: Training Loss: 0.0019207978803024162 NSE : 0.5058894753456116 WAPE : 38.89125513330971 Validation Loss: 0.00010687573376344517\n","Epoch 78: Training Loss: 0.001908805628772825 NSE : 0.5117202997207642 WAPE : 38.62179233286778 Validation Loss: 0.00010561454837443307\n","Epoch 79: Training Loss: 0.0018970993514813017 NSE : 0.5155468583106995 WAPE : 38.457251255967144 Validation Loss: 0.00010478684271220118\n","Epoch 80: Training Loss: 0.001885460094854352 NSE : 0.5201886892318726 WAPE : 38.249329803422896 Validation Loss: 0.00010378282604506239\n","Epoch 81: Training Loss: 0.0018738903463599854 NSE : 0.5256448984146118 WAPE : 37.99579329178045 Validation Loss: 0.00010260265116812661\n","Epoch 82: Training Loss: 0.0018625576949489187 NSE : 0.5298929214477539 WAPE : 37.805841028954994 Validation Loss: 0.00010168382141273469\n","Epoch 83: Training Loss: 0.0018512631413614145 NSE : 0.5348504781723022 WAPE : 37.575893769152195 Validation Loss: 0.00010061150533147156\n","Epoch 84: Training Loss: 0.001840133176301606 NSE : 0.5387907028198242 WAPE : 37.39957891225949 Validation Loss: 9.975922148441896e-05\n","Epoch 85: Training Loss: 0.0018290544030605815 NSE : 0.5442798137664795 WAPE : 37.13709532842759 Validation Loss: 9.857193072093651e-05\n","Epoch 86: Training Loss: 0.0018181901750722318 NSE : 0.5492235422134399 WAPE : 36.90100268912468 Validation Loss: 9.750261233421043e-05\n","Epoch 87: Training Loss: 0.001807545767405827 NSE : 0.5531938076019287 WAPE : 36.716599612265746 Validation Loss: 9.664383833296597e-05\n","Epoch 88: Training Loss: 0.0017970331264223205 NSE : 0.5571657419204712 WAPE : 36.53002855892101 Validation Loss: 9.578472236171365e-05\n","Epoch 89: Training Loss: 0.0017866264133772347 NSE : 0.5612351298332214 WAPE : 36.33648662733735 Validation Loss: 9.490452066529542e-05\n","Epoch 90: Training Loss: 0.0017763763789844234 NSE : 0.5655475854873657 WAPE : 36.12696837672761 Validation Loss: 9.397172834724188e-05\n","Epoch 91: Training Loss: 0.0017662845084487344 NSE : 0.5697454214096069 WAPE : 35.92213629067562 Validation Loss: 9.306373976869509e-05\n","Epoch 92: Training Loss: 0.0017564149065947277 NSE : 0.5729001760482788 WAPE : 35.7737257926664 Validation Loss: 9.238137863576412e-05\n","Epoch 93: Training Loss: 0.0017466927356508677 NSE : 0.5755863189697266 WAPE : 35.6508682328907 Validation Loss: 9.180034248856828e-05\n","Epoch 94: Training Loss: 0.0017369823954140884 NSE : 0.579947829246521 WAPE : 35.42982218423631 Validation Loss: 9.085698548005894e-05\n","Epoch 95: Training Loss: 0.0017275195214097039 NSE : 0.583369255065918 WAPE : 35.2609118008797 Validation Loss: 9.011690417537466e-05\n","Epoch 96: Training Loss: 0.001718102018458012 NSE : 0.5877043008804321 WAPE : 35.03790623501699 Validation Loss: 8.917926606955007e-05\n","Epoch 97: Training Loss: 0.0017089169423343264 NSE : 0.5901448726654053 WAPE : 34.92392278668362 Validation Loss: 8.86513662408106e-05\n","Epoch 98: Training Loss: 0.001699785479104321 NSE : 0.593552827835083 WAPE : 34.75215859581831 Validation Loss: 8.791421714704484e-05\n","Epoch 99: Training Loss: 0.0016907809522308526 NSE : 0.5968540906906128 WAPE : 34.58540993516917 Validation Loss: 8.720015466678888e-05\n","Epoch 100: Training Loss: 0.0016818332514958456 NSE : 0.6010723114013672 WAPE : 34.362898417794085 Validation Loss: 8.628774958197027e-05\n","Epoch 101: Training Loss: 0.001673211547313258 NSE : 0.6032617688179016 WAPE : 34.25834983635947 Validation Loss: 8.581417932873592e-05\n","Epoch 102: Training Loss: 0.0016645818159304326 NSE : 0.6063061356544495 WAPE : 34.10429634570887 Validation Loss: 8.515568333677948e-05\n","Epoch 103: Training Loss: 0.0016560036729060812 NSE : 0.6100705862045288 WAPE : 33.90612870275792 Validation Loss: 8.434143819613382e-05\n","Epoch 104: Training Loss: 0.0016476282671646914 NSE : 0.6129816770553589 WAPE : 33.75636321944508 Validation Loss: 8.371176954824477e-05\n","Epoch 105: Training Loss: 0.0016394023832617677 NSE : 0.6150493621826172 WAPE : 33.65651122553209 Validation Loss: 8.326452370965853e-05\n","Epoch 106: Training Loss: 0.0016311537265210063 NSE : 0.6184068918228149 WAPE : 33.4790456734277 Validation Loss: 8.253831038018689e-05\n","Epoch 107: Training Loss: 0.0016230595820161398 NSE : 0.6214983463287354 WAPE : 33.31618686289634 Validation Loss: 8.186961349565536e-05\n","Epoch 108: Training Loss: 0.0016151265808730386 NSE : 0.6241734027862549 WAPE : 33.17692147339017 Validation Loss: 8.129100751830265e-05\n","Epoch 109: Training Loss: 0.0016073039587354288 NSE : 0.6262498497962952 WAPE : 33.07439911613266 Validation Loss: 8.084187720669433e-05\n","Epoch 110: Training Loss: 0.0015994666373444488 NSE : 0.6297731995582581 WAPE : 32.88258322736654 Validation Loss: 8.007977885426953e-05\n","Epoch 111: Training Loss: 0.0015918377021080232 NSE : 0.6320856809616089 WAPE : 32.763294490421295 Validation Loss: 7.95795931480825e-05\n","Epoch 112: Training Loss: 0.0015843030823816662 NSE : 0.6345056295394897 WAPE : 32.637368410081095 Validation Loss: 7.905616075731814e-05\n","Epoch 113: Training Loss: 0.0015768489784022677 NSE : 0.6368825435638428 WAPE : 32.51316837255842 Validation Loss: 7.85420197644271e-05\n","Epoch 114: Training Loss: 0.0015694810590503039 NSE : 0.6395418643951416 WAPE : 32.371726668195365 Validation Loss: 7.796681165928021e-05\n","Epoch 115: Training Loss: 0.0015622636774423881 NSE : 0.6414629220962524 WAPE : 32.27802630756082 Validation Loss: 7.755129627184942e-05\n","Epoch 116: Training Loss: 0.001555103411192249 NSE : 0.64359450340271 WAPE : 32.17054678868483 Validation Loss: 7.709022611379623e-05\n","Epoch 117: Training Loss: 0.0015479487183256424 NSE : 0.6468114852905273 WAPE : 31.993631569072985 Validation Loss: 7.639440445927903e-05\n","Epoch 118: Training Loss: 0.0015410143432745826 NSE : 0.6488890647888184 WAPE : 31.887419482604074 Validation Loss: 7.594502676511183e-05\n","Epoch 119: Training Loss: 0.0015341334219556302 NSE : 0.6509105563163757 WAPE : 31.784515644868776 Validation Loss: 7.550777809228748e-05\n","Epoch 120: Training Loss: 0.0015273570679710247 NSE : 0.6523820161819458 WAPE : 31.715611515290487 Validation Loss: 7.518952043028548e-05\n","Epoch 121: Training Loss: 0.0015205953950498952 NSE : 0.6548495292663574 WAPE : 31.581640991432323 Validation Loss: 7.465579255949706e-05\n","Epoch 122: Training Loss: 0.0015139389333853615 NSE : 0.65763258934021 WAPE : 31.425734297804926 Validation Loss: 7.405380893032998e-05\n","Epoch 123: Training Loss: 0.0015074605826157494 NSE : 0.6589338779449463 WAPE : 31.366312980759208 Validation Loss: 7.377234578598291e-05\n","Epoch 124: Training Loss: 0.001500971596215095 NSE : 0.6609859466552734 WAPE : 31.257390923683058 Validation Loss: 7.332849054364488e-05\n","Epoch 125: Training Loss: 0.0014945231741876341 NSE : 0.6639765501022339 WAPE : 31.085409935169167 Validation Loss: 7.268162880791351e-05\n","Epoch 126: Training Loss: 0.0014882631394357304 NSE : 0.6656694412231445 WAPE : 30.9986679452169 Validation Loss: 7.231544441310689e-05\n","Epoch 127: Training Loss: 0.0014820755022810772 NSE : 0.666614830493927 WAPE : 30.961205728460943 Validation Loss: 7.211096817627549e-05\n","Epoch 128: Training Loss: 0.001475823276450683 NSE : 0.6693828105926514 WAPE : 30.80137583123137 Validation Loss: 7.151223690016195e-05\n","Epoch 129: Training Loss: 0.0014697468341182685 NSE : 0.6715331077575684 WAPE : 30.68157428446353 Validation Loss: 7.104712858563289e-05\n","Epoch 130: Training Loss: 0.0014637396934631397 NSE : 0.673508882522583 WAPE : 30.572864855850412 Validation Loss: 7.061978976707906e-05\n","Epoch 131: Training Loss: 0.0014578150794477551 NSE : 0.6751137971878052 WAPE : 30.488290842383943 Validation Loss: 7.027263200143352e-05\n","Epoch 132: Training Loss: 0.0014519948645101977 NSE : 0.6759152412414551 WAPE : 30.457699443413727 Validation Loss: 7.009928958723322e-05\n","Epoch 133: Training Loss: 0.0014460784295806661 NSE : 0.6789286136627197 WAPE : 30.277017364657816 Validation Loss: 6.944749475223944e-05\n","Epoch 134: Training Loss: 0.0014403025634237565 NSE : 0.6818236112594604 WAPE : 30.10228679827396 Validation Loss: 6.882131128804758e-05\n","Epoch 135: Training Loss: 0.0014347737305797637 NSE : 0.6821684837341309 WAPE : 30.101152779804462 Validation Loss: 6.874670361867175e-05\n","Epoch 136: Training Loss: 0.0014291594125097618 NSE : 0.6836177706718445 WAPE : 30.02447311917617 Validation Loss: 6.843323353677988e-05\n","Epoch 137: Training Loss: 0.0014235211501727463 NSE : 0.6865247488021851 WAPE : 29.846930437139108 Validation Loss: 6.780446710763499e-05\n","Epoch 138: Training Loss: 0.001418101523995574 NSE : 0.6880483031272888 WAPE : 29.7638323153572 Validation Loss: 6.747491715941578e-05\n","Epoch 139: Training Loss: 0.0014127260610621306 NSE : 0.6892586946487427 WAPE : 29.70267870171562 Validation Loss: 6.721310637658462e-05\n","Epoch 140: Training Loss: 0.0014073157672100933 NSE : 0.6915607452392578 WAPE : 29.565770986637762 Validation Loss: 6.671518349321559e-05\n","Epoch 141: Training Loss: 0.0014020112294019782 NSE : 0.6935635805130005 WAPE : 29.44819995413896 Validation Loss: 6.628197297686711e-05\n","Epoch 142: Training Loss: 0.0013968202301839483 NSE : 0.6948133707046509 WAPE : 29.38214963206938 Validation Loss: 6.601162749575451e-05\n","Epoch 143: Training Loss: 0.0013916456709921476 NSE : 0.6959754228591919 WAPE : 29.32241979529299 Validation Loss: 6.576027953997254e-05\n","Epoch 144: Training Loss: 0.0013864618404113571 NSE : 0.6978593468666077 WAPE : 29.21177377999208 Validation Loss: 6.535280408570543e-05\n","Epoch 145: Training Loss: 0.0013813526011290378 NSE : 0.7000080347061157 WAPE : 29.081470054824788 Validation Loss: 6.488803046522662e-05\n","Epoch 146: Training Loss: 0.0013763254919467727 NSE : 0.7015092968940735 WAPE : 28.996164349294368 Validation Loss: 6.456331902882084e-05\n","Epoch 147: Training Loss: 0.0013713431662836228 NSE : 0.7027425765991211 WAPE : 28.928929978528693 Validation Loss: 6.429654604289681e-05\n","Epoch 148: Training Loss: 0.0013664519728990854 NSE : 0.7035763263702393 WAPE : 28.889007942298473 Validation Loss: 6.411621870938689e-05\n","Epoch 149: Training Loss: 0.0013615180114356917 NSE : 0.7064114809036255 WAPE : 28.70678534948198 Validation Loss: 6.350297917379066e-05\n","Epoch 150: Training Loss: 0.0013567011019404163 NSE : 0.7087323665618896 WAPE : 28.560180108815743 Validation Loss: 6.300096720224246e-05\n","Epoch 151: Training Loss: 0.0013521052305804915 NSE : 0.7082966566085815 WAPE : 28.610327072606367 Validation Loss: 6.309521268121898e-05\n","Epoch 152: Training Loss: 0.001347393495052529 NSE : 0.7091410160064697 WAPE : 28.568766546455148 Validation Loss: 6.291257886914536e-05\n","Epoch 153: Training Loss: 0.0013426661580524524 NSE : 0.7115226984024048 WAPE : 28.415323841487567 Validation Loss: 6.239741924218833e-05\n","Epoch 154: Training Loss: 0.0013380890522967093 NSE : 0.7137265205383301 WAPE : 28.273955097871628 Validation Loss: 6.192074215505272e-05\n","Epoch 155: Training Loss: 0.00133359557185031 NSE : 0.7152276039123535 WAPE : 28.183091868003586 Validation Loss: 6.159605254651979e-05\n","Epoch 156: Training Loss: 0.0013292151597852353 NSE : 0.7153192758560181 WAPE : 28.194315315503115 Validation Loss: 6.157623283797875e-05\n","Epoch 157: Training Loss: 0.001324760150964721 NSE : 0.7161572575569153 WAPE : 28.15112046861646 Validation Loss: 6.13949669059366e-05\n","Epoch 158: Training Loss: 0.00132030289114482 NSE : 0.7188340425491333 WAPE : 27.973175460173856 Validation Loss: 6.0815978940809146e-05\n","Epoch 159: Training Loss: 0.0013159836189515772 NSE : 0.7206523418426514 WAPE : 27.856871860082133 Validation Loss: 6.042269160388969e-05\n","Epoch 160: Training Loss: 0.001311783154051227 NSE : 0.7212444543838501 WAPE : 27.83033082487336 Validation Loss: 6.029462383594364e-05\n","Epoch 161: Training Loss: 0.001307603298300819 NSE : 0.7215595841407776 WAPE : 27.824031185507913 Validation Loss: 6.022645175107755e-05\n","Epoch 162: Training Loss: 0.0013033570057814359 NSE : 0.7235435247421265 WAPE : 27.695382627003816 Validation Loss: 5.979732668492943e-05\n","Epoch 163: Training Loss: 0.0012991912108191173 NSE : 0.7254579067230225 WAPE : 27.570304975922955 Validation Loss: 5.9383248299127445e-05\n","Epoch 164: Training Loss: 0.0012951612970937276 NSE : 0.7267209887504578 WAPE : 27.492741447958142 Validation Loss: 5.911004336667247e-05\n","Epoch 165: Training Loss: 0.0012912137772218557 NSE : 0.7265604138374329 WAPE : 27.520958495757853 Validation Loss: 5.914476787438616e-05\n","Epoch 166: Training Loss: 0.0012871568997070426 NSE : 0.7284778356552124 WAPE : 27.39502407704655 Validation Loss: 5.873004192835651e-05\n","Epoch 167: Training Loss: 0.001283184603380505 NSE : 0.7298427224159241 WAPE : 27.309153446874152 Validation Loss: 5.843481267220341e-05\n","Epoch 168: Training Loss: 0.0012792654479198973 NSE : 0.7316501140594482 WAPE : 27.190394196493717 Validation Loss: 5.804387910757214e-05\n","Epoch 169: Training Loss: 0.0012754676381518948 NSE : 0.7324374914169312 WAPE : 27.148268745700527 Validation Loss: 5.7873574405675754e-05\n","Epoch 170: Training Loss: 0.0012716302226181142 NSE : 0.7337270975112915 WAPE : 27.068466365095578 Validation Loss: 5.7594636018620804e-05\n","Epoch 171: Training Loss: 0.0012679022538577556 NSE : 0.7340309023857117 WAPE : 27.061760230139043 Validation Loss: 5.752891593147069e-05\n","Epoch 172: Training Loss: 0.001264078644453548 NSE : 0.7363936901092529 WAPE : 26.90092972837756 Validation Loss: 5.701784175471403e-05\n","Epoch 173: Training Loss: 0.0012604012963493005 NSE : 0.7376773357391357 WAPE : 26.82022471910112 Validation Loss: 5.674019121215679e-05\n","Epoch 174: Training Loss: 0.0012567701742227655 NSE : 0.7384533882141113 WAPE : 26.777221654749745 Validation Loss: 5.657233850797638e-05\n","Epoch 175: Training Loss: 0.0012531810771179153 NSE : 0.7391958236694336 WAPE : 26.736286506430968 Validation Loss: 5.641173629555851e-05\n","Epoch 176: Training Loss: 0.0012495669407144305 NSE : 0.7406341433525085 WAPE : 26.642246357174127 Validation Loss: 5.6100634537870064e-05\n","Epoch 177: Training Loss: 0.0012460436510082218 NSE : 0.7422088384628296 WAPE : 26.5374830626837 Validation Loss: 5.5760039685992524e-05\n","Epoch 178: Training Loss: 0.001242574602656532 NSE : 0.7428614497184753 WAPE : 26.502247191011236 Validation Loss: 5.561887155636214e-05\n","Epoch 179: Training Loss: 0.0012391565651341807 NSE : 0.743493914604187 WAPE : 26.468057784911718 Validation Loss: 5.548207263927907e-05\n","Epoch 180: Training Loss: 0.0012357289442661568 NSE : 0.7443841695785522 WAPE : 26.4149903066436 Validation Loss: 5.5289510783040896e-05\n","Epoch 181: Training Loss: 0.0012322803549977834 NSE : 0.7458077669143677 WAPE : 26.320933480644555 Validation Loss: 5.498158498085104e-05\n","Epoch 182: Training Loss: 0.001228935046128754 NSE : 0.7474192380905151 WAPE : 26.211065018448647 Validation Loss: 5.463303023134358e-05\n","Epoch 183: Training Loss: 0.0012257254184078192 NSE : 0.7475813031196594 WAPE : 26.212697254591315 Validation Loss: 5.4597974667558447e-05\n","Epoch 184: Training Loss: 0.0012224544971104478 NSE : 0.7479498386383057 WAPE : 26.19871589085072 Validation Loss: 5.451825927593745e-05\n","Epoch 185: Training Loss: 0.0012191512887511635 NSE : 0.7494012713432312 WAPE : 26.101919909945593 Validation Loss: 5.420431261882186e-05\n","Epoch 186: Training Loss: 0.0012158730523879058 NSE : 0.7513655424118042 WAPE : 25.96566258781347 Validation Loss: 5.3779447625856847e-05\n","Epoch 187: Training Loss: 0.001212679660966387 NSE : 0.7526642084121704 WAPE : 25.87935836234392 Validation Loss: 5.349854473024607e-05\n","Epoch 188: Training Loss: 0.0012096041818949743 NSE : 0.7532801628112793 WAPE : 25.84576098059244 Validation Loss: 5.336530739441514e-05\n","Epoch 189: Training Loss: 0.0012065529872415937 NSE : 0.7530951499938965 WAPE : 25.87478268120323 Validation Loss: 5.340533607522957e-05\n","Epoch 190: Training Loss: 0.0012033904758936842 NSE : 0.754517138004303 WAPE : 25.77850993308457 Validation Loss: 5.309775224304758e-05\n","Epoch 191: Training Loss: 0.0012002752782791504 NSE : 0.7564443349838257 WAPE : 25.641616810156137 Validation Loss: 5.2680908993352205e-05\n","Epoch 192: Training Loss: 0.0011971922267548507 NSE : 0.7585789561271667 WAPE : 25.490025223572577 Validation Loss: 5.2219187637092546e-05\n","Epoch 193: Training Loss: 0.0011942920782530564 NSE : 0.7585576176643372 WAPE : 25.506201663505035 Validation Loss: 5.222380787017755e-05\n","Epoch 194: Training Loss: 0.001191408846352715 NSE : 0.7583996057510376 WAPE : 25.53247170165308 Validation Loss: 5.225797212915495e-05\n","Epoch 195: Training Loss: 0.001188356158309034 NSE : 0.7605619430541992 WAPE : 25.37603552146088 Validation Loss: 5.1790266297757626e-05\n","Epoch 196: Training Loss: 0.0011854560279971338 NSE : 0.7614859342575073 WAPE : 25.317854537116176 Validation Loss: 5.159041756996885e-05\n","Epoch 197: Training Loss: 0.0011825257397504174 NSE : 0.7636212706565857 WAPE : 25.161093160451102 Validation Loss: 5.11285470565781e-05\n","Epoch 198: Training Loss: 0.0011798228251791443 NSE : 0.7632642984390259 WAPE : 25.203608430093183 Validation Loss: 5.12057522428222e-05\n","Epoch 199: Training Loss: 0.0011770442924898816 NSE : 0.7631692290306091 WAPE : 25.224410581392924 Validation Loss: 5.122631773701869e-05\n","Epoch 200: Training Loss: 0.0011741453472495778 NSE : 0.7651188969612122 WAPE : 25.08207250213671 Validation Loss: 5.080460323370062e-05\n","Epoch 201: Training Loss: 0.0011713374551618472 NSE : 0.7674856185913086 WAPE : 24.90591398970211 Validation Loss: 5.029269232181832e-05\n","Epoch 202: Training Loss: 0.0011687178748616134 NSE : 0.7674950361251831 WAPE : 24.9186216672573 Validation Loss: 5.029065505368635e-05\n","Epoch 203: Training Loss: 0.001166108323559456 NSE : 0.7670261263847351 WAPE : 24.96809739217444 Validation Loss: 5.039206735091284e-05\n","Epoch 204: Training Loss: 0.0011634140064415988 NSE : 0.7678073644638062 WAPE : 24.918423630943696 Validation Loss: 5.022309778723866e-05\n","Epoch 205: Training Loss: 0.001160647044343932 NSE : 0.7702873945236206 WAPE : 24.731270976214798 Validation Loss: 4.968665962223895e-05\n","Epoch 206: Training Loss: 0.0011581201124499785 NSE : 0.7704488039016724 WAPE : 24.731062516937317 Validation Loss: 4.9651753215584904e-05\n","Epoch 207: Training Loss: 0.0011555637393030338 NSE : 0.770963191986084 WAPE : 24.70048987930208 Validation Loss: 4.954048927174881e-05\n","Epoch 208: Training Loss: 0.0011529753555805655 NSE : 0.7721178531646729 WAPE : 24.620654145212733 Validation Loss: 4.929073838866316e-05\n","Epoch 209: Training Loss: 0.0011503752048156457 NSE : 0.7737035155296326 WAPE : 24.505146859560984 Validation Loss: 4.8947757022688165e-05\n","Epoch 210: Training Loss: 0.0011478965971036814 NSE : 0.7742544412612915 WAPE : 24.47329428196202 Validation Loss: 4.882859866484068e-05\n","Epoch 211: Training Loss: 0.0011454222430984373 NSE : 0.7751079797744751 WAPE : 24.416864355548142 Validation Loss: 4.864397851633839e-05\n","Epoch 212: Training Loss: 0.0011429917612986173 NSE : 0.7751871347427368 WAPE : 24.424398073836276 Validation Loss: 4.862685455009341e-05\n","Epoch 213: Training Loss: 0.0011404655642763828 NSE : 0.7770717740058899 WAPE : 24.28263325759313 Validation Loss: 4.8219208110822365e-05\n","Epoch 214: Training Loss: 0.0011380669748177752 NSE : 0.7779975533485413 WAPE : 24.219126138708802 Validation Loss: 4.801896648132242e-05\n","Epoch 215: Training Loss: 0.0011356588374837884 NSE : 0.7793024778366089 WAPE : 24.12556336119739 Validation Loss: 4.773670298163779e-05\n","Epoch 216: Training Loss: 0.0011333889578963863 NSE : 0.7788028717041016 WAPE : 24.178566217089493 Validation Loss: 4.784476914210245e-05\n","Epoch 217: Training Loss: 0.0011309961400911561 NSE : 0.780125081539154 WAPE : 24.081453378082593 Validation Loss: 4.7558783990098163e-05\n","Epoch 218: Training Loss: 0.00112865333994705 NSE : 0.7814474701881409 WAPE : 23.985084738696298 Validation Loss: 4.727274426841177e-05\n","Epoch 219: Training Loss: 0.0011263730366408709 NSE : 0.7817806005477905 WAPE : 23.970801109003357 Validation Loss: 4.720069409813732e-05\n","Epoch 220: Training Loss: 0.001124074614381243 NSE : 0.7830085158348083 WAPE : 23.881319964145003 Validation Loss: 4.6935096179367974e-05\n","Epoch 221: Training Loss: 0.0011218575646125828 NSE : 0.7836240530014038 WAPE : 23.841308290425463 Validation Loss: 4.680194615502842e-05\n","Epoch 222: Training Loss: 0.0011196960795132327 NSE : 0.7835009694099426 WAPE : 23.862396030935358 Validation Loss: 4.6828579797875136e-05\n","Epoch 223: Training Loss: 0.001117419602451264 NSE : 0.7851667404174805 WAPE : 23.735194179816972 Validation Loss: 4.646827801479958e-05\n","Epoch 224: Training Loss: 0.001115280687372433 NSE : 0.7851657867431641 WAPE : 23.747797627733423 Validation Loss: 4.6468485379591584e-05\n","Epoch 225: Training Loss: 0.0011131046821901691 NSE : 0.7855898141860962 WAPE : 23.72488169936003 Validation Loss: 4.637676101992838e-05\n","Epoch 226: Training Loss: 0.0011109218748970306 NSE : 0.7868661880493164 WAPE : 23.6278209751725 Validation Loss: 4.6100678446237e-05\n","Epoch 227: Training Loss: 0.0011088496785305324 NSE : 0.7872447967529297 WAPE : 23.606483083529632 Validation Loss: 4.601879118126817e-05\n","Epoch 228: Training Loss: 0.0011067179143537942 NSE : 0.78802090883255 WAPE : 23.55425778074253 Validation Loss: 4.585091664921492e-05\n","Epoch 229: Training Loss: 0.001104574836062966 NSE : 0.7891801595687866 WAPE : 23.469141773154615 Validation Loss: 4.560016895993613e-05\n","Epoch 230: Training Loss: 0.001102568790429359 NSE : 0.7893612384796143 WAPE : 23.464805820183027 Validation Loss: 4.55610133940354e-05\n","Epoch 231: Training Loss: 0.001100594839954283 NSE : 0.7890856266021729 WAPE : 23.498776344041193 Validation Loss: 4.5620625314768404e-05\n","Epoch 232: Training Loss: 0.0010984829796143458 NSE : 0.7904766201972961 WAPE : 23.392349544516478 Validation Loss: 4.5319746277527884e-05\n","Epoch 233: Training Loss: 0.0010964131488435669 NSE : 0.7918062210083008 WAPE : 23.292016009672512 Validation Loss: 4.5032149500912055e-05\n","Epoch 234: Training Loss: 0.0010944978225779778 NSE : 0.7917637825012207 WAPE : 23.30775885430781 Validation Loss: 4.504133903537877e-05\n","Epoch 235: Training Loss: 0.0010925507181127614 NSE : 0.7918702960014343 WAPE : 23.309215984657396 Validation Loss: 4.5018303353572264e-05\n","Epoch 236: Training Loss: 0.0010905768335760513 NSE : 0.792531430721283 WAPE : 23.263809384836673 Validation Loss: 4.487529440666549e-05\n","Epoch 237: Training Loss: 0.001088555603473651 NSE : 0.7940326929092407 WAPE : 23.149436117654414 Validation Loss: 4.455056841834448e-05\n","Epoch 238: Training Loss: 0.0010866017828448093 NSE : 0.7951290607452393 WAPE : 23.068699839486356 Validation Loss: 4.43134231318254e-05\n","Epoch 239: Training Loss: 0.0010847926737369562 NSE : 0.7947121262550354 WAPE : 23.114915261303704 Validation Loss: 4.4403608626453206e-05\n","Epoch 240: Training Loss: 0.0010829291941263364 NSE : 0.7945817708969116 WAPE : 23.136845177294614 Validation Loss: 4.4431806600186974e-05\n","Epoch 241: Training Loss: 0.0010809245991367789 NSE : 0.7964911460876465 WAPE : 22.986741989952264 Validation Loss: 4.401880505611189e-05\n","Epoch 242: Training Loss: 0.001079052484328713 NSE : 0.7974861264228821 WAPE : 22.91387921869463 Validation Loss: 4.380360041977838e-05\n","Epoch 243: Training Loss: 0.0010772627952064795 NSE : 0.7974185943603516 WAPE : 22.930524692001416 Validation Loss: 4.3818199628731236e-05\n","Epoch 244: Training Loss: 0.0010754022810033348 NSE : 0.7986356616020203 WAPE : 22.836853515665716 Validation Loss: 4.355495184427127e-05\n","Epoch 245: Training Loss: 0.0010735984783423191 NSE : 0.7991802096366882 WAPE : 22.80217839944967 Validation Loss: 4.343716136645526e-05\n","Epoch 246: Training Loss: 0.0010718118219301687 NSE : 0.7994364500045776 WAPE : 22.792157761981198 Validation Loss: 4.3381736759329215e-05\n","Epoch 247: Training Loss: 0.001070046673703473 NSE : 0.799939751625061 WAPE : 22.757728627712577 Validation Loss: 4.3272881157463416e-05\n","Epoch 248: Training Loss: 0.0010683430732569832 NSE : 0.8003147840499878 WAPE : 22.73283025161035 Validation Loss: 4.319175423006527e-05\n","Epoch 249: Training Loss: 0.001066675356014457 NSE : 0.7998818755149841 WAPE : 22.77915615684476 Validation Loss: 4.328539580455981e-05\n","Epoch 250: Training Loss: 0.0010648356928868452 NSE : 0.8011716604232788 WAPE : 22.67870171561985 Validation Loss: 4.30064246756956e-05\n","Epoch 251: Training Loss: 0.001063051755863853 NSE : 0.802544355392456 WAPE : 22.57174959871589 Validation Loss: 4.2709514673333615e-05\n","Epoch 252: Training Loss: 0.0010613887602630712 NSE : 0.8029152154922485 WAPE : 22.54945904817494 Validation Loss: 4.262929724063724e-05\n","Epoch 253: Training Loss: 0.001059747206454631 NSE : 0.8032039999961853 WAPE : 22.533693273019114 Validation Loss: 4.2566825868561864e-05\n","Epoch 254: Training Loss: 0.0010580890570963675 NSE : 0.8034965991973877 WAPE : 22.517925413270518 Validation Loss: 4.250353595125489e-05\n","Epoch 255: Training Loss: 0.001056453170349414 NSE : 0.8038480877876282 WAPE : 22.496316524566925 Validation Loss: 4.242750947014429e-05\n","Epoch 256: Training Loss: 0.0010547921106081048 NSE : 0.804591178894043 WAPE : 22.439515540639135 Validation Loss: 4.226677629048936e-05\n","Epoch 257: Training Loss: 0.001053180764301942 NSE : 0.805092453956604 WAPE : 22.40577849117175 Validation Loss: 4.215836088405922e-05\n","Epoch 258: Training Loss: 0.0010516322145122103 NSE : 0.8047523498535156 WAPE : 22.444960496966917 Validation Loss: 4.2231920815538615e-05\n","Epoch 259: Training Loss: 0.001050011296683806 NSE : 0.8053589463233948 WAPE : 22.400491963894854 Validation Loss: 4.210070983390324e-05\n","Epoch 260: Training Loss: 0.0010483972537258524 NSE : 0.8062555193901062 WAPE : 22.330683121052303 Validation Loss: 4.190678373561241e-05\n","Epoch 261: Training Loss: 0.0010467919987604546 NSE : 0.8071063756942749 WAPE : 22.266948781555524 Validation Loss: 4.172274566371925e-05\n","Epoch 262: Training Loss: 0.0010452064466335287 NSE : 0.8082311153411865 WAPE : 22.17843488774468 Validation Loss: 4.14794594689738e-05\n","Epoch 263: Training Loss: 0.001043763040797785 NSE : 0.8072987794876099 WAPE : 22.270175731170916 Validation Loss: 4.168113082414493e-05\n","Epoch 264: Training Loss: 0.0010421757515359786 NSE : 0.8080033659934998 WAPE : 22.215990911175503 Validation Loss: 4.1528724977979437e-05\n","Epoch 265: Training Loss: 0.0010405932721369027 NSE : 0.8094505071640015 WAPE : 22.096850180317276 Validation Loss: 4.1215713281417266e-05\n","Epoch 266: Training Loss: 0.0010391118321422255 NSE : 0.8100534677505493 WAPE : 22.053398928519314 Validation Loss: 4.1085284465225413e-05\n","Epoch 267: Training Loss: 0.001037722801356722 NSE : 0.8091884851455688 WAPE : 22.13868795730754 Validation Loss: 4.127240026718937e-05\n","Epoch 268: Training Loss: 0.0010362378206991707 NSE : 0.8092156648635864 WAPE : 22.142957203310335 Validation Loss: 4.126650674152188e-05\n","Epoch 269: Training Loss: 0.001034708110637439 NSE : 0.8101298213005066 WAPE : 22.069885972775218 Validation Loss: 4.106877531739883e-05\n","Epoch 270: Training Loss: 0.0010332292076782323 NSE : 0.8109448552131653 WAPE : 22.00722936774301 Validation Loss: 4.08924970543012e-05\n","Epoch 271: Training Loss: 0.0010317515966562496 NSE : 0.8117981553077698 WAPE : 21.940205540847597 Validation Loss: 4.070791692356579e-05\n","Epoch 272: Training Loss: 0.0010303417384420754 NSE : 0.8119716048240662 WAPE : 21.93334306143295 Validation Loss: 4.0670409362064674e-05\n","Epoch 273: Training Loss: 0.0010289424621987564 NSE : 0.8118968605995178 WAPE : 21.94950490921598 Validation Loss: 4.0686569263925776e-05\n","Epoch 274: Training Loss: 0.001027475897444674 NSE : 0.8129972815513611 WAPE : 21.85949636238561 Validation Loss: 4.0448547224514186e-05\n","Epoch 275: Training Loss: 0.001026113851366972 NSE : 0.8129171133041382 WAPE : 21.873978028392155 Validation Loss: 4.046588219352998e-05\n","Epoch 276: Training Loss: 0.0010246992428619706 NSE : 0.8137050867080688 WAPE : 21.81102332659315 Validation Loss: 4.0295450162375346e-05\n","Epoch 277: Training Loss: 0.0010233025623165304 NSE : 0.8145793676376343 WAPE : 21.742127535385965 Validation Loss: 4.0106340748025104e-05\n","Epoch 278: Training Loss: 0.001021992527967086 NSE : 0.8140685558319092 WAPE : 21.796948156177688 Validation Loss: 4.02168370783329e-05\n","Epoch 279: Training Loss: 0.0010206609044871584 NSE : 0.8138458728790283 WAPE : 21.823193179212442 Validation Loss: 4.026500027975999e-05\n","Epoch 280: Training Loss: 0.0010192428298978484 NSE : 0.8151319026947021 WAPE : 21.713510245773488 Validation Loss: 3.998682586825453e-05\n","Epoch 281: Training Loss: 0.0010179039286413172 NSE : 0.8156278729438782 WAPE : 21.677792833170038 Validation Loss: 3.987954914919101e-05\n","Epoch 282: Training Loss: 0.0010166097636101767 NSE : 0.8159103989601135 WAPE : 21.66054908173688 Validation Loss: 3.981843838118948e-05\n","Epoch 283: Training Loss: 0.0010153103203265346 NSE : 0.8161929249763489 WAPE : 21.64148339621855 Validation Loss: 3.975733125116676e-05\n","Epoch 284: Training Loss: 0.001014020044294739 NSE : 0.8162021636962891 WAPE : 21.64745366992558 Validation Loss: 3.975533036282286e-05\n","Epoch 285: Training Loss: 0.001012687320326222 NSE : 0.8167234659194946 WAPE : 21.60907214775594 Validation Loss: 3.964257120969705e-05\n","Epoch 286: Training Loss: 0.001011362859117071 NSE : 0.8174506425857544 WAPE : 21.552508807404475 Validation Loss: 3.9485279557993636e-05\n","Epoch 287: Training Loss: 0.0010100389908984653 NSE : 0.8185308575630188 WAPE : 21.462750411707074 Validation Loss: 3.925164128304459e-05\n","Epoch 288: Training Loss: 0.0010088195645039377 NSE : 0.8189314603805542 WAPE : 21.433307623355777 Validation Loss: 3.916499190381728e-05\n","Epoch 289: Training Loss: 0.0010076977346216154 NSE : 0.81795334815979 WAPE : 21.528039857413855 Validation Loss: 3.937656219932251e-05\n","Epoch 290: Training Loss: 0.0010064275402328349 NSE : 0.8179455399513245 WAPE : 21.534395780784223 Validation Loss: 3.937824658351019e-05\n","Epoch 291: Training Loss: 0.0010050894029518531 NSE : 0.8194938898086548 WAPE : 21.40125075566488 Validation Loss: 3.904333425452933e-05\n","Epoch 292: Training Loss: 0.001003860012133373 NSE : 0.8205504417419434 WAPE : 21.31429613724959 Validation Loss: 3.8814810977783054e-05\n","Epoch 293: Training Loss: 0.001002743859316979 NSE : 0.8197585344314575 WAPE : 21.39482395614017 Validation Loss: 3.8986094295978546e-05\n","Epoch 294: Training Loss: 0.0010015159400609264 NSE : 0.8197541236877441 WAPE : 21.401717704446437 Validation Loss: 3.898705108440481e-05\n","Epoch 295: Training Loss: 0.0010002887856899179 NSE : 0.8204860687255859 WAPE : 21.34131871338934 Validation Loss: 3.8828718970762566e-05\n","Epoch 296: Training Loss: 0.0009990550120164698 NSE : 0.8216981291770935 WAPE : 21.238112609701695 Validation Loss: 3.856656257994473e-05\n","Epoch 297: Training Loss: 0.0009979307892535871 NSE : 0.8215822577476501 WAPE : 21.258199745679683 Validation Loss: 3.859161370201036e-05\n","Epoch 298: Training Loss: 0.0009967273026632029 NSE : 0.8219422101974487 WAPE : 21.2336036355298 Validation Loss: 3.851376459351741e-05\n","Epoch 299: Training Loss: 0.000995587308807444 NSE : 0.8219159841537476 WAPE : 21.24251735423485 Validation Loss: 3.8519436202477664e-05\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:15:47,848] Trial 0 finished with value: 3.8298974686767906e-05 and parameters: {'lr': 0.00025955235006424986, 'weight_decay': 0.0001198159412577847}. Best is trial 0 with value: 3.8298974686767906e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 300: Training Loss: 0.0009943796385414316 NSE : 0.8229352235794067 WAPE : 21.156523733088743 Validation Loss: 3.8298974686767906e-05\n","Epoch 1: Training Loss: 0.008697737357579172 NSE : -12.021366119384766 WAPE : 222.62673698692961 Validation Loss: 0.002816511783748865\n","Epoch 2: Training Loss: 0.0039337555408565095 NSE : -0.5768553018569946 WAPE : 62.13952596360301 Validation Loss: 0.0003410726203583181\n","Epoch 3: Training Loss: 0.004087241126399022 NSE : -2.309868574142456 WAPE : 103.57204144170436 Validation Loss: 0.000715922040399164\n","Epoch 4: Training Loss: 0.0043949831306235865 NSE : -0.35761773586273193 WAPE : 66.90138625419524 Validation Loss: 0.00029365174123086035\n","Epoch 5: Training Loss: 0.0035991910699522123 NSE : -1.2149837017059326 WAPE : 89.7260261407934 Validation Loss: 0.00047909931163303554\n","Epoch 6: Training Loss: 0.0030263619555626065 NSE : -0.4654346704483032 WAPE : 72.1773321381668 Validation Loss: 0.00031697243684902787\n","Epoch 7: Training Loss: 0.0026098334583366523 NSE : 0.5375158190727234 WAPE : 34.75852702674532 Validation Loss: 0.0001000349730020389\n","Epoch 8: Training Loss: 0.0024363422799069667 NSE : 0.7251404523849487 WAPE : 21.03966146213337 Validation Loss: 5.94519151491113e-05\n","Epoch 9: Training Loss: 0.0023547278196929256 NSE : 0.7339831590652466 WAPE : 20.671388964165853 Validation Loss: 5.753923687734641e-05\n","Epoch 10: Training Loss: 0.00220871448982507 NSE : 0.7105400562286377 WAPE : 24.81591586583561 Validation Loss: 6.260997179197147e-05\n","Epoch 11: Training Loss: 0.0020643001880671363 NSE : 0.28037863969802856 WAPE : 49.30023555898355 Validation Loss: 0.00015565355715807527\n","Epoch 12: Training Loss: 0.001911032377392985 NSE : -0.13406264781951904 WAPE : 63.547555814971545 Validation Loss: 0.0002452968910802156\n","Epoch 13: Training Loss: 0.0017916351825988386 NSE : 0.018199443817138672 WAPE : 59.209355652373304 Validation Loss: 0.00021236272004898638\n","Epoch 14: Training Loss: 0.0015823579965399404 NSE : 0.782060980796814 WAPE : 21.25532926142878 Validation Loss: 4.7140041715465486e-05\n","Epoch 15: Training Loss: 0.0014891557607370487 NSE : 0.5965315103530884 WAPE : 31.988046945029286 Validation Loss: 8.726993837626651e-05\n","Epoch 16: Training Loss: 0.001488214327764581 NSE : 0.2170848846435547 WAPE : 49.12296178941444 Validation Loss: 0.00016934391169343144\n","Epoch 17: Training Loss: 0.0015577618823954253 NSE : 0.812139093875885 WAPE : 16.96757520168435 Validation Loss: 4.0634171455167234e-05\n","Epoch 18: Training Loss: 0.0015350871326518245 NSE : 0.7932170629501343 WAPE : 20.822630339163243 Validation Loss: 4.472698492463678e-05\n","Epoch 19: Training Loss: 0.0013819754822179675 NSE : 0.5039709806442261 WAPE : 40.06528110733568 Validation Loss: 0.00010729069617809728\n","Epoch 20: Training Loss: 0.0013037235257797875 NSE : 0.3264923095703125 WAPE : 47.9884263409143 Validation Loss: 0.00014567923790309578\n","Epoch 21: Training Loss: 0.001248510392542812 NSE : 0.22721987962722778 WAPE : 52.31881761897813 Validation Loss: 0.00016715176752768457\n","Epoch 22: Training Loss: 0.0011750498902074469 NSE : 0.6604020595550537 WAPE : 32.31585541264514 Validation Loss: 7.345477206399664e-05\n","Epoch 23: Training Loss: 0.0011023559200111777 NSE : 0.8714593052864075 WAPE : 14.816910216589187 Validation Loss: 2.780324939521961e-05\n","Epoch 24: Training Loss: 0.0010923731902039435 NSE : 0.8351583480834961 WAPE : 17.77680682078756 Validation Loss: 3.5655120882438496e-05\n","Epoch 25: Training Loss: 0.0011174946512255701 NSE : 0.8518905639648438 WAPE : 16.09560150924517 Validation Loss: 3.2035964977694675e-05\n","Epoch 26: Training Loss: 0.0011299234956823057 NSE : 0.8737903237342834 WAPE : 14.767780534072669 Validation Loss: 2.729905827436596e-05\n","Epoch 27: Training Loss: 0.0011200473727512872 NSE : 0.6229087710380554 WAPE : 34.950582643680555 Validation Loss: 8.156454714480788e-05\n","Epoch 28: Training Loss: 0.0011069658867199905 NSE : 0.1849966049194336 WAPE : 54.22557378416126 Validation Loss: 0.00017628460773266852\n","Epoch 29: Training Loss: 0.0010947384139399219 NSE : 0.5456111431121826 WAPE : 39.308377978361925 Validation Loss: 9.828397014643997e-05\n","Epoch 30: Training Loss: 0.0010089215174957644 NSE : 0.8653374314308167 WAPE : 16.942424589856373 Validation Loss: 2.912741001637187e-05\n","Epoch 31: Training Loss: 0.000995426198187488 NSE : 0.8131593465805054 WAPE : 19.67156198536616 Validation Loss: 4.0413488022750244e-05\n","Epoch 32: Training Loss: 0.0010623134085108177 NSE : 0.8794975280761719 WAPE : 13.964445185632988 Validation Loss: 2.6064595658681355e-05\n","Epoch 33: Training Loss: 0.0010516961283428827 NSE : 0.7676462531089783 WAPE : 26.110844051614517 Validation Loss: 5.025794598623179e-05\n","Epoch 34: Training Loss: 0.001034416643960867 NSE : 0.33413970470428467 WAPE : 48.69021700610786 Validation Loss: 0.00014402509259525687\n","Epoch 35: Training Loss: 0.0010549543230808922 NSE : 0.3264550566673279 WAPE : 49.15348022763753 Validation Loss: 0.00014568724145647138\n","Epoch 36: Training Loss: 0.0009954846677828755 NSE : 0.8093082904815674 WAPE : 22.71511746680286 Validation Loss: 4.124646875425242e-05\n","Epoch 37: Training Loss: 0.0009392136728365585 NSE : 0.8442157506942749 WAPE : 16.99576514977799 Validation Loss: 3.369601836311631e-05\n","Epoch 38: Training Loss: 0.001018410872802633 NSE : 0.8172783255577087 WAPE : 18.829519918283964 Validation Loss: 3.952256156480871e-05\n","Epoch 39: Training Loss: 0.0010523086375542334 NSE : 0.8233146667480469 WAPE : 21.51928039857414 Validation Loss: 3.821690916083753e-05\n","Epoch 40: Training Loss: 0.0010081839791382663 NSE : 0.39717209339141846 WAPE : 46.161776906881244 Validation Loss: 0.000130391214042902\n","Epoch 41: Training Loss: 0.0010463201115271659 NSE : 0.16408950090408325 WAPE : 55.37585624648224 Validation Loss: 0.00018080681911669672\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:16:57,583] Trial 1 finished with value: 2.6064595658681355e-05 and parameters: {'lr': 0.005331202123074718, 'weight_decay': 0.00015174173660036834}. Best is trial 1 with value: 2.6064595658681355e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 42: Training Loss: 0.0010093921464431332 NSE : 0.7950759530067444 WAPE : 23.666715307164747 Validation Loss: 4.4324915506877005e-05\n","Early stopping!\n","Epoch 1: Training Loss: 0.006040068416041322 NSE : -11.457348823547363 WAPE : 216.70495924621127 Validation Loss: 0.0026945150457322598\n","Epoch 2: Training Loss: 0.0049607660475885496 NSE : -7.499561309814453 WAPE : 177.83894436221883 Validation Loss: 0.001838448690250516\n","Epoch 3: Training Loss: 0.004417223843120155 NSE : -1.572861909866333 WAPE : 94.2373183798545 Validation Loss: 0.0005565080791711807\n","Epoch 4: Training Loss: 0.004300287371734157 NSE : -3.3936939239501953 WAPE : 126.96496633382668 Validation Loss: 0.000950352696236223\n","Epoch 5: Training Loss: 0.00407102117605973 NSE : -3.0131540298461914 WAPE : 121.49305205228158 Validation Loss: 0.0008680421742610633\n","Epoch 6: Training Loss: 0.003897101851180196 NSE : -1.0877327919006348 WAPE : 85.40224719101124 Validation Loss: 0.00045157503336668015\n","Epoch 7: Training Loss: 0.003843815124128014 NSE : -1.6025888919830322 WAPE : 96.75260469867212 Validation Loss: 0.000562937930226326\n","Epoch 8: Training Loss: 0.00372115913341986 NSE : -0.794792890548706 WAPE : 78.76413666590231 Validation Loss: 0.00038821238558739424\n","Epoch 9: Training Loss: 0.0036783220784855075 NSE : -0.8875144720077515 WAPE : 81.19025244418503 Validation Loss: 0.00040826795157045126\n","Epoch 10: Training Loss: 0.0035910126025555655 NSE : -0.7667690515518188 WAPE : 78.32803985741386 Validation Loss: 0.000382150785299018\n","Epoch 11: Training Loss: 0.003521194968925556 NSE : -0.6887091398239136 WAPE : 76.47932292426674 Validation Loss: 0.0003652664599940181\n","Epoch 12: Training Loss: 0.0034450997627573088 NSE : -0.6800434589385986 WAPE : 76.42305559608931 Validation Loss: 0.00036339217331260443\n","Epoch 13: Training Loss: 0.003369779951754026 NSE : -0.5641320943832397 WAPE : 73.50012299097372 Validation Loss: 0.00033832056215032935\n","Epoch 14: Training Loss: 0.0033019629954651464 NSE : -0.5311777591705322 WAPE : 72.74681786912926 Validation Loss: 0.0003311925975140184\n","Epoch 15: Training Loss: 0.0032336184776795562 NSE : -0.43702542781829834 WAPE : 70.26835379708574 Validation Loss: 0.0003108275413978845\n","Epoch 16: Training Loss: 0.003169389394315658 NSE : -0.3775634765625 WAPE : 68.73525254841468 Validation Loss: 0.0002979659475386143\n","Epoch 17: Training Loss: 0.0031073603277036455 NSE : -0.3127189874649048 WAPE : 66.9951512372058 Validation Loss: 0.00028394011314958334\n","Epoch 18: Training Loss: 0.003047985846933443 NSE : -0.2721290588378906 WAPE : 65.92200287673803 Validation Loss: 0.00027516050613485277\n","Epoch 19: Training Loss: 0.0029905580304330215 NSE : -0.21960210800170898 WAPE : 64.44656146421796 Validation Loss: 0.000263799011008814\n","Epoch 20: Training Loss: 0.0029370712873060256 NSE : -0.17576122283935547 WAPE : 63.214700548247905 Validation Loss: 0.0002543162845540792\n","Epoch 21: Training Loss: 0.0028860369202448055 NSE : -0.12958288192749023 WAPE : 61.866110775260054 Validation Loss: 0.00024432793725281954\n","Epoch 22: Training Loss: 0.002837608124536928 NSE : -0.08799922466278076 WAPE : 60.63221529674178 Validation Loss: 0.000235333398450166\n","Epoch 23: Training Loss: 0.002790117179756635 NSE : -0.04954421520233154 WAPE : 59.478268120322696 Validation Loss: 0.00022701561101712286\n","Epoch 24: Training Loss: 0.0027462573798402445 NSE : -0.011000394821166992 WAPE : 58.28633966354673 Validation Loss: 0.0002186786150559783\n","Epoch 25: Training Loss: 0.0027051472279708833 NSE : 0.021667659282684326 WAPE : 57.269271017906654 Validation Loss: 0.0002116125397151336\n","Epoch 26: Training Loss: 0.0026665947116271127 NSE : 0.047336459159851074 WAPE : 56.47920201788581 Validation Loss: 0.00020606040197890252\n","Epoch 27: Training Loss: 0.002630157932799193 NSE : 0.07514119148254395 WAPE : 55.58355673219236 Validation Loss: 0.000200046255486086\n","Epoch 28: Training Loss: 0.0025962697764043696 NSE : 0.10502731800079346 WAPE : 54.58030059827813 Validation Loss: 0.00019358188728801906\n","Epoch 29: Training Loss: 0.002564855212767725 NSE : 0.12610751390457153 WAPE : 53.88540576598362 Validation Loss: 0.00018902230658568442\n","Epoch 30: Training Loss: 0.0025350509604322724 NSE : 0.14661049842834473 WAPE : 53.19116132663484 Validation Loss: 0.0001845874940045178\n","Epoch 31: Training Loss: 0.0025065469089895487 NSE : 0.16769272089004517 WAPE : 52.456675908361305 Validation Loss: 0.0001800274185370654\n","Epoch 32: Training Loss: 0.0024795247918518726 NSE : 0.18516206741333008 WAPE : 51.847341101915745 Validation Loss: 0.00017624882457312196\n","Epoch 33: Training Loss: 0.0024534972890251083 NSE : 0.20182925462722778 WAPE : 51.26057826603573 Validation Loss: 0.00017264373309444636\n","Epoch 34: Training Loss: 0.002428355530355475 NSE : 0.21912741661071777 WAPE : 50.63685560025849 Validation Loss: 0.00016890214465092868\n","Epoch 35: Training Loss: 0.002404134145763237 NSE : 0.23424071073532104 WAPE : 50.08912884867941 Validation Loss: 0.0001656331296544522\n","Epoch 36: Training Loss: 0.002380683279625373 NSE : 0.2481558918952942 WAPE : 49.58078839298743 Validation Loss: 0.00016262329882010818\n","Epoch 37: Training Loss: 0.0023577905994898174 NSE : 0.26112890243530273 WAPE : 49.10424631548227 Validation Loss: 0.0001598172530066222\n","Epoch 38: Training Loss: 0.0023353955111815594 NSE : 0.27431201934814453 WAPE : 48.61062725396594 Validation Loss: 0.00015696577611379325\n","Epoch 39: Training Loss: 0.0023134977200243156 NSE : 0.28729861974716187 WAPE : 48.117429280190116 Validation Loss: 0.00015415674715768546\n","Epoch 40: Training Loss: 0.0022919934199308045 NSE : 0.29854339361190796 WAPE : 47.69915157074065 Validation Loss: 0.000151724525494501\n","Epoch 41: Training Loss: 0.0022707319858454866 NSE : 0.3110182285308838 WAPE : 47.21844447687144 Validation Loss: 0.0001490262511651963\n","Epoch 42: Training Loss: 0.0022502057299789158 NSE : 0.322861909866333 WAPE : 46.75528131579496 Validation Loss: 0.000146464430144988\n","Epoch 43: Training Loss: 0.0022301815943137626 NSE : 0.33412545919418335 WAPE : 46.31660795063684 Validation Loss: 0.00014402814849745482\n","Epoch 44: Training Loss: 0.0022106566593720345 NSE : 0.34420526027679443 WAPE : 45.92764378478664 Validation Loss: 0.00014184790779836476\n","Epoch 45: Training Loss: 0.0021914261142228497 NSE : 0.35593926906585693 WAPE : 45.458331075024496 Validation Loss: 0.00013930985005572438\n","Epoch 46: Training Loss: 0.0021726416161982343 NSE : 0.3676111698150635 WAPE : 44.98630005628401 Validation Loss: 0.0001367852237308398\n","Epoch 47: Training Loss: 0.002154395567231404 NSE : 0.37619543075561523 WAPE : 44.64852515061183 Validation Loss: 0.0001349284575553611\n","Epoch 48: Training Loss: 0.0021364019476095564 NSE : 0.3848068118095398 WAPE : 44.30435054512101 Validation Loss: 0.00013306582695804536\n","Epoch 49: Training Loss: 0.0021187324573475053 NSE : 0.39376401901245117 WAPE : 43.94153134185237 Validation Loss: 0.00013112839951645583\n","Epoch 50: Training Loss: 0.0021014274025219493 NSE : 0.40330415964126587 WAPE : 43.54495007400304 Validation Loss: 0.0001290648797294125\n","Epoch 51: Training Loss: 0.0020844889631916885 NSE : 0.412659227848053 WAPE : 43.15178753830439 Validation Loss: 0.00012704136315733194\n","Epoch 52: Training Loss: 0.0020679059907706687 NSE : 0.42091798782348633 WAPE : 42.808380062954704 Validation Loss: 0.00012525501369964331\n","Epoch 53: Training Loss: 0.002051618076620798 NSE : 0.428779661655426 WAPE : 42.481307456588354 Validation Loss: 0.00012355453509371728\n","Epoch 54: Training Loss: 0.0020356099357741186 NSE : 0.4367269277572632 WAPE : 42.148602280544495 Validation Loss: 0.00012183555372757837\n","Epoch 55: Training Loss: 0.0020199109294480877 NSE : 0.44491302967071533 WAPE : 41.80002084592775 Validation Loss: 0.00012006486940663308\n","Epoch 56: Training Loss: 0.002004501314331719 NSE : 0.4531993865966797 WAPE : 41.44182526943362 Validation Loss: 0.00011827255366370082\n","Epoch 57: Training Loss: 0.001989416195101512 NSE : 0.46097445487976074 WAPE : 41.10503429155115 Validation Loss: 0.00011659082520054653\n","Epoch 58: Training Loss: 0.001974667885406234 NSE : 0.4669263958930969 WAPE : 40.85582956369473 Validation Loss: 0.0001153034099843353\n","Epoch 59: Training Loss: 0.001960064074410184 NSE : 0.4740276336669922 WAPE : 40.54529194721811 Validation Loss: 0.00011376744078006595\n","Epoch 60: Training Loss: 0.0019457039570625057 NSE : 0.4816296696662903 WAPE : 40.2067999416314 Validation Loss: 0.00011212311801500618\n","Epoch 61: Training Loss: 0.0019316933021400473 NSE : 0.4881352186203003 WAPE : 39.920476954826874 Validation Loss: 0.00011071596236433834\n","Epoch 62: Training Loss: 0.0019178890770490398 NSE : 0.4951106905937195 WAPE : 39.6063288236643 Validation Loss: 0.00010920717613771558\n","Epoch 63: Training Loss: 0.0019043436805077363 NSE : 0.5018396377563477 WAPE : 39.301219486773256 Validation Loss: 0.00010775168630061671\n","Epoch 64: Training Loss: 0.001891033178253565 NSE : 0.5084606409072876 WAPE : 38.9968814492089 Validation Loss: 0.00010631959594320506\n","Epoch 65: Training Loss: 0.0018780119071379886 NSE : 0.5141850113868713 WAPE : 38.736670071501536 Validation Loss: 0.00010508141713216901\n","Epoch 66: Training Loss: 0.0018651908621905022 NSE : 0.5198609828948975 WAPE : 38.47630026474329 Validation Loss: 0.00010385370842413977\n","Epoch 67: Training Loss: 0.0018525761861383216 NSE : 0.5254950523376465 WAPE : 38.215788705676346 Validation Loss: 0.00010263505828334019\n","Epoch 68: Training Loss: 0.0018401826000626897 NSE : 0.5309637188911438 WAPE : 37.96093473140022 Validation Loss: 0.00010145218402612954\n","Epoch 69: Training Loss: 0.0018280134499946143 NSE : 0.5364745259284973 WAPE : 37.700790060661646 Validation Loss: 0.00010026020754594356\n","Epoch 70: Training Loss: 0.0018160527133659343 NSE : 0.5417392253875732 WAPE : 37.45153530257864 Validation Loss: 9.912146197166294e-05\n","Epoch 71: Training Loss: 0.0018042759229501826 NSE : 0.5470443964004517 WAPE : 37.197840361885305 Validation Loss: 9.797395614441484e-05\n","Epoch 72: Training Loss: 0.0017926971304405015 NSE : 0.5521700382232666 WAPE : 36.952066873736214 Validation Loss: 9.686528937891126e-05\n","Epoch 73: Training Loss: 0.0017813580816437025 NSE : 0.5561147332191467 WAPE : 36.769606637343394 Validation Loss: 9.601204510545358e-05\n","Epoch 74: Training Loss: 0.0017701310162010486 NSE : 0.5613213181495667 WAPE : 36.51584498968127 Validation Loss: 9.488587238593027e-05\n","Epoch 75: Training Loss: 0.0017591018504390377 NSE : 0.5664160251617432 WAPE : 36.26536866023222 Validation Loss: 9.37838849495165e-05\n","Epoch 76: Training Loss: 0.0017483490610175068 NSE : 0.5699119567871094 WAPE : 36.10202413958433 Validation Loss: 9.302772377850488e-05\n","Epoch 77: Training Loss: 0.0017376382502334309 NSE : 0.5746663808822632 WAPE : 35.866114944445606 Validation Loss: 9.199933992931619e-05\n","Epoch 78: Training Loss: 0.0017270881580770947 NSE : 0.5795779228210449 WAPE : 35.61852369139689 Validation Loss: 9.093698463402689e-05\n","Epoch 79: Training Loss: 0.0017168183867397602 NSE : 0.5826578736305237 WAPE : 35.47289820933481 Validation Loss: 9.027079795487225e-05\n","Epoch 80: Training Loss: 0.0017065838965208968 NSE : 0.5863194465637207 WAPE : 35.292330783181505 Validation Loss: 8.947878814069554e-05\n","Epoch 81: Training Loss: 0.0016963966445473488 NSE : 0.590349018573761 WAPE : 35.08828250401284 Validation Loss: 8.86071939021349e-05\n","Epoch 82: Training Loss: 0.001686358859842585 NSE : 0.5950517058372498 WAPE : 34.84399324591941 Validation Loss: 8.759001502767205e-05\n","Epoch 83: Training Loss: 0.0016765323925937992 NSE : 0.599064290523529 WAPE : 34.636611702903835 Validation Loss: 8.672208787174895e-05\n","Epoch 84: Training Loss: 0.0016669015485604177 NSE : 0.6025816202163696 WAPE : 34.45807675470597 Validation Loss: 8.596128463977948e-05\n","Epoch 85: Training Loss: 0.001657562104810495 NSE : 0.6053423881530762 WAPE : 34.322473994705135 Validation Loss: 8.536413224646822e-05\n","Epoch 86: Training Loss: 0.0016482527835250949 NSE : 0.6095330119132996 WAPE : 34.10136957745304 Validation Loss: 8.445771527476609e-05\n","Epoch 87: Training Loss: 0.0016391825183745823 NSE : 0.6129027605056763 WAPE : 33.927168497633986 Validation Loss: 8.372885349672288e-05\n","Epoch 88: Training Loss: 0.0016302423900924623 NSE : 0.616478443145752 WAPE : 33.739488440933066 Validation Loss: 8.295541192637756e-05\n","Epoch 89: Training Loss: 0.0016214709203268285 NSE : 0.6197444200515747 WAPE : 33.5695399303746 Validation Loss: 8.224898192565888e-05\n","Epoch 90: Training Loss: 0.0016128203897096682 NSE : 0.6230434775352478 WAPE : 33.396481207396135 Validation Loss: 8.153540693456307e-05\n","Epoch 91: Training Loss: 0.0016043420546338893 NSE : 0.625619649887085 WAPE : 33.2655937962519 Validation Loss: 8.097817044472322e-05\n","Epoch 92: Training Loss: 0.0015959249139996246 NSE : 0.6288816928863525 WAPE : 33.09237039044422 Validation Loss: 8.027259900700301e-05\n","Epoch 93: Training Loss: 0.001587635273608612 NSE : 0.6327587366104126 WAPE : 32.880619540972674 Validation Loss: 7.943400123622268e-05\n","Epoch 94: Training Loss: 0.0015795676781635848 NSE : 0.6352885961532593 WAPE : 32.74946113276771 Validation Loss: 7.888677646405995e-05\n","Epoch 95: Training Loss: 0.0015716235939180478 NSE : 0.6367924213409424 WAPE : 32.68229138437806 Validation Loss: 7.856152660679072e-05\n","Epoch 96: Training Loss: 0.0015636419875590946 NSE : 0.6404513120651245 WAPE : 32.47987951053762 Validation Loss: 7.777012069709599e-05\n","Epoch 97: Training Loss: 0.001555804960844398 NSE : 0.6449074745178223 WAPE : 32.22605532509225 Validation Loss: 7.680623093619943e-05\n","Epoch 98: Training Loss: 0.0015482144426641753 NSE : 0.6478612422943115 WAPE : 32.06440557837026 Validation Loss: 7.61673363740556e-05\n","Epoch 99: Training Loss: 0.001540796354674967 NSE : 0.648349404335022 WAPE : 32.05984865856455 Validation Loss: 7.606175495311618e-05\n","Epoch 100: Training Loss: 0.0015333050287154038 NSE : 0.6506732702255249 WAPE : 31.940639136144753 Validation Loss: 7.555911724921316e-05\n","Epoch 101: Training Loss: 0.0015259145793606876 NSE : 0.654914915561676 WAPE : 31.69479060265577 Validation Loss: 7.464164809789509e-05\n","Epoch 102: Training Loss: 0.0015187575772870332 NSE : 0.6580395698547363 WAPE : 31.5202017885806 Validation Loss: 7.396578439511359e-05\n","Epoch 103: Training Loss: 0.001511825981651782 NSE : 0.6588607430458069 WAPE : 31.49319797377582 Validation Loss: 7.378816371783614e-05\n","Epoch 104: Training Loss: 0.0015048981222207658 NSE : 0.6601486802101135 WAPE : 31.43532550916179 Validation Loss: 7.35095891286619e-05\n","Epoch 105: Training Loss: 0.0014978889730627998 NSE : 0.664537787437439 WAPE : 31.17540180525734 Validation Loss: 7.256022945512086e-05\n","Epoch 106: Training Loss: 0.001491153519054933 NSE : 0.6675984263420105 WAPE : 31.00059410894082 Validation Loss: 7.189820462372154e-05\n","Epoch 107: Training Loss: 0.0014846084295641049 NSE : 0.6690738201141357 WAPE : 30.928206624835834 Validation Loss: 7.157906657084823e-05\n","Epoch 108: Training Loss: 0.0014780922238060157 NSE : 0.6706552505493164 WAPE : 30.849131767109295 Validation Loss: 7.123701652744785e-05\n","Epoch 109: Training Loss: 0.001471597120143997 NSE : 0.6733160018920898 WAPE : 30.697821600550334 Validation Loss: 7.066149555612355e-05\n","Epoch 110: Training Loss: 0.0014651649735242245 NSE : 0.6769472360610962 WAPE : 30.479989993954682 Validation Loss: 6.987607048358768e-05\n","Epoch 111: Training Loss: 0.0014590402925023227 NSE : 0.6781722903251648 WAPE : 30.42219674386609 Validation Loss: 6.961109465919435e-05\n","Epoch 112: Training Loss: 0.0014528591154885362 NSE : 0.6804109811782837 WAPE : 30.29668758208084 Validation Loss: 6.91268578520976e-05\n","Epoch 113: Training Loss: 0.001446743127416994 NSE : 0.6832020878791809 WAPE : 30.132125659252466 Validation Loss: 6.852314982097596e-05\n","Epoch 114: Training Loss: 0.0014407922253667493 NSE : 0.6854676008224487 WAPE : 30.00139876175189 Validation Loss: 6.80331140756607e-05\n","Epoch 115: Training Loss: 0.00143503206436435 NSE : 0.6858788728713989 WAPE : 29.995397219153237 Validation Loss: 6.794416549382731e-05\n","Epoch 116: Training Loss: 0.0014291729121396202 NSE : 0.6881113052368164 WAPE : 29.866331742094182 Validation Loss: 6.746128929080442e-05\n","Epoch 117: Training Loss: 0.0014233492784114787 NSE : 0.6912078857421875 WAPE : 29.676533739134058 Validation Loss: 6.679149373667315e-05\n","Epoch 118: Training Loss: 0.0014177386046867468 NSE : 0.693812370300293 WAPE : 29.51796085134769 Validation Loss: 6.622815999435261e-05\n","Epoch 119: Training Loss: 0.0014123351875241497 NSE : 0.6938165426254272 WAPE : 29.539575993829605 Validation Loss: 6.622725049965084e-05\n","Epoch 120: Training Loss: 0.00140684690541093 NSE : 0.6951713562011719 WAPE : 29.468303766859144 Validation Loss: 6.593420403078198e-05\n","Epoch 121: Training Loss: 0.001401288311171811 NSE : 0.6989820003509521 WAPE : 29.22467324008255 Validation Loss: 6.510994717245921e-05\n","Epoch 122: Training Loss: 0.001396009673953813 NSE : 0.7008527517318726 WAPE : 29.11571991411478 Validation Loss: 6.470533116953447e-05\n","Epoch 123: Training Loss: 0.001390885209730186 NSE : 0.7011117935180664 WAPE : 29.118805111421487 Validation Loss: 6.464929174399003e-05\n","Epoch 124: Training Loss: 0.0013856827390554827 NSE : 0.7032479047775269 WAPE : 28.98849930166142 Validation Loss: 6.418726115953177e-05\n","Epoch 125: Training Loss: 0.0013805021635562298 NSE : 0.7057822942733765 WAPE : 28.831039586416797 Validation Loss: 6.363908323692158e-05\n","Epoch 126: Training Loss: 0.001375523966999026 NSE : 0.7076834440231323 WAPE : 28.716572512559672 Validation Loss: 6.322784611256793e-05\n","Epoch 127: Training Loss: 0.001370593044157431 NSE : 0.7091884613037109 WAPE : 28.630939525963605 Validation Loss: 6.290231976890936e-05\n","Epoch 128: Training Loss: 0.0013657604713444016 NSE : 0.7104114294052124 WAPE : 28.56438681703529 Validation Loss: 6.26377877779305e-05\n","Epoch 129: Training Loss: 0.0013609204015665455 NSE : 0.7123857140541077 WAPE : 28.442890496341537 Validation Loss: 6.221074727363884e-05\n","Epoch 130: Training Loss: 0.0013561351424868917 NSE : 0.7146673202514648 WAPE : 28.299868670655187 Validation Loss: 6.171724089654163e-05\n","Epoch 131: Training Loss: 0.0013515181435650447 NSE : 0.7158668041229248 WAPE : 28.234623001396674 Validation Loss: 6.145779479993507e-05\n","Epoch 132: Training Loss: 0.0013469002515194006 NSE : 0.717271089553833 WAPE : 28.153259260803402 Validation Loss: 6.115404539741576e-05\n","Epoch 133: Training Loss: 0.0013423661594060832 NSE : 0.718874990940094 WAPE : 28.055571074190656 Validation Loss: 6.0807135014329106e-05\n","Epoch 134: Training Loss: 0.001337850037998578 NSE : 0.7212697267532349 WAPE : 27.901557190802777 Validation Loss: 6.028915231581777e-05\n","Epoch 135: Training Loss: 0.0013334744744497584 NSE : 0.7226252555847168 WAPE : 27.822325988618125 Validation Loss: 5.999594577588141e-05\n","Epoch 136: Training Loss: 0.00132915213907836 NSE : 0.7235090732574463 WAPE : 27.778078422380187 Validation Loss: 5.980476635158993e-05\n","Epoch 137: Training Loss: 0.0013248974491943954 NSE : 0.7242133617401123 WAPE : 27.74458735486023 Validation Loss: 5.9652440540958196e-05\n","Epoch 138: Training Loss: 0.001320580792707915 NSE : 0.7264295816421509 WAPE : 27.599754018052575 Validation Loss: 5.917307862546295e-05\n","Epoch 139: Training Loss: 0.0013163762823751313 NSE : 0.7289083003997803 WAPE : 27.433672427091366 Validation Loss: 5.8636935136746615e-05\n","Epoch 140: Training Loss: 0.0013123078060743865 NSE : 0.7299816012382507 WAPE : 27.37290446311313 Validation Loss: 5.8404781157150865e-05\n","Epoch 141: Training Loss: 0.001308293728470744 NSE : 0.7298296093940735 WAPE : 27.405624231306412 Validation Loss: 5.843765757163055e-05\n","Epoch 142: Training Loss: 0.001304245570281637 NSE : 0.731939435005188 WAPE : 27.26634008046528 Validation Loss: 5.798129132017493e-05\n","Epoch 143: Training Loss: 0.0013002867399336537 NSE : 0.7336324453353882 WAPE : 27.15572533405599 Validation Loss: 5.7615088735474274e-05\n","Epoch 144: Training Loss: 0.0012963940125700901 NSE : 0.7349046468734741 WAPE : 27.07963561318297 Validation Loss: 5.733993020839989e-05\n","Epoch 145: Training Loss: 0.0012925656910738326 NSE : 0.735791802406311 WAPE : 27.03131892184862 Validation Loss: 5.7148026826325804e-05\n","Epoch 146: Training Loss: 0.0012887712509837002 NSE : 0.7370312213897705 WAPE : 26.956173521502574 Validation Loss: 5.687994053005241e-05\n","Epoch 147: Training Loss: 0.0012849886197727756 NSE : 0.739255428314209 WAPE : 26.804540243063517 Validation Loss: 5.639885785058141e-05\n","Epoch 148: Training Loss: 0.0012813845123673673 NSE : 0.7392982244491577 WAPE : 26.819887015071608 Validation Loss: 5.638959919451736e-05\n","Epoch 149: Training Loss: 0.001277707944609574 NSE : 0.7402691841125488 WAPE : 26.763607179337516 Validation Loss: 5.617957504000515e-05\n","Epoch 150: Training Loss: 0.0012740055135509465 NSE : 0.7426702976226807 WAPE : 26.59923703904442 Validation Loss: 5.566022446146235e-05\n","Epoch 151: Training Loss: 0.0012704397513516597 NSE : 0.7450275421142578 WAPE : 26.434833545266933 Validation Loss: 5.515034354175441e-05\n","Epoch 152: Training Loss: 0.0012670946653088322 NSE : 0.7449003458023071 WAPE : 26.46204165016364 Validation Loss: 5.517786121345125e-05\n","Epoch 153: Training Loss: 0.0012636639821721474 NSE : 0.7448526620864868 WAPE : 26.484534406203746 Validation Loss: 5.5188178521348163e-05\n","Epoch 154: Training Loss: 0.0012601580456248485 NSE : 0.7466123104095459 WAPE : 26.365270684371804 Validation Loss: 5.480756226461381e-05\n","Epoch 155: Training Loss: 0.0012567272005981067 NSE : 0.7492252588272095 WAPE : 26.17762189656251 Validation Loss: 5.424238770501688e-05\n","Epoch 156: Training Loss: 0.0012534696825241554 NSE : 0.750063955783844 WAPE : 26.129644993850455 Validation Loss: 5.4060976253822446e-05\n","Epoch 157: Training Loss: 0.0012502677027441678 NSE : 0.7499127388000488 WAPE : 26.156853098747156 Validation Loss: 5.409369259723462e-05\n","Epoch 158: Training Loss: 0.0012470619758460089 NSE : 0.7500834465026855 WAPE : 26.158495757853707 Validation Loss: 5.4056763474363834e-05\n","Epoch 159: Training Loss: 0.0012437715367923374 NSE : 0.7525820732116699 WAPE : 25.976552500469037 Validation Loss: 5.35163126187399e-05\n","Epoch 160: Training Loss: 0.0012406320784066338 NSE : 0.7538168430328369 WAPE : 25.893706614412874 Validation Loss: 5.324922676663846e-05\n","Epoch 161: Training Loss: 0.0012375846135910251 NSE : 0.7539823055267334 WAPE : 25.895989243501283 Validation Loss: 5.3213443607091904e-05\n","Epoch 162: Training Loss: 0.001234533011484018 NSE : 0.7545117735862732 WAPE : 25.868207875591505 Validation Loss: 5.309891639626585e-05\n","Epoch 163: Training Loss: 0.0012314505329413805 NSE : 0.7559502124786377 WAPE : 25.769033374330323 Validation Loss: 5.278778917272575e-05\n","Epoch 164: Training Loss: 0.0012284605045351782 NSE : 0.756697416305542 WAPE : 25.72548623126472 Validation Loss: 5.2626168326241896e-05\n","Epoch 165: Training Loss: 0.0012255130241101142 NSE : 0.7576027512550354 WAPE : 25.667851410227012 Validation Loss: 5.243034320301376e-05\n","Epoch 166: Training Loss: 0.0012225690834384295 NSE : 0.7590237855911255 WAPE : 25.568234975297578 Validation Loss: 5.2122970373602584e-05\n","Epoch 167: Training Loss: 0.001219733033394732 NSE : 0.7591385841369629 WAPE : 25.573340143003065 Validation Loss: 5.2098152082180604e-05\n","Epoch 168: Training Loss: 0.0012168786724942038 NSE : 0.7597823739051819 WAPE : 25.535811219278315 Validation Loss: 5.1958890253445134e-05\n","Epoch 169: Training Loss: 0.001214023425745836 NSE : 0.7612366676330566 WAPE : 25.43407266890413 Validation Loss: 5.164432150195353e-05\n","Epoch 170: Training Loss: 0.0012112654958400526 NSE : 0.7620626091957092 WAPE : 25.38141794000542 Validation Loss: 5.1465682190610096e-05\n","Epoch 171: Training Loss: 0.0012085371754437801 NSE : 0.7627066969871521 WAPE : 25.344099560150923 Validation Loss: 5.132636215421371e-05\n","Epoch 172: Training Loss: 0.0012058228803653037 NSE : 0.7637155652046204 WAPE : 25.276775551895938 Validation Loss: 5.1108148909406736e-05\n","Epoch 173: Training Loss: 0.0012031618534820154 NSE : 0.7643085718154907 WAPE : 25.244003668883288 Validation Loss: 5.0979870138689876e-05\n","Epoch 174: Training Loss: 0.0012004780819552252 NSE : 0.7655987739562988 WAPE : 25.154580892622626 Validation Loss: 5.070081169833429e-05\n","Epoch 175: Training Loss: 0.0011978699740211596 NSE : 0.7667185068130493 WAPE : 25.077038210585563 Validation Loss: 5.045861325925216e-05\n","Epoch 176: Training Loss: 0.0011953444545724778 NSE : 0.7665716409683228 WAPE : 25.10469658752163 Validation Loss: 5.049036553828046e-05\n","Epoch 177: Training Loss: 0.0011927608275072998 NSE : 0.7676114439964294 WAPE : 25.033782910508435 Validation Loss: 5.0265469326404855e-05\n","Epoch 178: Training Loss: 0.001190191219393455 NSE : 0.7694718837738037 WAPE : 24.894736403243627 Validation Loss: 4.986306157661602e-05\n","Epoch 179: Training Loss: 0.001187778028906905 NSE : 0.7692022919654846 WAPE : 24.931243876508724 Validation Loss: 4.9921371100936085e-05\n","Epoch 180: Training Loss: 0.0011853168634843314 NSE : 0.7700473070144653 WAPE : 24.87490567217694 Validation Loss: 4.97385990456678e-05\n","Epoch 181: Training Loss: 0.0011828742508441792 NSE : 0.7710316181182861 WAPE : 24.807104292176525 Validation Loss: 4.952568269800395e-05\n","Epoch 182: Training Loss: 0.0011804678915723343 NSE : 0.7716329097747803 WAPE : 24.772258239352944 Validation Loss: 4.939562131767161e-05\n","Epoch 183: Training Loss: 0.0011780824652305455 NSE : 0.7727702260017395 WAPE : 24.690112776469118 Validation Loss: 4.914962119073607e-05\n","Epoch 184: Training Loss: 0.0011757581414713059 NSE : 0.7735105752944946 WAPE : 24.641241583456672 Validation Loss: 4.8989499191520736e-05\n","Epoch 185: Training Loss: 0.0011734706777133397 NSE : 0.7734755277633667 WAPE : 24.658183068937483 Validation Loss: 4.89970734633971e-05\n","Epoch 186: Training Loss: 0.0011711433216987643 NSE : 0.774661660194397 WAPE : 24.57186216672573 Validation Loss: 4.874051592196338e-05\n","Epoch 187: Training Loss: 0.0011688611148201744 NSE : 0.7755658626556396 WAPE : 24.509245168956244 Validation Loss: 4.854493454331532e-05\n","Epoch 188: Training Loss: 0.0011666313494060887 NSE : 0.7763948440551758 WAPE : 24.453990952867358 Validation Loss: 4.836562948185019e-05\n","Epoch 189: Training Loss: 0.0011644491105471388 NSE : 0.7768020629882812 WAPE : 24.431960976423255 Validation Loss: 4.8277550376951694e-05\n","Epoch 190: Training Loss: 0.0011622506499406882 NSE : 0.77750164270401 WAPE : 24.38546413458131 Validation Loss: 4.81262213725131e-05\n","Epoch 191: Training Loss: 0.0011600742809605435 NSE : 0.7782772779464722 WAPE : 24.33459798628338 Validation Loss: 4.795845961780287e-05\n","Epoch 192: Training Loss: 0.001157925245934166 NSE : 0.7790858745574951 WAPE : 24.278693377248757 Validation Loss: 4.778355287271552e-05\n","Epoch 193: Training Loss: 0.0011558088981473702 NSE : 0.780082106590271 WAPE : 24.206854141043546 Validation Loss: 4.7568075387971476e-05\n","Epoch 194: Training Loss: 0.0011537433965713717 NSE : 0.7801278829574585 WAPE : 24.217068645640076 Validation Loss: 4.755817280965857e-05\n","Epoch 195: Training Loss: 0.0011516577778820647 NSE : 0.7810260057449341 WAPE : 24.152988263742678 Validation Loss: 4.736390837933868e-05\n","Epoch 196: Training Loss: 0.0011496039796838886 NSE : 0.7819873094558716 WAPE : 24.082866731983906 Validation Loss: 4.715597606264055e-05\n","Epoch 197: Training Loss: 0.001147608139035583 NSE : 0.782158613204956 WAPE : 24.08101561359988 Validation Loss: 4.711893052444793e-05\n","Epoch 198: Training Loss: 0.0011456143911345862 NSE : 0.7828328609466553 WAPE : 24.03453336390736 Validation Loss: 4.697309486800805e-05\n","Epoch 199: Training Loss: 0.001143639954534592 NSE : 0.7837244272232056 WAPE : 23.96952742281795 Validation Loss: 4.6780234697507694e-05\n","Epoch 200: Training Loss: 0.0011416974439271144 NSE : 0.7839756608009338 WAPE : 23.96095557732797 Validation Loss: 4.672590148402378e-05\n","Epoch 201: Training Loss: 0.0011397546222724486 NSE : 0.7845953702926636 WAPE : 23.919814054324487 Validation Loss: 4.6591860154876485e-05\n","Epoch 202: Training Loss: 0.0011378128228898277 NSE : 0.7858737707138062 WAPE : 23.821300368972924 Validation Loss: 4.631534102372825e-05\n","Epoch 203: Training Loss: 0.0011359749323673896 NSE : 0.7858548164367676 WAPE : 23.835104542327656 Validation Loss: 4.631943374988623e-05\n","Epoch 204: Training Loss: 0.0011341116951371077 NSE : 0.7862452268600464 WAPE : 23.81184048696087 Validation Loss: 4.6235003537731245e-05\n","Epoch 205: Training Loss: 0.0011322458849463146 NSE : 0.7870500683784485 WAPE : 23.753303037251673 Validation Loss: 4.606090806191787e-05\n","Epoch 206: Training Loss: 0.0011303974888505763 NSE : 0.7875759601593018 WAPE : 23.720364386817035 Validation Loss: 4.594715210259892e-05\n","Epoch 207: Training Loss: 0.0011285857644907082 NSE : 0.7883560061454773 WAPE : 23.66405119759855 Validation Loss: 4.5778437197441235e-05\n","Epoch 208: Training Loss: 0.00112680546772026 NSE : 0.7891724705696106 WAPE : 23.603356194367432 Validation Loss: 4.560183151625097e-05\n","Epoch 209: Training Loss: 0.0011250816401116026 NSE : 0.7890300750732422 WAPE : 23.62739571824644 Validation Loss: 4.563263792078942e-05\n","Epoch 210: Training Loss: 0.0011233012942284404 NSE : 0.7892436981201172 WAPE : 23.6189281023952 Validation Loss: 4.558642831398174e-05\n","Epoch 211: Training Loss: 0.001121530820910266 NSE : 0.7905154228210449 WAPE : 23.519132392487126 Validation Loss: 4.531135346041992e-05\n","Epoch 212: Training Loss: 0.001119835269946634 NSE : 0.7910714149475098 WAPE : 23.482931354359927 Validation Loss: 4.5191089157015085e-05\n","Epoch 213: Training Loss: 0.0011181427180417813 NSE : 0.7919053435325623 WAPE : 23.42056242313064 Validation Loss: 4.501072544371709e-05\n","Epoch 214: Training Loss: 0.0011165168120896851 NSE : 0.7914573550224304 WAPE : 23.468712347043006 Validation Loss: 4.5107615733286366e-05\n","Epoch 215: Training Loss: 0.0011148222979500133 NSE : 0.7919674515724182 WAPE : 23.43611348523066 Validation Loss: 4.4997286750003695e-05\n","Epoch 216: Training Loss: 0.0011131404821753677 NSE : 0.7931889891624451 WAPE : 23.34015967980655 Validation Loss: 4.473306762520224e-05\n","Epoch 217: Training Loss: 0.0011115183488072944 NSE : 0.7936662435531616 WAPE : 23.308615621938255 Validation Loss: 4.462983997655101e-05\n","Epoch 218: Training Loss: 0.0011099202579316625 NSE : 0.7941271066665649 WAPE : 23.280288090721477 Validation Loss: 4.453014844330028e-05\n","Epoch 219: Training Loss: 0.001108344818931073 NSE : 0.7945887446403503 WAPE : 23.250184486460572 Validation Loss: 4.443030047696084e-05\n","Epoch 220: Training Loss: 0.0011067654168073204 NSE : 0.7946557998657227 WAPE : 23.25342811281816 Validation Loss: 4.441579949343577e-05\n","Epoch 221: Training Loss: 0.0011051829810639902 NSE : 0.795253336429596 WAPE : 23.21291613683267 Validation Loss: 4.428655302035622e-05\n","Epoch 222: Training Loss: 0.0011035835350412526 NSE : 0.7967393398284912 WAPE : 23.093510662692044 Validation Loss: 4.396513759274967e-05\n","Epoch 223: Training Loss: 0.0011021027708011388 NSE : 0.7968242168426514 WAPE : 23.094953200892206 Validation Loss: 4.394676579977386e-05\n","Epoch 224: Training Loss: 0.0011006344875568175 NSE : 0.7963286638259888 WAPE : 23.14894415375956 Validation Loss: 4.405395520734601e-05\n","Epoch 225: Training Loss: 0.0010990734817823977 NSE : 0.7973724603652954 WAPE : 23.06784932563424 Validation Loss: 4.382817496662028e-05\n","Epoch 226: Training Loss: 0.0010975438517561997 NSE : 0.7983691692352295 WAPE : 22.990731900523233 Validation Loss: 4.3612602894427255e-05\n","Epoch 227: Training Loss: 0.0010961007797050115 NSE : 0.7991979122161865 WAPE : 22.92667027996081 Validation Loss: 4.3433334212750196e-05\n","Epoch 228: Training Loss: 0.0010947241876237968 NSE : 0.798814058303833 WAPE : 22.96898334410373 Validation Loss: 4.351636744104326e-05\n","Epoch 229: Training Loss: 0.001093259658318857 NSE : 0.7989171147346497 WAPE : 22.970688540993518 Validation Loss: 4.34940739069134e-05\n","Epoch 230: Training Loss: 0.0010917745498773002 NSE : 0.7998684048652649 WAPE : 22.897888307519125 Validation Loss: 4.32883134635631e-05\n","Epoch 231: Training Loss: 0.0010903463785325584 NSE : 0.8009229898452759 WAPE : 22.8136248983761 Validation Loss: 4.306019764044322e-05\n","Epoch 232: Training Loss: 0.0010889996865444118 NSE : 0.8010008335113525 WAPE : 22.815227950219924 Validation Loss: 4.3043368350481614e-05\n","Epoch 233: Training Loss: 0.0010876359838221106 NSE : 0.8006179928779602 WAPE : 22.858381105251087 Validation Loss: 4.312617966206744e-05\n","Epoch 234: Training Loss: 0.001086231834051432 NSE : 0.8015875816345215 WAPE : 22.780617456379897 Validation Loss: 4.291645745979622e-05\n","Epoch 235: Training Loss: 0.001084864371932781 NSE : 0.8024498224258423 WAPE : 22.71272643889016 Validation Loss: 4.2729952838271856e-05\n","Epoch 236: Training Loss: 0.001083538306374976 NSE : 0.8026845455169678 WAPE : 22.700089637489317 Validation Loss: 4.267917756806128e-05\n","Epoch 237: Training Loss: 0.0010822369199559034 NSE : 0.8027521967887878 WAPE : 22.703068520564507 Validation Loss: 4.266455289325677e-05\n","Epoch 238: Training Loss: 0.0010808946612996806 NSE : 0.8031216859817505 WAPE : 22.679366700715015 Validation Loss: 4.2584626498864964e-05\n","Epoch 239: Training Loss: 0.0010795820430757885 NSE : 0.8039122223854065 WAPE : 22.61815263388297 Validation Loss: 4.2413637856952846e-05\n","Epoch 240: Training Loss: 0.0010783091402117861 NSE : 0.8037079572677612 WAPE : 22.645583790206583 Validation Loss: 4.245781747158617e-05\n","Epoch 241: Training Loss: 0.0010770047456389875 NSE : 0.8045822381973267 WAPE : 22.575722832544663 Validation Loss: 4.226870441925712e-05\n","Epoch 242: Training Loss: 0.0010757566656138806 NSE : 0.8049498796463013 WAPE : 22.55014487919785 Validation Loss: 4.2189189116470516e-05\n","Epoch 243: Training Loss: 0.0010745198683252966 NSE : 0.8050152063369751 WAPE : 22.553021617227074 Validation Loss: 4.217506284476258e-05\n","Epoch 244: Training Loss: 0.0010732464038483158 NSE : 0.8057132363319397 WAPE : 22.499189093410603 Validation Loss: 4.202407944831066e-05\n","Epoch 245: Training Loss: 0.0010720253940235125 NSE : 0.8060140013694763 WAPE : 22.481620145504575 Validation Loss: 4.195903238723986e-05\n","Epoch 246: Training Loss: 0.001070810496003105 NSE : 0.805980920791626 WAPE : 22.491557399262057 Validation Loss: 4.196617373963818e-05\n","Epoch 247: Training Loss: 0.001069598698450136 NSE : 0.806673526763916 WAPE : 22.43638239769861 Validation Loss: 4.1816372686298564e-05\n","Epoch 248: Training Loss: 0.001068418993781961 NSE : 0.8068544864654541 WAPE : 22.428304600696254 Validation Loss: 4.177723894827068e-05\n","Epoch 249: Training Loss: 0.0010672328176042356 NSE : 0.8071674108505249 WAPE : 22.408173688269997 Validation Loss: 4.170955071458593e-05\n","Epoch 250: Training Loss: 0.0010660135571924911 NSE : 0.8081045746803284 WAPE : 22.332350795272145 Validation Loss: 4.150683889747597e-05\n","Epoch 251: Training Loss: 0.001064894739556621 NSE : 0.8080592751502991 WAPE : 22.345319046924182 Validation Loss: 4.151663961238228e-05\n","Epoch 252: Training Loss: 0.0010637601235430338 NSE : 0.80800461769104 WAPE : 22.357782827124723 Validation Loss: 4.1528452129568905e-05\n","Epoch 253: Training Loss: 0.0010625878871906025 NSE : 0.8085382580757141 WAPE : 22.316903962810866 Validation Loss: 4.1413029975956306e-05\n","Epoch 254: Training Loss: 0.0010614204443299968 NSE : 0.8092808723449707 WAPE : 22.25761397510996 Validation Loss: 4.125239865970798e-05\n","Epoch 255: Training Loss: 0.0010603330260892108 NSE : 0.8094044923782349 WAPE : 22.25310708553084 Validation Loss: 4.122565587749705e-05\n","Epoch 256: Training Loss: 0.001059250091202557 NSE : 0.8094749450683594 WAPE : 22.254893581538845 Validation Loss: 4.121041274629533e-05\n","Epoch 257: Training Loss: 0.0010581406208984845 NSE : 0.8097589612007141 WAPE : 22.236157261678933 Validation Loss: 4.114899638807401e-05\n","Epoch 258: Training Loss: 0.0010570141321295523 NSE : 0.8103736639022827 WAPE : 22.187090116945654 Validation Loss: 4.1016031900653616e-05\n","Epoch 259: Training Loss: 0.0010559564952927758 NSE : 0.8107110857963562 WAPE : 22.164561922828373 Validation Loss: 4.094304676982574e-05\n","Epoch 260: Training Loss: 0.0010549009748501703 NSE : 0.8109099268913269 WAPE : 22.15343645118926 Validation Loss: 4.0900038584368303e-05\n","Epoch 261: Training Loss: 0.001053842896453716 NSE : 0.8107500076293945 WAPE : 22.175091200933895 Validation Loss: 4.093463212484494e-05\n","Epoch 262: Training Loss: 0.0010527527183512575 NSE : 0.8117081522941589 WAPE : 22.096128911217193 Validation Loss: 4.072738374816254e-05\n","Epoch 263: Training Loss: 0.0010517355767660774 NSE : 0.8120246529579163 WAPE : 22.073740384815828 Validation Loss: 4.0658924262970686e-05\n","Epoch 264: Training Loss: 0.001050723582466162 NSE : 0.8118262887001038 WAPE : 22.098988972504223 Validation Loss: 4.070183786097914e-05\n","Epoch 265: Training Loss: 0.0010496823229004804 NSE : 0.8122904300689697 WAPE : 22.06528944570678 Validation Loss: 4.060144419781864e-05\n","Epoch 266: Training Loss: 0.0010486347914593352 NSE : 0.812949001789093 WAPE : 22.011782118363175 Validation Loss: 4.0458980947732925e-05\n","Epoch 267: Training Loss: 0.001047653621299105 NSE : 0.8129833936691284 WAPE : 22.01607637947927 Validation Loss: 4.045154491905123e-05\n","Epoch 268: Training Loss: 0.0010466800649737706 NSE : 0.813330352306366 WAPE : 21.9905276104313 Validation Loss: 4.0376497054239735e-05\n","Epoch 269: Training Loss: 0.0010456855497977813 NSE : 0.8135391473770142 WAPE : 21.977984615705324 Validation Loss: 4.03313351853285e-05\n","Epoch 270: Training Loss: 0.0010446889627928613 NSE : 0.813965916633606 WAPE : 21.946701131933878 Validation Loss: 4.023903238703497e-05\n","Epoch 271: Training Loss: 0.0010437484902467986 NSE : 0.8142188191413879 WAPE : 21.92951991828396 Validation Loss: 4.0184331737691537e-05\n","Epoch 272: Training Loss: 0.0010427955626255425 NSE : 0.814170777797699 WAPE : 21.94133122094599 Validation Loss: 4.01947254431434e-05\n","Epoch 273: Training Loss: 0.0010418162528367247 NSE : 0.8145495653152466 WAPE : 21.912834837714453 Validation Loss: 4.011278724647127e-05\n","Epoch 274: Training Loss: 0.0010408441899016907 NSE : 0.8151797652244568 WAPE : 21.862283462925518 Validation Loss: 3.997647945652716e-05\n","Epoch 275: Training Loss: 0.0010399468965260894 NSE : 0.8154586553573608 WAPE : 21.842546538533696 Validation Loss: 3.9916154491947964e-05\n","Epoch 276: Training Loss: 0.0010390524289505265 NSE : 0.8153975605964661 WAPE : 21.85480811323508 Validation Loss: 3.992936035501771e-05\n","Epoch 277: Training Loss: 0.0010381212869106093 NSE : 0.8155039548873901 WAPE : 21.85146025723875 Validation Loss: 3.9906357415020466e-05\n","Epoch 278: Training Loss: 0.0010371696339461778 NSE : 0.8162209391593933 WAPE : 21.79194513351817 Validation Loss: 3.975126674049534e-05\n","Epoch 279: Training Loss: 0.0010362909947616572 NSE : 0.8165404796600342 WAPE : 21.769685851868836 Validation Loss: 3.968216333305463e-05\n","Epoch 280: Training Loss: 0.0010354243809160835 NSE : 0.8165227174758911 WAPE : 21.77694857309625 Validation Loss: 3.96859941247385e-05\n","Epoch 281: Training Loss: 0.0010345307655370561 NSE : 0.8163511157035828 WAPE : 21.79967480352713 Validation Loss: 3.972311606048606e-05\n","Epoch 282: Training Loss: 0.001033601439303311 NSE : 0.8175386786460876 WAPE : 21.695165829355233 Validation Loss: 3.946624929085374e-05\n","Epoch 283: Training Loss: 0.0010327895465707115 NSE : 0.8176800608634949 WAPE : 21.688878697546436 Validation Loss: 3.943566480302252e-05\n","Epoch 284: Training Loss: 0.0010319625325792003 NSE : 0.8170598745346069 WAPE : 21.752934064330535 Validation Loss: 3.956981163355522e-05\n","Epoch 285: Training Loss: 0.0010310608381587372 NSE : 0.8174409866333008 WAPE : 21.723195263805213 Validation Loss: 3.948737503378652e-05\n","Epoch 286: Training Loss: 0.0010301638667442603 NSE : 0.8184648752212524 WAPE : 21.63486689875133 Validation Loss: 3.92659057979472e-05\n","Epoch 287: Training Loss: 0.0010293502650711162 NSE : 0.8185586333274841 WAPE : 21.632638469075065 Validation Loss: 3.924563861801289e-05\n","Epoch 288: Training Loss: 0.0010285618418492959 NSE : 0.8184589743614197 WAPE : 21.648241645994453 Validation Loss: 3.926719364244491e-05\n","Epoch 289: Training Loss: 0.001027743385748181 NSE : 0.8185446262359619 WAPE : 21.64497925830189 Validation Loss: 3.9248661778401583e-05\n","Epoch 290: Training Loss: 0.0010268806645399309 NSE : 0.819111704826355 WAPE : 21.598265618811364 Validation Loss: 3.9126007322920486e-05\n","Epoch 291: Training Loss: 0.0010260896724503255 NSE : 0.8193396329879761 WAPE : 21.58308978341081 Validation Loss: 3.907670543412678e-05\n","Epoch 292: Training Loss: 0.0010252877009406802 NSE : 0.8193359375 WAPE : 21.589535344270498 Validation Loss: 3.907749123754911e-05\n","Epoch 293: Training Loss: 0.0010244863324260223 NSE : 0.8193753957748413 WAPE : 21.59226199161994 Validation Loss: 3.906896017724648e-05\n","Epoch 294: Training Loss: 0.0010236671546408616 NSE : 0.8203251957893372 WAPE : 21.508919972483376 Validation Loss: 3.886352715198882e-05\n","Epoch 295: Training Loss: 0.001022920669583982 NSE : 0.8203229308128357 WAPE : 21.514729732546748 Validation Loss: 3.8864007365191355e-05\n","Epoch 296: Training Loss: 0.0010221575516879966 NSE : 0.8201771974563599 WAPE : 21.535044089137187 Validation Loss: 3.8895541365491226e-05\n","Epoch 297: Training Loss: 0.001021370495436713 NSE : 0.8204079866409302 WAPE : 21.518788434679283 Validation Loss: 3.884561738232151e-05\n","Epoch 298: Training Loss: 0.001020587873426848 NSE : 0.8207277059555054 WAPE : 21.494765587542474 Validation Loss: 3.87764630431775e-05\n","Epoch 299: Training Loss: 0.0010198136696999427 NSE : 0.8212073445320129 WAPE : 21.455024910883658 Validation Loss: 3.867271516355686e-05\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:25:08,759] Trial 2 finished with value: 3.863331221509725e-05 and parameters: {'lr': 0.0003414812182280687, 'weight_decay': 0.00020763495431088736}. Best is trial 1 with value: 2.6064595658681355e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 300: Training Loss: 0.0010190999687438307 NSE : 0.8213895559310913 WAPE : 21.443301161118175 Validation Loss: 3.863331221509725e-05\n","Epoch 1: Training Loss: 0.007711579091846943 NSE : -6.362849712371826 WAPE : 164.36398657522253 Validation Loss: 0.0015925789484754205\n","Epoch 2: Training Loss: 0.005182270324439742 NSE : -13.121689796447754 WAPE : 231.53688686915012 Validation Loss: 0.0030545105691999197\n","Epoch 3: Training Loss: 0.004536012260359712 NSE : -2.407850742340088 WAPE : 110.12219882845886 Validation Loss: 0.0007371155079454184\n","Epoch 4: Training Loss: 0.004271976687959977 NSE : -1.6833553314208984 WAPE : 97.23711408976257 Validation Loss: 0.000580407737288624\n","Epoch 5: Training Loss: 0.0040226804703706875 NSE : -3.499088764190674 WAPE : 129.17926247107627 Validation Loss: 0.0009731495520099998\n","Epoch 6: Training Loss: 0.003779532857151935 NSE : -1.0180258750915527 WAPE : 83.90785266098267 Validation Loss: 0.00043649744475260377\n","Epoch 7: Training Loss: 0.0036950312824046705 NSE : -0.8815416097640991 WAPE : 80.88987096370724 Validation Loss: 0.00040697603253647685\n","Epoch 8: Training Loss: 0.003592585118894931 NSE : -0.6966438293457031 WAPE : 76.39209522419796 Validation Loss: 0.0003669827710837126\n","Epoch 9: Training Loss: 0.0035100207351206336 NSE : -0.40431249141693115 WAPE : 68.44307602509849 Validation Loss: 0.000303751730825752\n","Epoch 10: Training Loss: 0.0034293644966965076 NSE : -0.5348221063613892 WAPE : 72.35557315878344 Validation Loss: 0.00033198087476193905\n","Epoch 11: Training Loss: 0.0033367383293807507 NSE : -0.37583673000335693 WAPE : 67.9726751579079 Validation Loss: 0.000297592458082363\n","Epoch 12: Training Loss: 0.0032542474182264414 NSE : -0.3908560276031494 WAPE : 68.60392320360218 Validation Loss: 0.000300841114949435\n","Epoch 13: Training Loss: 0.003171005613694433 NSE : -0.290974497795105 WAPE : 65.80697921661005 Validation Loss: 0.0002792367886286229\n","Epoch 14: Training Loss: 0.0030876412874931702 NSE : -0.20542407035827637 WAPE : 63.34623835233787 Validation Loss: 0.0002607323112897575\n","Epoch 15: Training Loss: 0.003010671533047571 NSE : -0.16100895404815674 WAPE : 62.09135936294845 Validation Loss: 0.00025112536968663335\n","Epoch 16: Training Loss: 0.0029388459624897223 NSE : -0.08649539947509766 WAPE : 59.76153509411937 Validation Loss: 0.0002350081194890663\n","Epoch 17: Training Loss: 0.0028749420162057504 NSE : -0.04070770740509033 WAPE : 58.32772299931208 Validation Loss: 0.00022510428971145302\n","Epoch 18: Training Loss: 0.0028137659010099014 NSE : 0.01836693286895752 WAPE : 56.38334827291489 Validation Loss: 0.00021232648578006774\n","Epoch 19: Training Loss: 0.0027533765405678423 NSE : 0.061144471168518066 WAPE : 55.01608888703592 Validation Loss: 0.00020307373779360205\n","Epoch 20: Training Loss: 0.002693732356419787 NSE : 0.08183199167251587 WAPE : 54.42440641220737 Validation Loss: 0.0001985990529647097\n","Epoch 21: Training Loss: 0.0026356313628639327 NSE : 0.13774341344833374 WAPE : 52.51317671092952 Validation Loss: 0.00018650545098353177\n","Epoch 22: Training Loss: 0.0025816662346187513 NSE : 0.16353589296340942 WAPE : 51.75554814367014 Validation Loss: 0.0001809265377232805\n","Epoch 23: Training Loss: 0.002534376251787762 NSE : 0.14801025390625 WAPE : 52.55398469908903 Validation Loss: 0.00018428475596010685\n","Epoch 24: Training Loss: 0.002487230311089661 NSE : 0.19031566381454468 WAPE : 51.066369264764134 Validation Loss: 0.00017513410421088338\n","Epoch 25: Training Loss: 0.0024441840214421973 NSE : 0.22959959506988525 WAPE : 49.60458610410456 Validation Loss: 0.00016663703718222678\n","Epoch 26: Training Loss: 0.0024046683556662174 NSE : 0.24458837509155273 WAPE : 49.067211440245146 Validation Loss: 0.000163394957780838\n","Epoch 27: Training Loss: 0.0023662544972467003 NSE : 0.2410203218460083 WAPE : 49.2966250442976 Validation Loss: 0.0001641667477088049\n","Epoch 28: Training Loss: 0.0023283434929908253 NSE : 0.2493322491645813 WAPE : 49.036943153155036 Validation Loss: 0.0001623688731342554\n","Epoch 29: Training Loss: 0.002290164481564716 NSE : 0.2902570366859436 WAPE : 47.39139063184007 Validation Loss: 0.0001535168703412637\n","Epoch 30: Training Loss: 0.002254112096125027 NSE : 0.31712067127227783 WAPE : 46.28311271393134 Validation Loss: 0.0001477062760386616\n","Epoch 31: Training Loss: 0.0022206630328582833 NSE : 0.3185356855392456 WAPE : 46.27424068708178 Validation Loss: 0.00014740019105374813\n","Epoch 32: Training Loss: 0.0021880365638935473 NSE : 0.3205757737159729 WAPE : 46.259990410873236 Validation Loss: 0.00014695896243210882\n","Epoch 33: Training Loss: 0.0021544248802456423 NSE : 0.3482069969177246 WAPE : 45.11931792124408 Validation Loss: 0.00014098233077675104\n","Epoch 34: Training Loss: 0.002122382478773943 NSE : 0.38365232944488525 WAPE : 43.556557086573136 Validation Loss: 0.0001333155232714489\n","Epoch 35: Training Loss: 0.002092361854920455 NSE : 0.38831281661987305 WAPE : 43.393208396739695 Validation Loss: 0.00013230746844783425\n","Epoch 36: Training Loss: 0.002062925760583312 NSE : 0.3817352056503296 WAPE : 43.736724270913676 Validation Loss: 0.00013373020919971168\n","Epoch 37: Training Loss: 0.0020338789572633686 NSE : 0.3958085775375366 WAPE : 43.10719392966584 Validation Loss: 0.00013068615226075053\n","Epoch 38: Training Loss: 0.0020055707018400426 NSE : 0.4252541661262512 WAPE : 41.74279877425945 Validation Loss: 0.00012431709910742939\n","Epoch 39: Training Loss: 0.001979291193492827 NSE : 0.43314194679260254 WAPE : 41.44725771820475 Validation Loss: 0.00012261097435839474\n","Epoch 40: Training Loss: 0.0019537941061571473 NSE : 0.43072015047073364 WAPE : 41.65239832398741 Validation Loss: 0.00012313479965087026\n","Epoch 41: Training Loss: 0.001928040991515445 NSE : 0.44079506397247314 WAPE : 41.21211565320715 Validation Loss: 0.00012095559941371903\n","Epoch 42: Training Loss: 0.001902828229503939 NSE : 0.45913922786712646 WAPE : 40.340926809947675 Validation Loss: 0.00011698777234414592\n","Epoch 43: Training Loss: 0.0018790620397339808 NSE : 0.46975117921829224 WAPE : 39.858331075024495 Validation Loss: 0.00011469241144368425\n","Epoch 44: Training Loss: 0.0018562594104878372 NSE : 0.4738612174987793 WAPE : 39.711017072814826 Validation Loss: 0.00011380341311451048\n","Epoch 45: Training Loss: 0.001833746193369734 NSE : 0.4789300560951233 WAPE : 39.508803235287985 Validation Loss: 0.00011270703544141725\n","Epoch 46: Training Loss: 0.0018112995339834015 NSE : 0.49045801162719727 WAPE : 38.95867086364679 Validation Loss: 0.00011021354293916374\n","Epoch 47: Training Loss: 0.0017894373113449547 NSE : 0.5012505054473877 WAPE : 38.44087469512831 Validation Loss: 0.00010787913197418675\n","Epoch 48: Training Loss: 0.0017683228606983903 NSE : 0.5065758228302002 WAPE : 38.214754747660045 Validation Loss: 0.00010672727512428537\n","Epoch 49: Training Loss: 0.001747519503624062 NSE : 0.5094398260116577 WAPE : 38.11435242125451 Validation Loss: 0.0001061078000930138\n","Epoch 50: Training Loss: 0.0017270208454647218 NSE : 0.5154232978820801 WAPE : 37.840570344583185 Validation Loss: 0.00010481357458047569\n","Epoch 51: Training Loss: 0.001706852998722752 NSE : 0.5230140686035156 WAPE : 37.464263826061575 Validation Loss: 0.00010317167470930144\n","Epoch 52: Training Loss: 0.0016870543731783982 NSE : 0.5292043685913086 WAPE : 37.156728023180676 Validation Loss: 0.00010183273843722418\n","Epoch 53: Training Loss: 0.0016679641012160573 NSE : 0.5331681370735168 WAPE : 36.98077588543078 Validation Loss: 0.00010097537597175688\n","Epoch 54: Training Loss: 0.0016495729605594533 NSE : 0.5400211811065674 WAPE : 36.654878989389424 Validation Loss: 9.94930524029769e-05\n","Epoch 55: Training Loss: 0.0016315241082338616 NSE : 0.5461084842681885 WAPE : 36.3706155802464 Validation Loss: 9.817638783715665e-05\n","Epoch 56: Training Loss: 0.001613803967302374 NSE : 0.552513837814331 WAPE : 36.06791186341748 Validation Loss: 9.679092181613669e-05\n","Epoch 57: Training Loss: 0.0015963950700097485 NSE : 0.561333417892456 WAPE : 35.62385191052928 Validation Loss: 9.488325304118916e-05\n","Epoch 58: Training Loss: 0.0015794387818459654 NSE : 0.5705628395080566 WAPE : 35.155410560547 Validation Loss: 9.288693399867043e-05\n","Epoch 59: Training Loss: 0.0015629463659934117 NSE : 0.579651951789856 WAPE : 34.70077963769778 Validation Loss: 9.092096297536045e-05\n","Epoch 60: Training Loss: 0.0015468144056285382 NSE : 0.587520182132721 WAPE : 34.323001396677164 Validation Loss: 8.921907283365726e-05\n","Epoch 61: Training Loss: 0.0015305224851545063 NSE : 0.592545747756958 WAPE : 34.1420170519689 Validation Loss: 8.813203749014065e-05\n","Epoch 62: Training Loss: 0.0015143652854021639 NSE : 0.6046090126037598 WAPE : 33.48939776114736 Validation Loss: 8.552276995033026e-05\n","Epoch 63: Training Loss: 0.0014987660433689598 NSE : 0.6188874244689941 WAPE : 32.65850201163203 Validation Loss: 8.243436604971066e-05\n","Epoch 64: Training Loss: 0.0014839963614576845 NSE : 0.6304906010627747 WAPE : 31.984811657042794 Validation Loss: 7.992461178218946e-05\n","Epoch 65: Training Loss: 0.0014695951622343273 NSE : 0.6406928300857544 WAPE : 31.38958537449709 Validation Loss: 7.771786476951092e-05\n","Epoch 66: Training Loss: 0.0014553761520801345 NSE : 0.6487119197845459 WAPE : 30.924541910737734 Validation Loss: 7.598333468195051e-05\n","Epoch 67: Training Loss: 0.0014415656396522536 NSE : 0.6537229418754578 WAPE : 30.65904400575348 Validation Loss: 7.489946437999606e-05\n","Epoch 68: Training Loss: 0.0014279350916694966 NSE : 0.6584489345550537 WAPE : 30.413333055387632 Validation Loss: 7.387724326690659e-05\n","Epoch 69: Training Loss: 0.0014148276959531358 NSE : 0.6646008491516113 WAPE : 30.069542014967375 Validation Loss: 7.254657248267904e-05\n","Epoch 70: Training Loss: 0.001401633820933057 NSE : 0.6695376634597778 WAPE : 29.82106897917492 Validation Loss: 7.147875294322148e-05\n","Epoch 71: Training Loss: 0.0013881264530937187 NSE : 0.6773471832275391 WAPE : 29.34946113276771 Validation Loss: 6.978956662351266e-05\n","Epoch 72: Training Loss: 0.001375640071273665 NSE : 0.6854604482650757 WAPE : 28.81247628775719 Validation Loss: 6.803465657867491e-05\n","Epoch 73: Training Loss: 0.0013639674289152026 NSE : 0.6905239224433899 WAPE : 28.523570490504678 Validation Loss: 6.693942850688472e-05\n","Epoch 74: Training Loss: 0.001351901053567417 NSE : 0.7006218433380127 WAPE : 27.866398449062974 Validation Loss: 6.475526606664062e-05\n","Epoch 75: Training Loss: 0.0013399868130363757 NSE : 0.7037662267684937 WAPE : 27.72881532592608 Validation Loss: 6.407514592865482e-05\n","Epoch 76: Training Loss: 0.0013283391417644452 NSE : 0.7091062068939209 WAPE : 27.407246044485213 Validation Loss: 6.292010948527604e-05\n","Epoch 77: Training Loss: 0.001316645294537011 NSE : 0.7159017324447632 WAPE : 26.98339413395593 Validation Loss: 6.145023507997394e-05\n","Epoch 78: Training Loss: 0.0013052816921117483 NSE : 0.7262212038040161 WAPE : 26.272775218361094 Validation Loss: 5.9218153182882816e-05\n","Epoch 79: Training Loss: 0.001294506735575851 NSE : 0.7357656955718994 WAPE : 25.611702903837735 Validation Loss: 5.715368752134964e-05\n","Epoch 80: Training Loss: 0.0012840495501222904 NSE : 0.7479553818702698 WAPE : 24.708628129494905 Validation Loss: 5.451706601888873e-05\n","Epoch 81: Training Loss: 0.0012741774116875604 NSE : 0.758978009223938 WAPE : 23.889756311104627 Validation Loss: 5.2132887503830716e-05\n","Epoch 82: Training Loss: 0.001264816011826042 NSE : 0.7676012516021729 WAPE : 23.2359613099581 Validation Loss: 5.0267670303583145e-05\n","Epoch 83: Training Loss: 0.0012558059543152922 NSE : 0.7771981954574585 WAPE : 22.467019657709866 Validation Loss: 4.819185778615065e-05\n","Epoch 84: Training Loss: 0.0012469850362322177 NSE : 0.7848197221755981 WAPE : 21.855977569781743 Validation Loss: 4.654332951758988e-05\n","Epoch 85: Training Loss: 0.0012382749055177555 NSE : 0.7917982935905457 WAPE : 21.274442892580932 Validation Loss: 4.5033862988930196e-05\n","Epoch 86: Training Loss: 0.0012299465424803202 NSE : 0.7989668846130371 WAPE : 20.66426382606158 Validation Loss: 4.348331640358083e-05\n","Epoch 87: Training Loss: 0.0012218646215842455 NSE : 0.8047897219657898 WAPE : 20.179871172166518 Validation Loss: 4.222383722662926e-05\n","Epoch 88: Training Loss: 0.001213920490044984 NSE : 0.8085294961929321 WAPE : 19.89169081319964 Validation Loss: 4.141492900089361e-05\n","Epoch 89: Training Loss: 0.001206030339744757 NSE : 0.8131310343742371 WAPE : 19.493352233641158 Validation Loss: 4.041961074108258e-05\n","Epoch 90: Training Loss: 0.001198561068122217 NSE : 0.81541508436203 WAPE : 19.3357132434179 Validation Loss: 3.992558049503714e-05\n","Epoch 91: Training Loss: 0.0011913420166820288 NSE : 0.8167892098426819 WAPE : 19.25883137729045 Validation Loss: 3.962835398851894e-05\n","Epoch 92: Training Loss: 0.001184227427074802 NSE : 0.8183153867721558 WAPE : 19.168839507202268 Validation Loss: 3.929825106752105e-05\n","Epoch 93: Training Loss: 0.0011771394874813268 NSE : 0.8176988363265991 WAPE : 19.294890663108962 Validation Loss: 3.9431601180695e-05\n","Epoch 94: Training Loss: 0.001169931001641089 NSE : 0.8162463903427124 WAPE : 19.514294052656815 Validation Loss: 3.974577339249663e-05\n","Epoch 95: Training Loss: 0.0011625560546235647 NSE : 0.8125600218772888 WAPE : 19.946782431052092 Validation Loss: 4.0543131035519764e-05\n","Epoch 96: Training Loss: 0.0011550412145879818 NSE : 0.8101986050605774 WAPE : 20.236050947447414 Validation Loss: 4.105390326003544e-05\n","Epoch 97: Training Loss: 0.0011475638539195643 NSE : 0.8081316351890564 WAPE : 20.50821329553272 Validation Loss: 4.150097811361775e-05\n","Epoch 98: Training Loss: 0.0011399150862416718 NSE : 0.8055103421211243 WAPE : 20.82802109607888 Validation Loss: 4.2067964386660606e-05\n","Epoch 99: Training Loss: 0.0011321204774503713 NSE : 0.8051533699035645 WAPE : 20.93827520793813 Validation Loss: 4.214518048684113e-05\n","Epoch 100: Training Loss: 0.001124399730542791 NSE : 0.8083186149597168 WAPE : 20.705000938066746 Validation Loss: 4.146054561715573e-05\n","Epoch 101: Training Loss: 0.0011168990849910188 NSE : 0.8127309083938599 WAPE : 20.355310500093807 Validation Loss: 4.050614734296687e-05\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:27:56,424] Trial 3 finished with value: 3.929825106752105e-05 and parameters: {'lr': 0.0004062110932267948, 'weight_decay': 1.5496659794496835e-05}. Best is trial 1 with value: 2.6064595658681355e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 102: Training Loss: 0.0011097072174379718 NSE : 0.8168050050735474 WAPE : 20.043488774467907 Validation Loss: 3.9624945202376693e-05\n","Early stopping!\n","Epoch 1: Training Loss: 0.005193514363782015 NSE : -7.337360382080078 WAPE : 175.7102916345292 Validation Loss: 0.0018033646047115326\n","Epoch 2: Training Loss: 0.004741641307191458 NSE : -6.092678546905518 WAPE : 161.89311042087928 Validation Loss: 0.0015341410180553794\n","Epoch 3: Training Loss: 0.004453492372704204 NSE : -2.377640724182129 WAPE : 109.79065685518334 Validation Loss: 0.0007305811741389334\n","Epoch 4: Training Loss: 0.004326375641539926 NSE : -2.646303415298462 WAPE : 115.00090471326428 Validation Loss: 0.0007886926759965718\n","Epoch 5: Training Loss: 0.004162483288382646 NSE : -2.9265079498291016 WAPE : 119.95292989514499 Validation Loss: 0.0008493007626384497\n","Epoch 6: Training Loss: 0.004009475276689045 NSE : -1.6195926666259766 WAPE : 96.66329240582851 Validation Loss: 0.0005666159559041262\n","Epoch 7: Training Loss: 0.003934196785849053 NSE : -1.5179023742675781 WAPE : 94.76765545850618 Validation Loss: 0.0005446204449981451\n","Epoch 8: Training Loss: 0.003849732573144138 NSE : -1.3133625984191895 WAPE : 90.59172416668405 Validation Loss: 0.0005003785481676459\n","Epoch 9: Training Loss: 0.0037789597190567292 NSE : -0.9620840549468994 WAPE : 82.73877134101853 Validation Loss: 0.0004243972653057426\n","Epoch 10: Training Loss: 0.003722176123119425 NSE : -0.9558076858520508 WAPE : 82.689725042213 Validation Loss: 0.0004230397171340883\n","Epoch 11: Training Loss: 0.0036501153008430265 NSE : -0.7771283388137817 WAPE : 78.43693898396948 Validation Loss: 0.0003843915183097124\n","Epoch 12: Training Loss: 0.003586926166462945 NSE : -0.7200616598129272 WAPE : 77.12312855683642 Validation Loss: 0.00037204803084023297\n","Epoch 13: Training Loss: 0.0035199790036131162 NSE : -0.6671652793884277 WAPE : 75.89787163077692 Validation Loss: 0.0003606065583880991\n","Epoch 14: Training Loss: 0.0034563515037007164 NSE : -0.5870827436447144 WAPE : 73.89739634362428 Validation Loss: 0.0003432848025113344\n","Epoch 15: Training Loss: 0.0033952242265513632 NSE : -0.552603006362915 WAPE : 73.08830751912613 Validation Loss: 0.00033582685864530504\n","Epoch 16: Training Loss: 0.0033320833899779245 NSE : -0.48734819889068604 WAPE : 71.41211148402161 Validation Loss: 0.000321712315781042\n","Epoch 17: Training Loss: 0.003271351717557991 NSE : -0.438082218170166 WAPE : 70.14878155552313 Validation Loss: 0.0003110560937784612\n","Epoch 18: Training Loss: 0.0032098010269692168 NSE : -0.3651224374771118 WAPE : 68.18308561422526 Validation Loss: 0.00029527494916692376\n","Epoch 19: Training Loss: 0.003149897831463022 NSE : -0.3060598373413086 WAPE : 66.59782785432866 Validation Loss: 0.0002824997645802796\n","Epoch 20: Training Loss: 0.003093981693382375 NSE : -0.24539494514465332 WAPE : 64.89392341206145 Validation Loss: 0.0002693779824767262\n","Epoch 21: Training Loss: 0.003038138082047226 NSE : -0.21692168712615967 WAPE : 64.17482228846595 Validation Loss: 0.00026321920449845493\n","Epoch 22: Training Loss: 0.002989527696627192 NSE : -0.16698873043060303 WAPE : 62.71814221091909 Validation Loss: 0.0002524188021197915\n","Epoch 23: Training Loss: 0.0029429150072246557 NSE : -0.11928665637969971 WAPE : 61.283200266827876 Validation Loss: 0.0002421008248347789\n","Epoch 24: Training Loss: 0.0029028301305515924 NSE : -0.09195363521575928 WAPE : 60.48429676262742 Validation Loss: 0.0002361887163715437\n","Epoch 25: Training Loss: 0.0028628261061385274 NSE : -0.04187214374542236 WAPE : 58.88221633903817 Validation Loss: 0.00022535618336405605\n","Epoch 26: Training Loss: 0.002828006903655478 NSE : -0.021866202354431152 WAPE : 58.2834712638886 Validation Loss: 0.00022102889488451183\n","Epoch 27: Training Loss: 0.0027937887698499253 NSE : 0.016035258769989014 WAPE : 57.04025765566697 Validation Loss: 0.00021283085516188294\n","Epoch 28: Training Loss: 0.0027626469382084906 NSE : 0.03775972127914429 WAPE : 56.35305497071147 Validation Loss: 0.0002081318525597453\n","Epoch 29: Training Loss: 0.0027328534833941376 NSE : 0.06399530172348022 WAPE : 55.48827416564174 Validation Loss: 0.00020245711493771523\n","Epoch 30: Training Loss: 0.0027046540944866138 NSE : 0.08663231134414673 WAPE : 54.7370536365721 Validation Loss: 0.00019756074470933527\n","Epoch 31: Training Loss: 0.0026778985684359213 NSE : 0.10691750049591064 WAPE : 54.05881886973379 Validation Loss: 0.00019317308033350855\n","Epoch 32: Training Loss: 0.0026521812105784193 NSE : 0.12751436233520508 WAPE : 53.355435575660294 Validation Loss: 0.0001887179969344288\n","Epoch 33: Training Loss: 0.0026275980126229115 NSE : 0.14551568031311035 WAPE : 52.740255571074194 Validation Loss: 0.00018482431187294424\n","Epoch 34: Training Loss: 0.0026038943342427956 NSE : 0.1617550253868103 WAPE : 52.189322715807464 Validation Loss: 0.00018131175602320582\n","Epoch 35: Training Loss: 0.002580899224994937 NSE : 0.17768901586532593 WAPE : 51.644656146421795 Validation Loss: 0.0001778652222128585\n","Epoch 36: Training Loss: 0.0025586340052541345 NSE : 0.19314062595367432 WAPE : 51.10942861312042 Validation Loss: 0.00017452306929044425\n","Epoch 37: Training Loss: 0.0025370597431901842 NSE : 0.20710068941116333 WAPE : 50.624285505826435 Validation Loss: 0.00017150351777672768\n","Epoch 38: Training Loss: 0.0025160358964058105 NSE : 0.22122323513031006 WAPE : 50.12336620041275 Validation Loss: 0.00016844880883581936\n","Epoch 39: Training Loss: 0.0024955700146165327 NSE : 0.23437917232513428 WAPE : 49.65322799191178 Validation Loss: 0.00016560321091674268\n","Epoch 40: Training Loss: 0.002475583980412921 NSE : 0.24639755487442017 WAPE : 49.22216339038169 Validation Loss: 0.00016300365678034723\n","Epoch 41: Training Loss: 0.002455980851664208 NSE : 0.2581806182861328 WAPE : 48.79342936357383 Validation Loss: 0.00016045497613959014\n","Epoch 42: Training Loss: 0.002436774786474416 NSE : 0.269974946975708 WAPE : 48.357445123095204 Validation Loss: 0.0001579038507770747\n","Epoch 43: Training Loss: 0.0024180149903259007 NSE : 0.28087395429611206 WAPE : 47.952763127722996 Validation Loss: 0.00015554644051007926\n","Epoch 44: Training Loss: 0.002399635840447445 NSE : 0.29123616218566895 WAPE : 47.564491046674036 Validation Loss: 0.00015330508176703006\n","Epoch 45: Training Loss: 0.0023815803479010356 NSE : 0.3015967011451721 WAPE : 47.1705655500198 Validation Loss: 0.00015106413047760725\n","Epoch 46: Training Loss: 0.0023637335007151705 NSE : 0.3121454119682312 WAPE : 46.76384482291384 Validation Loss: 0.000148782433825545\n","Epoch 47: Training Loss: 0.0023461740938728326 NSE : 0.32155662775039673 WAPE : 46.4010422963874 Validation Loss: 0.000146746780956164\n","Epoch 48: Training Loss: 0.0023288133243113407 NSE : 0.330999493598938 WAPE : 46.03287819724417 Validation Loss: 0.0001447043032385409\n","Epoch 49: Training Loss: 0.002311660523446335 NSE : 0.33924996852874756 WAPE : 45.713535260886786 Validation Loss: 0.00014291972911451012\n","Epoch 50: Training Loss: 0.002294541547598783 NSE : 0.3476393222808838 WAPE : 45.389301869879716 Validation Loss: 0.000141105119837448\n","Epoch 51: Training Loss: 0.002277759712342231 NSE : 0.356400728225708 WAPE : 45.04057868295428 Validation Loss: 0.00013921005302108824\n","Epoch 52: Training Loss: 0.002261122042000352 NSE : 0.3646742105484009 WAPE : 44.7086031143816 Validation Loss: 0.00013742050214204937\n","Epoch 53: Training Loss: 0.0022442079807660775 NSE : 0.3732950687408447 WAPE : 44.35714077255008 Validation Loss: 0.00013555580517277122\n","Epoch 54: Training Loss: 0.0022273611784839886 NSE : 0.3819710612297058 WAPE : 43.99643534635509 Validation Loss: 0.0001336791756330058\n","Epoch 55: Training Loss: 0.0022109370620455593 NSE : 0.391757607460022 WAPE : 43.58142210919097 Validation Loss: 0.00013156235218048096\n","Epoch 56: Training Loss: 0.0021948755993435043 NSE : 0.40031296014785767 WAPE : 43.220128827833484 Validation Loss: 0.00012971185788046569\n","Epoch 57: Training Loss: 0.0021790750533909886 NSE : 0.4082029461860657 WAPE : 42.88756123491276 Validation Loss: 0.00012800526747014374\n","Epoch 58: Training Loss: 0.0021634934419125784 NSE : 0.4163029193878174 WAPE : 42.547159742344334 Validation Loss: 0.0001262532314285636\n","Epoch 59: Training Loss: 0.002148178757124697 NSE : 0.4245230555534363 WAPE : 42.2005044714515 Validation Loss: 0.00012447522021830082\n","Epoch 60: Training Loss: 0.0021331135803848156 NSE : 0.43246883153915405 WAPE : 41.86267953555273 Validation Loss: 0.00012275656627025455\n","Epoch 61: Training Loss: 0.0021183002872930956 NSE : 0.439996600151062 WAPE : 41.54083925705114 Validation Loss: 0.00012112831609556451\n","Epoch 62: Training Loss: 0.002103755837197241 NSE : 0.4468376636505127 WAPE : 41.24944654061829 Validation Loss: 0.00011964861187152565\n","Epoch 63: Training Loss: 0.002089466397592332 NSE : 0.4525564908981323 WAPE : 41.010460486543955 Validation Loss: 0.00011841162631753832\n","Epoch 64: Training Loss: 0.0020752916070705396 NSE : 0.4601830840110779 WAPE : 40.67294407037585 Validation Loss: 0.00011676198482746258\n","Epoch 65: Training Loss: 0.002061391519418976 NSE : 0.46661317348480225 WAPE : 40.39158241437536 Validation Loss: 0.00011537116370163858\n","Epoch 66: Training Loss: 0.0020477213229241897 NSE : 0.4729769825935364 WAPE : 40.110191574076005 Validation Loss: 0.00011399467621231452\n","Epoch 67: Training Loss: 0.00203421151945804 NSE : 0.47924983501434326 WAPE : 39.83066019053178 Validation Loss: 0.00011263785563642159\n","Epoch 68: Training Loss: 0.0020209113345117657 NSE : 0.4848065972328186 WAPE : 39.586099935377625 Validation Loss: 0.00011143594019813463\n","Epoch 69: Training Loss: 0.0020077342705917545 NSE : 0.49081361293792725 WAPE : 39.31707906860395 Validation Loss: 0.00011013663606718183\n","Epoch 70: Training Loss: 0.0019947410482927808 NSE : 0.49657154083251953 WAPE : 39.059606845802676 Validation Loss: 0.0001088911885744892\n","Epoch 71: Training Loss: 0.001981905688808183 NSE : 0.5022810101509094 WAPE : 38.80342707052177 Validation Loss: 0.00010765625484054908\n","Epoch 72: Training Loss: 0.001969271048437804 NSE : 0.5079584121704102 WAPE : 38.54791436492881 Validation Loss: 0.00010642823326634243\n","Epoch 73: Training Loss: 0.0019567825402191374 NSE : 0.5136362314224243 WAPE : 38.29194721811094 Validation Loss: 0.0001052001171046868\n","Epoch 74: Training Loss: 0.0019445076768533909 NSE : 0.518369197845459 WAPE : 38.084555252131494 Validation Loss: 0.0001041763971443288\n","Epoch 75: Training Loss: 0.0019323347323734197 NSE : 0.5242183804512024 WAPE : 37.81496320693753 Validation Loss: 0.00010291119542671368\n","Epoch 76: Training Loss: 0.0019204429363526287 NSE : 0.5292974710464478 WAPE : 37.582581142773755 Validation Loss: 0.0001018125913105905\n","Epoch 77: Training Loss: 0.0019087022410531063 NSE : 0.5346916317939758 WAPE : 37.33057055304246 Validation Loss: 0.00010064584785141051\n","Epoch 78: Training Loss: 0.0018971521203638986 NSE : 0.5396022796630859 WAPE : 37.10299972900294 Validation Loss: 9.958367445506155e-05\n","Epoch 79: Training Loss: 0.0018857490986192715 NSE : 0.5446998476982117 WAPE : 36.863179837818684 Validation Loss: 9.848106856225058e-05\n","Epoch 80: Training Loss: 0.0018745156285149278 NSE : 0.5497280955314636 WAPE : 36.624302182568634 Validation Loss: 9.739347296999767e-05\n","Epoch 81: Training Loss: 0.0018634459684108151 NSE : 0.5544271469116211 WAPE : 36.401217402180485 Validation Loss: 9.637706534704193e-05\n","Epoch 82: Training Loss: 0.0018525383375163074 NSE : 0.5587068796157837 WAPE : 36.19926205415772 Validation Loss: 9.545137436361983e-05\n","Epoch 83: Training Loss: 0.0018417367900838144 NSE : 0.5633219480514526 WAPE : 35.976744283004315 Validation Loss: 9.445313480682671e-05\n","Epoch 84: Training Loss: 0.0018310958166694036 NSE : 0.5676418542861938 WAPE : 35.768647724666984 Validation Loss: 9.351872722618282e-05\n","Epoch 85: Training Loss: 0.001820580946514383 NSE : 0.5720592737197876 WAPE : 35.55215442663276 Validation Loss: 9.256324847228825e-05\n","Epoch 86: Training Loss: 0.0018102463909599464 NSE : 0.5758967399597168 WAPE : 35.366579808634384 Validation Loss: 9.17332072276622e-05\n","Epoch 87: Training Loss: 0.0017999632154896972 NSE : 0.5804353952407837 WAPE : 35.14007212691001 Validation Loss: 9.075149864656851e-05\n","Epoch 88: Training Loss: 0.001789778964848665 NSE : 0.5850096940994263 WAPE : 34.909067978570384 Validation Loss: 8.9762092102319e-05\n","Epoch 89: Training Loss: 0.001779762619662506 NSE : 0.5885778665542603 WAPE : 34.734339496779306 Validation Loss: 8.899030217435211e-05\n","Epoch 90: Training Loss: 0.0017698336405373993 NSE : 0.5923584699630737 WAPE : 34.54543786871235 Validation Loss: 8.817255729809403e-05\n","Epoch 91: Training Loss: 0.0017600054989088676 NSE : 0.5964207649230957 WAPE : 34.337941673094164 Validation Loss: 8.729387627681717e-05\n","Epoch 92: Training Loss: 0.001750308215378027 NSE : 0.6001594066619873 WAPE : 34.1475016155594 Validation Loss: 8.648522634757683e-05\n","Epoch 93: Training Loss: 0.0017407490540790604 NSE : 0.6038567423820496 WAPE : 33.9574201079819 Validation Loss: 8.568548219045624e-05\n","Epoch 94: Training Loss: 0.0017313172884314554 NSE : 0.6070412993431091 WAPE : 33.79615184173772 Validation Loss: 8.49966672831215e-05\n","Epoch 95: Training Loss: 0.001722029895063315 NSE : 0.6097306609153748 WAPE : 33.663317420941816 Validation Loss: 8.441496174782515e-05\n","Epoch 96: Training Loss: 0.0017128502613559249 NSE : 0.6133468151092529 WAPE : 33.47224364720352 Validation Loss: 8.363280358025804e-05\n","Epoch 97: Training Loss: 0.0017038441337717813 NSE : 0.616854190826416 WAPE : 33.285699693564865 Validation Loss: 8.287415403174236e-05\n","Epoch 98: Training Loss: 0.0016949948758337996 NSE : 0.6201228499412537 WAPE : 33.11225323633028 Validation Loss: 8.216714923037216e-05\n","Epoch 99: Training Loss: 0.0016862703841979965 NSE : 0.6229375600814819 WAPE : 32.966029476141834 Validation Loss: 8.155832620104775e-05\n","Epoch 100: Training Loss: 0.0016776600596131175 NSE : 0.6258477568626404 WAPE : 32.81293906735319 Validation Loss: 8.092883945209906e-05\n","Epoch 101: Training Loss: 0.0016691353421265376 NSE : 0.6293262243270874 WAPE : 32.622536532488375 Validation Loss: 8.01764617790468e-05\n","Epoch 102: Training Loss: 0.0016607506368018221 NSE : 0.632470428943634 WAPE : 32.45181672260324 Validation Loss: 7.949637074489146e-05\n","Epoch 103: Training Loss: 0.0016525578075743397 NSE : 0.6350473761558533 WAPE : 32.31599716495383 Validation Loss: 7.89389741839841e-05\n","Epoch 104: Training Loss: 0.0016443939739474445 NSE : 0.6378012895584106 WAPE : 32.168726939192425 Validation Loss: 7.834329881006852e-05\n","Epoch 105: Training Loss: 0.0016363155127692153 NSE : 0.6413589119911194 WAPE : 31.96900002084593 Validation Loss: 7.757378625683486e-05\n","Epoch 106: Training Loss: 0.0016284025596178253 NSE : 0.6432005167007446 WAPE : 31.877901231994333 Validation Loss: 7.717545668128878e-05\n","Epoch 107: Training Loss: 0.00162056383669551 NSE : 0.646284818649292 WAPE : 31.705576285672592 Validation Loss: 7.650833140360191e-05\n","Epoch 108: Training Loss: 0.001612829155419604 NSE : 0.6489379405975342 WAPE : 31.559750682704134 Validation Loss: 7.593446207465604e-05\n","Epoch 109: Training Loss: 0.0016052163473432302 NSE : 0.6508994102478027 WAPE : 31.458283129390672 Validation Loss: 7.551018643425778e-05\n","Epoch 110: Training Loss: 0.0015976360200511408 NSE : 0.6538612842559814 WAPE : 31.293085405765986 Validation Loss: 7.486953836632892e-05\n","Epoch 111: Training Loss: 0.0015901507140370086 NSE : 0.656669557094574 WAPE : 31.13648662733735 Validation Loss: 7.426212687278166e-05\n","Epoch 112: Training Loss: 0.001582757375217625 NSE : 0.6592105627059937 WAPE : 30.996185195222115 Validation Loss: 7.371250103460625e-05\n","Epoch 113: Training Loss: 0.0015754842997921514 NSE : 0.6608846783638 WAPE : 30.911921786079088 Validation Loss: 7.335039117606357e-05\n","Epoch 114: Training Loss: 0.0015682980183555628 NSE : 0.6634984016418457 WAPE : 30.765458297721537 Validation Loss: 7.27850419934839e-05\n","Epoch 115: Training Loss: 0.0015610777454639901 NSE : 0.6663668155670166 WAPE : 30.600988096975257 Validation Loss: 7.216461381176487e-05\n","Epoch 116: Training Loss: 0.001554092805235996 NSE : 0.6680301427841187 WAPE : 30.514156469533678 Validation Loss: 7.180483225965872e-05\n","Epoch 117: Training Loss: 0.0015472419663637993 NSE : 0.6701575517654419 WAPE : 30.396654228596443 Validation Loss: 7.134467887226492e-05\n","Epoch 118: Training Loss: 0.001540260219371703 NSE : 0.6726763248443604 WAPE : 30.25619436743032 Validation Loss: 7.079987699398771e-05\n","Epoch 119: Training Loss: 0.0015335276093537686 NSE : 0.674511194229126 WAPE : 30.15939213274687 Validation Loss: 7.040298805804923e-05\n","Epoch 120: Training Loss: 0.0015268296183421626 NSE : 0.6761335134506226 WAPE : 30.075687394467494 Validation Loss: 7.005209045019001e-05\n","Epoch 121: Training Loss: 0.0015202036902337568 NSE : 0.6784014701843262 WAPE : 29.94757040712097 Validation Loss: 6.956153083592653e-05\n","Epoch 122: Training Loss: 0.001513617432465253 NSE : 0.6811590194702148 WAPE : 29.784799149486147 Validation Loss: 6.896507693454623e-05\n","Epoch 123: Training Loss: 0.0015072777141540428 NSE : 0.6823229193687439 WAPE : 29.729640824664898 Validation Loss: 6.871332152513787e-05\n","Epoch 124: Training Loss: 0.0015009047383500729 NSE : 0.6844427585601807 WAPE : 29.60940151341435 Validation Loss: 6.825479795224965e-05\n","Epoch 125: Training Loss: 0.0014945717030059313 NSE : 0.6864927411079407 WAPE : 29.49373579871172 Validation Loss: 6.781138654332608e-05\n","Epoch 126: Training Loss: 0.0014883862213537213 NSE : 0.6877449750900269 WAPE : 29.431800462779595 Validation Loss: 6.754052446922287e-05\n","Epoch 127: Training Loss: 0.001482223701714247 NSE : 0.6906377077102661 WAPE : 29.255792041024787 Validation Loss: 6.691482849419117e-05\n","Epoch 128: Training Loss: 0.0014761636412004009 NSE : 0.6921993494033813 WAPE : 29.17071147151404 Validation Loss: 6.657706398982555e-05\n","Epoch 129: Training Loss: 0.0014702665739605436 NSE : 0.6936162114143372 WAPE : 29.095566071168 Validation Loss: 6.62705788272433e-05\n","Epoch 130: Training Loss: 0.0014642727364844177 NSE : 0.6959967613220215 WAPE : 28.952783973650746 Validation Loss: 6.575566658284515e-05\n","Epoch 131: Training Loss: 0.0014584435948563623 NSE : 0.6981282830238342 WAPE : 28.82640136749286 Validation Loss: 6.52946182526648e-05\n","Epoch 132: Training Loss: 0.0014527558896588744 NSE : 0.6988964080810547 WAPE : 28.795847491192596 Validation Loss: 6.512849358841777e-05\n","Epoch 133: Training Loss: 0.0014469547513726866 NSE : 0.7000061273574829 WAPE : 28.741016447436994 Validation Loss: 6.488844519481063e-05\n","Epoch 134: Training Loss: 0.0014412812788577867 NSE : 0.702747106552124 WAPE : 28.568316274415796 Validation Loss: 6.42955710645765e-05\n","Epoch 135: Training Loss: 0.0014357482205014094 NSE : 0.7047332525253296 WAPE : 28.44869608721936 Validation Loss: 6.386597669916227e-05\n","Epoch 136: Training Loss: 0.0014302239833341446 NSE : 0.7057245969772339 WAPE : 28.40081299118217 Validation Loss: 6.365154695231467e-05\n","Epoch 137: Training Loss: 0.0014248098586904234 NSE : 0.7065104246139526 WAPE : 28.367530382939695 Validation Loss: 6.348156603053212e-05\n","Epoch 138: Training Loss: 0.0014193994002198451 NSE : 0.708458662033081 WAPE : 28.249240165933585 Validation Loss: 6.306017166934907e-05\n","Epoch 139: Training Loss: 0.001413900955412828 NSE : 0.7114208340644836 WAPE : 28.0558900168852 Validation Loss: 6.241945084184408e-05\n","Epoch 140: Training Loss: 0.0014087396848481148 NSE : 0.7121320962905884 WAPE : 28.02477746972129 Validation Loss: 6.22656152700074e-05\n","Epoch 141: Training Loss: 0.0014036161555850413 NSE : 0.7125616073608398 WAPE : 28.01441287444498 Validation Loss: 6.21727085672319e-05\n","Epoch 142: Training Loss: 0.0013983661565362127 NSE : 0.7146843671798706 WAPE : 27.88097392174439 Validation Loss: 6.171356653794646e-05\n","Epoch 143: Training Loss: 0.0013931987105024746 NSE : 0.7163448929786682 WAPE : 27.78012549248504 Validation Loss: 6.13543888903223e-05\n","Epoch 144: Training Loss: 0.001388242531902506 NSE : 0.7173736095428467 WAPE : 27.72433970523858 Validation Loss: 6.113187555456534e-05\n","Epoch 145: Training Loss: 0.0013831745436618803 NSE : 0.7193197011947632 WAPE : 27.60074628421338 Validation Loss: 6.0710943216690794e-05\n","Epoch 146: Training Loss: 0.0013783244630758418 NSE : 0.7200886011123657 WAPE : 27.563857330470494 Validation Loss: 6.0544622101588175e-05\n","Epoch 147: Training Loss: 0.001373446463730943 NSE : 0.7217587828636169 WAPE : 27.4609201392508 Validation Loss: 6.0183363530086353e-05\n","Epoch 148: Training Loss: 0.0013685281528523774 NSE : 0.7230950593948364 WAPE : 27.381980780054615 Validation Loss: 5.989432247588411e-05\n","Epoch 149: Training Loss: 0.0013638542395710829 NSE : 0.7234649658203125 WAPE : 27.37342561130683 Validation Loss: 5.9814308770000935e-05\n","Epoch 150: Training Loss: 0.0013590760363513255 NSE : 0.7255594730377197 WAPE : 27.237478893498157 Validation Loss: 5.936127854511142e-05\n","Epoch 151: Training Loss: 0.0013543295272029354 NSE : 0.7277551293373108 WAPE : 27.093281357486816 Validation Loss: 5.8886351325782016e-05\n","Epoch 152: Training Loss: 0.0013497546287908335 NSE : 0.7284460067749023 WAPE : 27.06132663484188 Validation Loss: 5.873692498425953e-05\n","Epoch 153: Training Loss: 0.0013452267694447073 NSE : 0.7293663024902344 WAPE : 27.012407496195618 Validation Loss: 5.853785842191428e-05\n","Epoch 154: Training Loss: 0.0013406586185737979 NSE : 0.7308259606361389 WAPE : 26.923387046340498 Validation Loss: 5.8222147345077246e-05\n","Epoch 155: Training Loss: 0.0013361391684156843 NSE : 0.732376217842102 WAPE : 26.825807258552043 Validation Loss: 5.788682392449118e-05\n","Epoch 156: Training Loss: 0.0013317481261765352 NSE : 0.7329301834106445 WAPE : 26.800992266160804 Validation Loss: 5.7766999816522e-05\n","Epoch 157: Training Loss: 0.00132737285366602 NSE : 0.7343630194664001 WAPE : 26.709616226470157 Validation Loss: 5.745707676396705e-05\n","Epoch 158: Training Loss: 0.00132304074941203 NSE : 0.7362999320030212 WAPE : 26.57963352859019 Validation Loss: 5.703812348656356e-05\n","Epoch 159: Training Loss: 0.001318753270425077 NSE : 0.7376971244812012 WAPE : 26.49115715744929 Validation Loss: 5.6735916587058455e-05\n","Epoch 160: Training Loss: 0.0013145568618710968 NSE : 0.7376245856285095 WAPE : 26.51580329782577 Validation Loss: 5.675160718965344e-05\n","Epoch 161: Training Loss: 0.001310351086431183 NSE : 0.7383230328559875 WAPE : 26.479331262637846 Validation Loss: 5.660052920575254e-05\n","Epoch 162: Training Loss: 0.0013061681711405981 NSE : 0.7407470941543579 WAPE : 26.309960184278005 Validation Loss: 5.607622006209567e-05\n","Epoch 163: Training Loss: 0.001302017906709807 NSE : 0.7429320812225342 WAPE : 26.1585541264514 Validation Loss: 5.560358476941474e-05\n","Epoch 164: Training Loss: 0.0012980498677279684 NSE : 0.7431902885437012 WAPE : 26.154647599591417 Validation Loss: 5.55477527086623e-05\n","Epoch 165: Training Loss: 0.0012941216145918588 NSE : 0.7425898313522339 WAPE : 26.21819641033124 Validation Loss: 5.567762491409667e-05\n","Epoch 166: Training Loss: 0.0012900584115413949 NSE : 0.7446335554122925 WAPE : 26.076098059244128 Validation Loss: 5.523556319531053e-05\n","Epoch 167: Training Loss: 0.0012860214219472255 NSE : 0.7478722333908081 WAPE : 25.837589376915222 Validation Loss: 5.4535044910153374e-05\n","Epoch 168: Training Loss: 0.001282238881685771 NSE : 0.7482218742370605 WAPE : 25.828039857413852 Validation Loss: 5.445941860671155e-05\n","Epoch 169: Training Loss: 0.0012784479040419683 NSE : 0.7481037974357605 WAPE : 25.853490650601408 Validation Loss: 5.448495357995853e-05\n","Epoch 170: Training Loss: 0.0012746407483064104 NSE : 0.7487499713897705 WAPE : 25.815488524316777 Validation Loss: 5.434518607216887e-05\n","Epoch 171: Training Loss: 0.0012708230060525239 NSE : 0.7506626844406128 WAPE : 25.678997727793874 Validation Loss: 5.39314751222264e-05\n","Epoch 172: Training Loss: 0.0012671061149376328 NSE : 0.7526031136512756 WAPE : 25.538456567509538 Validation Loss: 5.351175423129462e-05\n","Epoch 173: Training Loss: 0.001263453400497383 NSE : 0.7532153725624084 WAPE : 25.505759729836775 Validation Loss: 5.337933180271648e-05\n","Epoch 174: Training Loss: 0.0012598561961567611 NSE : 0.7530761957168579 WAPE : 25.53080402743324 Validation Loss: 5.3409432439366356e-05\n","Epoch 175: Training Loss: 0.0012562408610392595 NSE : 0.7536647319793701 WAPE : 25.498578307727588 Validation Loss: 5.32821322849486e-05\n","Epoch 176: Training Loss: 0.0012526015498224297 NSE : 0.755298912525177 WAPE : 25.37963352859019 Validation Loss: 5.292865171213634e-05\n","Epoch 177: Training Loss: 0.0012490722047004965 NSE : 0.7572314739227295 WAPE : 25.23501282024557 Validation Loss: 5.25106443092227e-05\n","Epoch 178: Training Loss: 0.0012456773474696092 NSE : 0.7568594217300415 WAPE : 25.28090304559004 Validation Loss: 5.259112367639318e-05\n","Epoch 179: Training Loss: 0.0012422134750522673 NSE : 0.758127748966217 WAPE : 25.194090179483435 Validation Loss: 5.23167873325292e-05\n","Epoch 180: Training Loss: 0.0012387416518322425 NSE : 0.7591496109962463 WAPE : 25.126084509391088 Validation Loss: 5.209575829212554e-05\n","Epoch 181: Training Loss: 0.0012353608599369181 NSE : 0.760245144367218 WAPE : 25.05423693481478 Validation Loss: 5.185879490454681e-05\n","Epoch 182: Training Loss: 0.0012320781534072012 NSE : 0.7599518299102783 WAPE : 25.092826916261906 Validation Loss: 5.19222448929213e-05\n","Epoch 183: Training Loss: 0.0012287163945075008 NSE : 0.7617504000663757 WAPE : 24.959990410873235 Validation Loss: 5.1533206715248525e-05\n","Epoch 184: Training Loss: 0.0012253739450898138 NSE : 0.7633941173553467 WAPE : 24.84210877405099 Validation Loss: 5.1177674322389066e-05\n","Epoch 185: Training Loss: 0.0012221902206874802 NSE : 0.7636795043945312 WAPE : 24.833299284984676 Validation Loss: 5.111593782203272e-05\n","Epoch 186: Training Loss: 0.0012190214674774325 NSE : 0.7637215256690979 WAPE : 24.843638865147692 Validation Loss: 5.110685742693022e-05\n","Epoch 187: Training Loss: 0.001215791911818087 NSE : 0.7649017572402954 WAPE : 24.760880531988075 Validation Loss: 5.085155862616375e-05\n","Epoch 188: Training Loss: 0.0012125677912990795 NSE : 0.7664023637771606 WAPE : 24.654929019616016 Validation Loss: 5.052698907093145e-05\n","Epoch 189: Training Loss: 0.001209468077377096 NSE : 0.7673179507255554 WAPE : 24.59450084426007 Validation Loss: 5.03289484186098e-05\n","Epoch 190: Training Loss: 0.0012064732777616882 NSE : 0.767068088054657 WAPE : 24.62772508390486 Validation Loss: 5.038299786974676e-05\n","Epoch 191: Training Loss: 0.0012034153787681134 NSE : 0.7678428292274475 WAPE : 24.577611473598633 Validation Loss: 5.0215421651955694e-05\n","Epoch 192: Training Loss: 0.0012003120768895315 NSE : 0.7698571085929871 WAPE : 24.425569614975714 Validation Loss: 4.977973367203958e-05\n","Epoch 193: Training Loss: 0.0011973181885878148 NSE : 0.7700749039649963 WAPE : 24.42426674449146 Validation Loss: 4.973262548446655e-05\n","Epoch 194: Training Loss: 0.0011944275065616239 NSE : 0.7698620557785034 WAPE : 24.45547935210857 Validation Loss: 4.9778667744249105e-05\n","Epoch 195: Training Loss: 0.0011914162701032183 NSE : 0.7719740271568298 WAPE : 24.293373079568905 Validation Loss: 4.932184310746379e-05\n","Epoch 196: Training Loss: 0.0011884983014169848 NSE : 0.7728635668754578 WAPE : 24.232494632173605 Validation Loss: 4.9129437684314325e-05\n","Epoch 197: Training Loss: 0.0011856823730340693 NSE : 0.7728068232536316 WAPE : 24.250803610514684 Validation Loss: 4.914171950076707e-05\n","Epoch 198: Training Loss: 0.0011828643200715305 NSE : 0.7730748653411865 WAPE : 24.24166475578996 Validation Loss: 4.9083744670497254e-05\n","Epoch 199: Training Loss: 0.0011799909398177988 NSE : 0.7740119099617004 WAPE : 24.179158241437534 Validation Loss: 4.888105104328133e-05\n","Epoch 200: Training Loss: 0.0011771749873332737 NSE : 0.7752141356468201 WAPE : 24.091951387296493 Validation Loss: 4.862101559410803e-05\n","Epoch 201: Training Loss: 0.001174419103335822 NSE : 0.7766855955123901 WAPE : 23.981026036563758 Validation Loss: 4.830273246625438e-05\n","Epoch 202: Training Loss: 0.0011717206775756495 NSE : 0.776715099811554 WAPE : 23.992937399678972 Validation Loss: 4.8296351451426744e-05\n","Epoch 203: Training Loss: 0.0011690321939568094 NSE : 0.776446521282196 WAPE : 24.029251005816015 Validation Loss: 4.835444633499719e-05\n","Epoch 204: Training Loss: 0.0011662637684821675 NSE : 0.7779882550239563 WAPE : 23.912457526422212 Validation Loss: 4.802096736966632e-05\n","Epoch 205: Training Loss: 0.0011635772470981465 NSE : 0.7791837453842163 WAPE : 23.824477288361717 Validation Loss: 4.7762390749994665e-05\n","Epoch 206: Training Loss: 0.0011609729840529326 NSE : 0.7796335220336914 WAPE : 23.79925788497217 Validation Loss: 4.766510392073542e-05\n","Epoch 207: Training Loss: 0.0011584215503717132 NSE : 0.7793322801589966 WAPE : 23.83662629505326 Validation Loss: 4.7730256483191624e-05\n","Epoch 208: Training Loss: 0.0011558345800040115 NSE : 0.7801971435546875 WAPE : 23.77681932834421 Validation Loss: 4.754318797495216e-05\n","Epoch 209: Training Loss: 0.0011532106900631334 NSE : 0.7815403938293457 WAPE : 23.6755873340143 Validation Loss: 4.725264443550259e-05\n","Epoch 210: Training Loss: 0.0011506688861118164 NSE : 0.7824097871780396 WAPE : 23.614225261095246 Validation Loss: 4.7064590035006404e-05\n","Epoch 211: Training Loss: 0.0011481932269816753 NSE : 0.7828079462051392 WAPE : 23.593963019324175 Validation Loss: 4.697848635260016e-05\n","Epoch 212: Training Loss: 0.0011456769389042165 NSE : 0.7835549116134644 WAPE : 23.54374101019366 Validation Loss: 4.681690552388318e-05\n","Epoch 213: Training Loss: 0.0011432157248236763 NSE : 0.7839420437812805 WAPE : 23.523191094619666 Validation Loss: 4.6733173803659156e-05\n","Epoch 214: Training Loss: 0.0011407961374061415 NSE : 0.7846605181694031 WAPE : 23.47426153300953 Validation Loss: 4.657776662497781e-05\n","Epoch 215: Training Loss: 0.001138357354193431 NSE : 0.7854436635971069 WAPE : 23.419991244710346 Validation Loss: 4.640838233171962e-05\n","Epoch 216: Training Loss: 0.0011359762247593608 NSE : 0.786041796207428 WAPE : 23.380940568259987 Validation Loss: 4.627899033948779e-05\n","Epoch 217: Training Loss: 0.0011335965659782232 NSE : 0.7865967750549316 WAPE : 23.34578599570574 Validation Loss: 4.615895159076899e-05\n","Epoch 218: Training Loss: 0.001131233977048396 NSE : 0.7873539328575134 WAPE : 23.2931667048842 Validation Loss: 4.599518797476776e-05\n","Epoch 219: Training Loss: 0.0011289586645943928 NSE : 0.7877399325370789 WAPE : 23.272685580871777 Validation Loss: 4.5911696361145005e-05\n","Epoch 220: Training Loss: 0.0011266355586485588 NSE : 0.7882508039474487 WAPE : 23.24170436305268 Validation Loss: 4.580118911690079e-05\n","Epoch 221: Training Loss: 0.001124352316310251 NSE : 0.7884773015975952 WAPE : 23.233849617477226 Validation Loss: 4.57522037322633e-05\n","Epoch 222: Training Loss: 0.0011220813958061626 NSE : 0.7898737788200378 WAPE : 23.125830189072566 Validation Loss: 4.545014235191047e-05\n","Epoch 223: Training Loss: 0.0011198012944078073 NSE : 0.7908318638801575 WAPE : 23.056077630234935 Validation Loss: 4.52429085271433e-05\n","Epoch 224: Training Loss: 0.0011175875806657132 NSE : 0.7910880446434021 WAPE : 23.04703675137062 Validation Loss: 4.518750210991129e-05\n","Epoch 225: Training Loss: 0.0011154588714816782 NSE : 0.7909561991691589 WAPE : 23.070202830876987 Validation Loss: 4.521601294982247e-05\n","Epoch 226: Training Loss: 0.001113241547045618 NSE : 0.791785717010498 WAPE : 23.009730879072773 Validation Loss: 4.503658783505671e-05\n","Epoch 227: Training Loss: 0.0011110516688859207 NSE : 0.7927033305168152 WAPE : 22.94203998248942 Validation Loss: 4.4838110625278205e-05\n","Epoch 228: Training Loss: 0.0011089308823102328 NSE : 0.7933832406997681 WAPE : 22.895290904921723 Validation Loss: 4.4691052607959136e-05\n","Epoch 229: Training Loss: 0.0011067792006542732 NSE : 0.7944090366363525 WAPE : 22.819616018010883 Validation Loss: 4.4469165004557e-05\n","Epoch 230: Training Loss: 0.001104687171846308 NSE : 0.7945433855056763 WAPE : 22.820616622542786 Validation Loss: 4.444011574378237e-05\n","Epoch 231: Training Loss: 0.0011026469651369553 NSE : 0.7945149540901184 WAPE : 22.83606137041129 Validation Loss: 4.444625665200874e-05\n","Epoch 232: Training Loss: 0.00110051733418004 NSE : 0.7952484488487244 WAPE : 22.783533801671844 Validation Loss: 4.428760803421028e-05\n","Epoch 233: Training Loss: 0.0010984759628627216 NSE : 0.7957745790481567 WAPE : 22.74677826186654 Validation Loss: 4.4173808419145644e-05\n","Epoch 234: Training Loss: 0.0010964676725961908 NSE : 0.79665207862854 WAPE : 22.680148423005566 Validation Loss: 4.3983996874885634e-05\n","Epoch 235: Training Loss: 0.0010944269774881832 NSE : 0.7976688742637634 WAPE : 22.603768943736842 Validation Loss: 4.376406650408171e-05\n","Epoch 236: Training Loss: 0.0010924313314717438 NSE : 0.7976393699645996 WAPE : 22.618192241145692 Validation Loss: 4.377045115688816e-05\n","Epoch 237: Training Loss: 0.0010905337226176925 NSE : 0.7972585558891296 WAPE : 22.66276708844927 Validation Loss: 4.3852825911017135e-05\n","Epoch 238: Training Loss: 0.0010885117540055944 NSE : 0.7985175848007202 WAPE : 22.56300264743282 Validation Loss: 4.358050136943348e-05\n","Epoch 239: Training Loss: 0.0010865320446100668 NSE : 0.7996797561645508 WAPE : 22.472660565758478 Validation Loss: 4.3329124309821054e-05\n","Epoch 240: Training Loss: 0.001084651241399115 NSE : 0.7998932600021362 WAPE : 22.465802255529383 Validation Loss: 4.328293653088622e-05\n","Epoch 241: Training Loss: 0.0010827722403519147 NSE : 0.7997350096702576 WAPE : 22.49075691563653 Validation Loss: 4.3317177187418565e-05\n","Epoch 242: Training Loss: 0.0010808828019435168 NSE : 0.799834668636322 WAPE : 22.491905526255447 Validation Loss: 4.329560761107132e-05\n","Epoch 243: Training Loss: 0.001078957035588246 NSE : 0.8011962175369263 WAPE : 22.3821308707344 Validation Loss: 4.300110231270082e-05\n","Epoch 244: Training Loss: 0.0010770736626000144 NSE : 0.8016747236251831 WAPE : 22.349573700777555 Validation Loss: 4.289759817766026e-05\n","Epoch 245: Training Loss: 0.0010752823945949785 NSE : 0.8022458553314209 WAPE : 22.31076692168185 Validation Loss: 4.2774074245244265e-05\n","Epoch 246: Training Loss: 0.001073452765467664 NSE : 0.8022744059562683 WAPE : 22.319730670613495 Validation Loss: 4.276790423318744e-05\n","Epoch 247: Training Loss: 0.0010716207593759464 NSE : 0.8025264143943787 WAPE : 22.308415501031874 Validation Loss: 4.2713381844805554e-05\n","Epoch 248: Training Loss: 0.0010698112459976983 NSE : 0.8037192225456238 WAPE : 22.211963477934585 Validation Loss: 4.2455383663764223e-05\n","Epoch 249: Training Loss: 0.0010679979732231004 NSE : 0.8044670820236206 WAPE : 22.15427862667028 Validation Loss: 4.2293613660149276e-05\n","Epoch 250: Training Loss: 0.0010663634034244751 NSE : 0.8036295175552368 WAPE : 22.23876925642576 Validation Loss: 4.247479955665767e-05\n","Epoch 251: Training Loss: 0.0010645486704561336 NSE : 0.8041378855705261 WAPE : 22.203502115861667 Validation Loss: 4.2364827095298097e-05\n","Epoch 252: Training Loss: 0.0010627667279550224 NSE : 0.8051896691322327 WAPE : 22.119317921244086 Validation Loss: 4.2137326090596616e-05\n","Epoch 253: Training Loss: 0.0010610158874442277 NSE : 0.8065683245658875 WAPE : 22.006433053303038 Validation Loss: 4.183913188171573e-05\n","Epoch 254: Training Loss: 0.001059361948591686 NSE : 0.8057998418807983 WAPE : 22.08393195889183 Validation Loss: 4.200535113341175e-05\n","Epoch 255: Training Loss: 0.0010577451985227526 NSE : 0.8056995272636414 WAPE : 22.101824018677952 Validation Loss: 4.202704076305963e-05\n","Epoch 256: Training Loss: 0.0010559678444224119 NSE : 0.8072741031646729 WAPE : 21.971561985366158 Validation Loss: 4.168646046309732e-05\n","Epoch 257: Training Loss: 0.0010542478112256504 NSE : 0.8079913854598999 WAPE : 21.91745221071064 Validation Loss: 4.153131885686889e-05\n","Epoch 258: Training Loss: 0.0010526920609663648 NSE : 0.8074852824211121 WAPE : 21.971176335702822 Validation Loss: 4.164078927715309e-05\n","Epoch 259: Training Loss: 0.001051086199368001 NSE : 0.8079355359077454 WAPE : 21.940228471368115 Validation Loss: 4.154340422246605e-05\n","Epoch 260: Training Loss: 0.0010493527693142823 NSE : 0.8094614744186401 WAPE : 21.815059098205168 Validation Loss: 4.121333404327743e-05\n","Epoch 261: Training Loss: 0.0010477474534127396 NSE : 0.809237539768219 WAPE : 21.84518771757937 Validation Loss: 4.126178100705147e-05\n","Epoch 262: Training Loss: 0.0010461892902640102 NSE : 0.8095635771751404 WAPE : 21.82512976590023 Validation Loss: 4.119125514989719e-05\n","Epoch 263: Training Loss: 0.0010446126721035398 NSE : 0.8101389408111572 WAPE : 21.78310437556023 Validation Loss: 4.106679625692777e-05\n","Epoch 264: Training Loss: 0.0010429542790006963 NSE : 0.8107679486274719 WAPE : 21.735927956473702 Validation Loss: 4.093075403943658e-05\n","Epoch 265: Training Loss: 0.001041453176185314 NSE : 0.8109383583068848 WAPE : 21.72785015947135 Validation Loss: 4.089389767614193e-05\n","Epoch 266: Training Loss: 0.001039956488966709 NSE : 0.8114127516746521 WAPE : 21.693560692918638 Validation Loss: 4.079127756995149e-05\n","Epoch 267: Training Loss: 0.0010383948842900281 NSE : 0.8117133378982544 WAPE : 21.6771757937087 Validation Loss: 4.072627052664757e-05\n","Epoch 268: Training Loss: 0.0010368220682721585 NSE : 0.8121135234832764 WAPE : 21.65080152592191 Validation Loss: 4.0639697544975206e-05\n","Epoch 269: Training Loss: 0.0010353241750635789 NSE : 0.8129055500030518 WAPE : 21.58700256404911 Validation Loss: 4.046837784699164e-05\n","Epoch 270: Training Loss: 0.001033884238495375 NSE : 0.8131415843963623 WAPE : 21.573408934564632 Validation Loss: 4.0417340642306954e-05\n","Epoch 271: Training Loss: 0.0010323656952095916 NSE : 0.8130121231079102 WAPE : 21.59454253611557 Validation Loss: 4.044533125124872e-05\n","Epoch 272: Training Loss: 0.0010308675978194515 NSE : 0.8135490417480469 WAPE : 21.554922765837695 Validation Loss: 4.032921060570516e-05\n","Epoch 273: Training Loss: 0.0010294063845321944 NSE : 0.8149186968803406 WAPE : 21.43850242855058 Validation Loss: 4.003294088761322e-05\n","Epoch 274: Training Loss: 0.0010279652315148269 NSE : 0.8147162795066833 WAPE : 21.464084551082944 Validation Loss: 4.007672760053538e-05\n","Epoch 275: Training Loss: 0.0010265783967042807 NSE : 0.8144328594207764 WAPE : 21.498843051009985 Validation Loss: 4.013803481939249e-05\n","Epoch 276: Training Loss: 0.0010251360254187603 NSE : 0.8153725862503052 WAPE : 21.422177982531114 Validation Loss: 3.993477002950385e-05\n","Epoch 277: Training Loss: 0.001023620879095688 NSE : 0.8164374232292175 WAPE : 21.333724541910737 Validation Loss: 3.970444595324807e-05\n","Epoch 278: Training Loss: 0.0010222926562164503 NSE : 0.8161794543266296 WAPE : 21.366014883992413 Validation Loss: 3.9760241634212434e-05\n","Epoch 279: Training Loss: 0.0010209676643171406 NSE : 0.8160977363586426 WAPE : 21.381286610660606 Validation Loss: 3.97779222112149e-05\n","Epoch 280: Training Loss: 0.0010194872247666353 NSE : 0.8168383836746216 WAPE : 21.322740822580307 Validation Loss: 3.961772017646581e-05\n","Epoch 281: Training Loss: 0.001018131591990823 NSE : 0.8172462582588196 WAPE : 21.293473140022094 Validation Loss: 3.952949555241503e-05\n","Epoch 282: Training Loss: 0.0010167758637180668 NSE : 0.8180290460586548 WAPE : 21.22875487273561 Validation Loss: 3.936018765671179e-05\n","Epoch 283: Training Loss: 0.0010153934272238985 NSE : 0.8182494640350342 WAPE : 21.21818181818182 Validation Loss: 3.931249739252962e-05\n","Epoch 284: Training Loss: 0.0010141053098777775 NSE : 0.8181124329566956 WAPE : 21.239919951637447 Validation Loss: 3.9342150557786226e-05\n","Epoch 285: Training Loss: 0.0010127796795131871 NSE : 0.8179666996002197 WAPE : 21.260019595172082 Validation Loss: 3.937367728212848e-05\n","Epoch 286: Training Loss: 0.0010114069864357589 NSE : 0.8193725347518921 WAPE : 21.14091013320548 Validation Loss: 3.906957863364369e-05\n","Epoch 287: Training Loss: 0.001010092837987031 NSE : 0.8195366859436035 WAPE : 21.13195472264493 Validation Loss: 3.903407559846528e-05\n","Epoch 288: Training Loss: 0.0010088145436384366 NSE : 0.8198737502098083 WAPE : 21.1081069813012 Validation Loss: 3.896117414114997e-05\n","Epoch 289: Training Loss: 0.0010075272853100614 NSE : 0.8207765817642212 WAPE : 21.03609263930291 Validation Loss: 3.8765887438785285e-05\n","Epoch 290: Training Loss: 0.0010062466922136082 NSE : 0.8200074434280396 WAPE : 21.115219611848826 Validation Loss: 3.893226312357001e-05\n","Epoch 291: Training Loss: 0.0010050038131339534 NSE : 0.8205753564834595 WAPE : 21.069458631256385 Validation Loss: 3.880941585521214e-05\n","Epoch 292: Training Loss: 0.001003657306682726 NSE : 0.8217740654945374 WAPE : 20.96784724104146 Validation Loss: 3.855014074360952e-05\n","Epoch 293: Training Loss: 0.001002446430902637 NSE : 0.8219857811927795 WAPE : 20.957484730357926 Validation Loss: 3.8504334952449426e-05\n","Epoch 294: Training Loss: 0.0010012494904003688 NSE : 0.8218157291412354 WAPE : 20.97974192741448 Validation Loss: 3.854112947010435e-05\n","Epoch 295: Training Loss: 0.000999997053440893 NSE : 0.8217140436172485 WAPE : 20.994492505888974 Validation Loss: 3.8563117414014414e-05\n","Epoch 296: Training Loss: 0.000998775426978682 NSE : 0.822597861289978 WAPE : 20.922052906964623 Validation Loss: 3.8371952541638166e-05\n","Epoch 297: Training Loss: 0.0009975371062864724 NSE : 0.8230574131011963 WAPE : 20.886842050405452 Validation Loss: 3.827253749477677e-05\n","Epoch 298: Training Loss: 0.0009963907273231598 NSE : 0.8229116201400757 WAPE : 20.907054261949927 Validation Loss: 3.830408240901306e-05\n","Epoch 299: Training Loss: 0.0009951181059477676 NSE : 0.8232251405715942 WAPE : 20.887853077901234 Validation Loss: 3.8236266846070066e-05\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:36:11,416] Trial 4 finished with value: 3.793459109147079e-05 and parameters: {'lr': 0.000259387534514252, 'weight_decay': 0.000122227151992193}. Best is trial 1 with value: 2.6064595658681355e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 300: Training Loss: 0.0009939008136825578 NSE : 0.8246198892593384 WAPE : 20.766738237685267 Validation Loss: 3.793459109147079e-05\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:36:13,570] Trial 5 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006477113754954189 NSE : -11.79155158996582 WAPE : 220.41043130224512 Validation Loss: 0.002766803139820695\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:36:15,396] Trial 6 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005799000704428181 NSE : -14.516860961914062 WAPE : 243.1514706802026 Validation Loss: 0.003356285160407424\n","Epoch 1: Training Loss: 0.004887882263574284 NSE : -2.529541015625 WAPE : 112.27622521940339 Validation Loss: 0.0007634370704181492\n","Epoch 2: Training Loss: 0.0042556829466775525 NSE : -2.8079187870025635 WAPE : 118.03751433157532 Validation Loss: 0.0008236499270424247\n","Epoch 3: Training Loss: 0.0038771040126448497 NSE : -0.673729658126831 WAPE : 75.38834712638885 Validation Loss: 0.0003620264760684222\n","Epoch 4: Training Loss: 0.0038294053374556825 NSE : -1.2033917903900146 WAPE : 88.4849596631298 Validation Loss: 0.000476591958431527\n","Epoch 5: Training Loss: 0.003583232573873829 NSE : -0.33731400966644287 WAPE : 66.52217381334556 Validation Loss: 0.00028926003142260015\n","Epoch 6: Training Loss: 0.003515360273013357 NSE : -1.0132477283477783 WAPE : 84.64766629838861 Validation Loss: 0.00043546396773308516\n","Epoch 7: Training Loss: 0.003307651299110148 NSE : -0.3438069820404053 WAPE : 67.31329344812491 Validation Loss: 0.0002906644076574594\n","Epoch 8: Training Loss: 0.0031932779093040153 NSE : -0.29546916484832764 WAPE : 66.23259886181233 Validation Loss: 0.0002802090020850301\n","Epoch 9: Training Loss: 0.00308342173229903 NSE : -0.37783002853393555 WAPE : 69.02181734798107 Validation Loss: 0.0002980235731229186\n","Epoch 10: Training Loss: 0.0029551108655141434 NSE : -0.175881028175354 WAPE : 63.08418419461758 Validation Loss: 0.0002543421578593552\n","Epoch 11: Training Loss: 0.002857426681657671 NSE : 0.004866480827331543 WAPE : 57.18198078005462 Validation Loss: 0.00021524663316085935\n","Epoch 12: Training Loss: 0.0027844284541060915 NSE : 0.051464736461639404 WAPE : 55.71953888807821 Validation Loss: 0.00020516746735665947\n","Epoch 13: Training Loss: 0.0027166068084625294 NSE : 0.022286415100097656 WAPE : 56.96750953701194 Validation Loss: 0.00021147870575077832\n","Epoch 14: Training Loss: 0.0026458403717697365 NSE : 0.05044037103652954 WAPE : 56.13275520627046 Validation Loss: 0.00020538903481792659\n","Epoch 15: Training Loss: 0.0025758933970791986 NSE : 0.16690605878829956 WAPE : 52.009368159929956 Validation Loss: 0.00018019757408183068\n","Epoch 16: Training Loss: 0.0025175837236020016 NSE : 0.254749596118927 WAPE : 48.670288299180754 Validation Loss: 0.0001611970947124064\n","Epoch 17: Training Loss: 0.002469117471264326 NSE : 0.25887149572372437 WAPE : 48.644585270267456 Validation Loss: 0.00016030554252211004\n","Epoch 18: Training Loss: 0.002421128509013215 NSE : 0.22009575366973877 WAPE : 50.32273665339476 Validation Loss: 0.0001686926989350468\n","Epoch 19: Training Loss: 0.002370568314290722 NSE : 0.2290029525756836 WAPE : 50.00799232869859 Validation Loss: 0.0001667660690145567\n","Epoch 20: Training Loss: 0.002322010423085885 NSE : 0.2936398386955261 WAPE : 47.33204644472702 Validation Loss: 0.00015278517093975097\n","Epoch 21: Training Loss: 0.0022793512434873264 NSE : 0.32721805572509766 WAPE : 45.90795272143587 Validation Loss: 0.0001455222227377817\n","Epoch 22: Training Loss: 0.0022388518536899937 NSE : 0.3044942021369934 WAPE : 47.00414833962186 Validation Loss: 0.00015043739404063672\n","Epoch 23: Training Loss: 0.002195026321714977 NSE : 0.31205451488494873 WAPE : 46.74252360801317 Validation Loss: 0.0001488020789111033\n","Epoch 24: Training Loss: 0.0021518069916055538 NSE : 0.36050254106521606 WAPE : 44.61281607637948 Validation Loss: 0.0001383228081976995\n","Epoch 25: Training Loss: 0.00211161818333494 NSE : 0.4014930725097656 WAPE : 42.822059160742945 Validation Loss: 0.00012945658818352968\n","Epoch 26: Training Loss: 0.0020739177061841474 NSE : 0.4456669092178345 WAPE : 40.77163703070605 Validation Loss: 0.00011990182974841446\n","Epoch 27: Training Loss: 0.0020389218889249605 NSE : 0.4826663136482239 WAPE : 38.878772591774194 Validation Loss: 0.0001118988948292099\n","Epoch 28: Training Loss: 0.0020067494360773708 NSE : 0.47564131021499634 WAPE : 39.37865793917158 Validation Loss: 0.00011341839854139835\n","Epoch 29: Training Loss: 0.0019730191079361248 NSE : 0.48303091526031494 WAPE : 39.10019803631361 Validation Loss: 0.00011182002344867215\n","Epoch 30: Training Loss: 0.0019397511323404615 NSE : 0.510528564453125 WAPE : 37.66937941673094 Validation Loss: 0.00010587229917291552\n","Epoch 31: Training Loss: 0.0019087349746769178 NSE : 0.513861894607544 WAPE : 37.59706072418753 Validation Loss: 0.00010515130270505324\n","Epoch 32: Training Loss: 0.0018780790114760748 NSE : 0.5204057097434998 WAPE : 37.33613224656563 Validation Loss: 0.00010373587429057807\n","Epoch 33: Training Loss: 0.0018479681648386759 NSE : 0.5392951965332031 WAPE : 36.37120760459444 Validation Loss: 9.965008939616382e-05\n","Epoch 34: Training Loss: 0.0018193867972513544 NSE : 0.5598561763763428 WAPE : 35.28250818202664 Validation Loss: 9.520278399577364e-05\n","Epoch 35: Training Loss: 0.0017922316765179858 NSE : 0.574669599533081 WAPE : 34.50471743344938 Validation Loss: 9.199864871334285e-05\n","Epoch 36: Training Loss: 0.0017659008126429399 NSE : 0.5872318744659424 WAPE : 33.851712492964495 Validation Loss: 8.928144234232605e-05\n","Epoch 37: Training Loss: 0.0017403739320798195 NSE : 0.6002109050750732 WAPE : 33.17685685101415 Validation Loss: 8.647407230455428e-05\n","Epoch 38: Training Loss: 0.0017155017876575585 NSE : 0.6168562173843384 WAPE : 32.19673552771466 Validation Loss: 8.28737101983279e-05\n","Epoch 39: Training Loss: 0.001691838308943261 NSE : 0.6265329122543335 WAPE : 31.729790915344687 Validation Loss: 8.078066457528621e-05\n","Epoch 40: Training Loss: 0.0016684046895534266 NSE : 0.6423340439796448 WAPE : 30.754695545225243 Validation Loss: 7.736287079751492e-05\n","Epoch 41: Training Loss: 0.0016459959815620095 NSE : 0.6552928686141968 WAPE : 30.01112755623189 Validation Loss: 7.455989543814212e-05\n","Epoch 42: Training Loss: 0.0016241687508227187 NSE : 0.6705830097198486 WAPE : 29.013916741364575 Validation Loss: 7.125264528440312e-05\n","Epoch 43: Training Loss: 0.0016031819068302866 NSE : 0.6807010769844055 WAPE : 28.45178545371162 Validation Loss: 6.906412454554811e-05\n","Epoch 44: Training Loss: 0.001582414603944926 NSE : 0.6944613456726074 WAPE : 27.52509432782306 Validation Loss: 6.608777039218694e-05\n","Epoch 45: Training Loss: 0.0015623175640939735 NSE : 0.7064645290374756 WAPE : 26.695036584603198 Validation Loss: 6.349149771267548e-05\n","Epoch 46: Training Loss: 0.0015428686874656705 NSE : 0.7127620577812195 WAPE : 26.360134247774695 Validation Loss: 6.212934385985136e-05\n","Epoch 47: Training Loss: 0.0015233116700983373 NSE : 0.7227335572242737 WAPE : 25.701027704237976 Validation Loss: 5.997251719236374e-05\n","Epoch 48: Training Loss: 0.0015037756966194138 NSE : 0.7339746356010437 WAPE : 24.894142294302807 Validation Loss: 5.75410776946228e-05\n","Epoch 49: Training Loss: 0.0014838018478258164 NSE : 0.7418744564056396 WAPE : 24.3671989326885 Validation Loss: 5.5832351790741086e-05\n","Epoch 50: Training Loss: 0.0014637449185102014 NSE : 0.7501351237297058 WAPE : 23.795822476079298 Validation Loss: 5.404558760346845e-05\n","Epoch 51: Training Loss: 0.0014451787847065134 NSE : 0.763024628162384 WAPE : 22.75786412624294 Validation Loss: 5.125759344082326e-05\n","Epoch 52: Training Loss: 0.0014278253074735403 NSE : 0.7741108536720276 WAPE : 21.856940651643704 Validation Loss: 4.88596415380016e-05\n","Epoch 53: Training Loss: 0.001411797349646804 NSE : 0.7804162502288818 WAPE : 21.36155594004711 Validation Loss: 4.749579602503218e-05\n","Epoch 54: Training Loss: 0.0013965110510980594 NSE : 0.7874548435211182 WAPE : 20.779306247524545 Validation Loss: 4.59733564639464e-05\n","Epoch 55: Training Loss: 0.0013820139274685062 NSE : 0.7914152145385742 WAPE : 20.496033019949554 Validation Loss: 4.5116736146155745e-05\n","Epoch 56: Training Loss: 0.0013681454838661011 NSE : 0.789713978767395 WAPE : 20.803056013007858 Validation Loss: 4.548470315057784e-05\n","Epoch 57: Training Loss: 0.0013542580636567436 NSE : 0.7857835292816162 WAPE : 21.311867586666942 Validation Loss: 4.633485514204949e-05\n","Epoch 58: Training Loss: 0.0013403446091615479 NSE : 0.777306854724884 WAPE : 22.22696212294928 Validation Loss: 4.8168360081035644e-05\n","Epoch 59: Training Loss: 0.0013260873947729124 NSE : 0.7637110948562622 WAPE : 23.510239519709824 Validation Loss: 5.1109112973790616e-05\n","Epoch 60: Training Loss: 0.001311474768044718 NSE : 0.7476574778556824 WAPE : 24.93053928415084 Validation Loss: 5.458149826154113e-05\n","Epoch 61: Training Loss: 0.0012966379299541586 NSE : 0.7305980920791626 WAPE : 26.34633007441996 Validation Loss: 5.827142740599811e-05\n","Epoch 62: Training Loss: 0.0012817428305424983 NSE : 0.7147562503814697 WAPE : 27.60983302411874 Validation Loss: 6.169800326460972e-05\n","Epoch 63: Training Loss: 0.001266669349206495 NSE : 0.704915463924408 WAPE : 28.38332534239436 Validation Loss: 6.382657011272386e-05\n","Epoch 64: Training Loss: 0.0012516316082837875 NSE : 0.7026201486587524 WAPE : 28.633943424152093 Validation Loss: 6.432303052861243e-05\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:38:01,163] Trial 7 finished with value: 4.5116736146155745e-05 and parameters: {'lr': 0.0005573245799166213, 'weight_decay': 2.828400805604859e-05}. Best is trial 1 with value: 2.6064595658681355e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 65: Training Loss: 0.0012368300785965403 NSE : 0.7098021507263184 WAPE : 28.242379771111715 Validation Loss: 6.276956992223859e-05\n","Early stopping!\n","Epoch 1: Training Loss: 0.006096231914852979 NSE : 0.079159677028656 WAPE : 37.23342019136562 Validation Loss: 0.00019917705503758043\n","Epoch 2: Training Loss: 0.004494542896281928 NSE : 0.2339717149734497 WAPE : 42.74590064830836 Validation Loss: 0.00016569133731536567\n","Epoch 3: Training Loss: 0.004544290466583334 NSE : -6.049798488616943 WAPE : 163.18235600675408 Validation Loss: 0.0015248663257807493\n","Epoch 4: Training Loss: 0.003831121666735271 NSE : -0.673474907875061 WAPE : 75.08384649058807 Validation Loss: 0.00036197135341353714\n","Epoch 5: Training Loss: 0.003845153965812642 NSE : -2.101013422012329 WAPE : 106.47007567071773 Validation Loss: 0.0006707468419335783\n","Epoch 6: Training Loss: 0.0036755943874595687 NSE : -1.4833259582519531 WAPE : 94.55477684434345 Validation Loss: 0.0005371415172703564\n","Epoch 7: Training Loss: 0.0035382635760470293 NSE : -0.8221317529678345 WAPE : 79.71651205936921 Validation Loss: 0.00039412573096342385\n","Epoch 8: Training Loss: 0.00353816882125102 NSE : -1.393683671951294 WAPE : 92.93514831877593 Validation Loss: 0.0005177519051358104\n","Epoch 9: Training Loss: 0.003430074779316783 NSE : -1.0007822513580322 WAPE : 84.23365783494194 Validation Loss: 0.0004327676724642515\n","Epoch 10: Training Loss: 0.0033864217111840844 NSE : -0.9562116861343384 WAPE : 83.26557295032416 Validation Loss: 0.0004231271450407803\n","Epoch 11: Training Loss: 0.003358886038768105 NSE : -1.0638999938964844 WAPE : 85.84791644952159 Validation Loss: 0.0004464200173970312\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:38:20,382] Trial 8 finished with value: 0.00016569133731536567 and parameters: {'lr': 0.0027634910240079617, 'weight_decay': 0.004594261134375954}. Best is trial 1 with value: 2.6064595658681355e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12: Training Loss: 0.003308808631118154 NSE : -0.8995809555053711 WAPE : 82.00699589335223 Validation Loss: 0.0004108779248781502\n","Early stopping!\n","Epoch 1: Training Loss: 0.006816296739998506 NSE : -0.6341276168823242 WAPE : 70.90146963790623 Validation Loss: 0.0003534605784807354\n","Epoch 2: Training Loss: 0.004758800678246189 NSE : -7.1681108474731445 WAPE : 174.12550082341414 Validation Loss: 0.0017667562933638692\n","Epoch 3: Training Loss: 0.0046842400333844125 NSE : -8.496514320373535 WAPE : 188.5248504304684 Validation Loss: 0.0020540887489914894\n","Epoch 4: Training Loss: 0.004547443641058635 NSE : -5.055340766906738 WAPE : 149.20754622584477 Validation Loss: 0.0013097655028104782\n","Epoch 5: Training Loss: 0.004429170814546524 NSE : -2.9996743202209473 WAPE : 120.12223218194326 Validation Loss: 0.0008651264943182468\n","Epoch 6: Training Loss: 0.0044018423213856295 NSE : -2.9057517051696777 WAPE : 118.79035250463821 Validation Loss: 0.0008448112057521939\n","Epoch 7: Training Loss: 0.0043354566987545695 NSE : -3.5356040000915527 WAPE : 128.7633945508745 Validation Loss: 0.0009810477495193481\n","Epoch 8: Training Loss: 0.004267784115654649 NSE : -3.6107749938964844 WAPE : 130.02387275645702 Validation Loss: 0.0009973071282729506\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:38:34,653] Trial 9 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9: Training Loss: 0.004205226003250573 NSE : -3.066843032836914 WAPE : 121.85820599945801 Validation Loss: 0.0008796551846899092\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:38:36,846] Trial 10 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.015584142791340128 NSE : -7.209973335266113 WAPE : 176.25345312793146 Validation Loss: 0.00177581119351089\n","Epoch 1: Training Loss: 0.0064180281551671214 NSE : -0.7411431074142456 WAPE : 73.01061891559483 Validation Loss: 0.0003766079607885331\n","Epoch 2: Training Loss: 0.005114288953336654 NSE : -5.201085567474365 WAPE : 150.1091409393175 Validation Loss: 0.0013412900734692812\n","Epoch 3: Training Loss: 0.004875540529610589 NSE : -8.202962875366211 WAPE : 184.6864814158554 Validation Loss: 0.0019905937369912863\n","Epoch 4: Training Loss: 0.004810304926650133 NSE : -6.991926670074463 WAPE : 171.71114214838133 Validation Loss: 0.0017286476213485003\n","Epoch 5: Training Loss: 0.004659933474613354 NSE : -4.77679967880249 WAPE : 145.10750244939652 Validation Loss: 0.0012495171977207065\n","Epoch 6: Training Loss: 0.004559614200843498 NSE : -3.6693992614746094 WAPE : 130.03096037189135 Validation Loss: 0.0010099874343723059\n","Epoch 7: Training Loss: 0.004482555530557875 NSE : -3.613158702850342 WAPE : 129.42949698776346 Validation Loss: 0.0009978227317333221\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:38:49,787] Trial 11 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8: Training Loss: 0.004397079919726821 NSE : -3.8383045196533203 WAPE : 132.91402722478162 Validation Loss: 0.0010465214727446437\n","Epoch 1: Training Loss: 0.012157930243120063 NSE : -2.5542497634887695 WAPE : 105.51870088178275 Validation Loss: 0.0007687816396355629\n","Epoch 2: Training Loss: 0.005266069419121777 NSE : 0.17844253778457642 WAPE : 34.38642096266494 Validation Loss: 0.00017770226986613125\n","Epoch 3: Training Loss: 0.0036727470305777388 NSE : 0.46203720569610596 WAPE : 27.85018031727502 Validation Loss: 0.00011636094131972641\n","Epoch 4: Training Loss: 0.0031153668378465227 NSE : 0.437788724899292 WAPE : 31.097312959913282 Validation Loss: 0.00012160587357357144\n","Epoch 5: Training Loss: 0.0029697349018533714 NSE : -0.5017558336257935 WAPE : 68.44770799024411 Validation Loss: 0.0003248286375310272\n","Epoch 6: Training Loss: 0.0027337786814314313 NSE : -1.0459094047546387 WAPE : 82.05470805278189 Validation Loss: 0.00044252866064198315\n","Epoch 7: Training Loss: 0.0028834632466896437 NSE : -2.7769103050231934 WAPE : 117.15236288591025 Validation Loss: 0.0008169428328983486\n","Epoch 8: Training Loss: 0.0033988426221185364 NSE : -3.2329940795898438 WAPE : 122.62688707760938 Validation Loss: 0.0009155934676527977\n","Epoch 9: Training Loss: 0.0035825190407194896 NSE : -0.7276023626327515 WAPE : 73.13211732088136 Validation Loss: 0.00037367912591435015\n","Epoch 10: Training Loss: 0.0029167428183427546 NSE : -2.40909743309021 WAPE : 110.2258864730775 Validation Loss: 0.0007373852422460914\n","Epoch 11: Training Loss: 0.002919422993727494 NSE : -0.5300240516662598 WAPE : 68.93123762273041 Validation Loss: 0.0003309430321678519\n","Epoch 12: Training Loss: 0.002429732783639338 NSE : -1.5007915496826172 WAPE : 92.50940776719267 Validation Loss: 0.0005409193690866232\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:39:10,086] Trial 12 finished with value: 0.00011636094131972641 and parameters: {'lr': 0.009634979850142607, 'weight_decay': 0.0006553668501896673}. Best is trial 1 with value: 2.6064595658681355e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13: Training Loss: 0.00271550707248025 NSE : -0.1659301519393921 WAPE : 56.24225886473078 Validation Loss: 0.0002521898131817579\n","Early stopping!\n","Epoch 1: Training Loss: 0.005237855541054159 NSE : -5.707724094390869 WAPE : 158.7370703133143 Validation Loss: 0.001450875774025917\n","Epoch 2: Training Loss: 0.0037496191880563856 NSE : -0.20109891891479492 WAPE : 49.765816847678806 Validation Loss: 0.0002597967686597258\n","Epoch 3: Training Loss: 0.004141281573538436 NSE : -0.23812973499298096 WAPE : 52.60059619353359 Validation Loss: 0.00026780652115121484\n","Epoch 4: Training Loss: 0.004326081820181571 NSE : -7.192614555358887 WAPE : 176.6950365846032 Validation Loss: 0.0017720561008900404\n","Epoch 5: Training Loss: 0.003364204545505345 NSE : 0.42430824041366577 WAPE : 37.642684121656835 Validation Loss: 0.00012452169903554022\n","Epoch 6: Training Loss: 0.003132641973934369 NSE : 0.3622763156890869 WAPE : 33.51794209001272 Validation Loss: 0.00013793916150461882\n","Epoch 7: Training Loss: 0.0032410690910182893 NSE : -2.4724409580230713 WAPE : 114.86186237518501 Validation Loss: 0.0007510862778872252\n","Epoch 8: Training Loss: 0.0027078910134150647 NSE : -0.31038331985473633 WAPE : 68.38947280648725 Validation Loss: 0.0002834349579643458\n","Epoch 9: Training Loss: 0.0023562177584608435 NSE : 0.6339950561523438 WAPE : 23.34821037710283 Validation Loss: 7.916659524198622e-05\n","Epoch 10: Training Loss: 0.0024184045651054475 NSE : 0.38342714309692383 WAPE : 44.176852681828606 Validation Loss: 0.00013336424308363348\n","Epoch 11: Training Loss: 0.0023170716194726992 NSE : -0.8895473480224609 WAPE : 84.1889558274791 Validation Loss: 0.0004087076522409916\n","Epoch 12: Training Loss: 0.0020675624673458515 NSE : 0.27014797925949097 WAPE : 49.540509891392716 Validation Loss: 0.00015786648145876825\n","Epoch 13: Training Loss: 0.0019124576610920485 NSE : 0.6900774836540222 WAPE : 27.932363302828794 Validation Loss: 6.703600229229778e-05\n","Epoch 14: Training Loss: 0.0018388412627245998 NSE : 0.7873541116714478 WAPE : 19.372608450939108 Validation Loss: 4.5995155232958496e-05\n","Epoch 15: Training Loss: 0.0017942757258424535 NSE : 0.7985894680023193 WAPE : 17.216676742198413 Validation Loss: 4.356495264801197e-05\n","Epoch 16: Training Loss: 0.0017585179557499941 NSE : 0.6984084248542786 WAPE : 28.28415709491151 Validation Loss: 6.52340313536115e-05\n","Epoch 17: Training Loss: 0.0016916455369937466 NSE : 0.4678781032562256 WAPE : 41.781205311542394 Validation Loss: 0.00011509755131555721\n","Epoch 18: Training Loss: 0.0016144160144904163 NSE : 0.2347157597541809 WAPE : 51.915328010673115 Validation Loss: 0.0001655303785810247\n","Epoch 19: Training Loss: 0.001533837006718386 NSE : 0.2433147430419922 WAPE : 51.68782806278793 Validation Loss: 0.00016367045463994145\n","Epoch 20: Training Loss: 0.0014564126959157875 NSE : 0.4943360686302185 WAPE : 40.924708678159725 Validation Loss: 0.00010937471961369738\n","Epoch 21: Training Loss: 0.0013923436190452776 NSE : 0.7038741707801819 WAPE : 29.03366408872027 Validation Loss: 6.40517973806709e-05\n","Epoch 22: Training Loss: 0.0013501611369974853 NSE : 0.8150675296783447 WAPE : 19.668247482854223 Validation Loss: 4.000075932708569e-05\n","Epoch 23: Training Loss: 0.0013257734221951978 NSE : 0.8500040769577026 WAPE : 15.136396989848034 Validation Loss: 3.244400068069808e-05\n","Epoch 24: Training Loss: 0.0013103963951834885 NSE : 0.8512067794799805 WAPE : 14.580110900335619 Validation Loss: 3.218386336811818e-05\n","Epoch 25: Training Loss: 0.0013017046303502866 NSE : 0.8534313440322876 WAPE : 14.485493318880158 Validation Loss: 3.170269701513462e-05\n","Epoch 26: Training Loss: 0.0012977728101759567 NSE : 0.857883095741272 WAPE : 14.830078589147611 Validation Loss: 3.0739782232558355e-05\n","Epoch 27: Training Loss: 0.001289821366299293 NSE : 0.7855616211891174 WAPE : 23.292599695649454 Validation Loss: 4.638285463443026e-05\n","Epoch 28: Training Loss: 0.0012698950231424533 NSE : 0.5559815168380737 WAPE : 38.28443017656501 Validation Loss: 9.604086517356336e-05\n","Epoch 29: Training Loss: 0.0012402182474033907 NSE : 0.29945313930511475 WAPE : 49.90023138979801 Validation Loss: 0.00015152775449678302\n","Epoch 30: Training Loss: 0.0012058253432769561 NSE : 0.3171846866607666 WAPE : 49.28382147547477 Validation Loss: 0.00014769243716727942\n","Epoch 31: Training Loss: 0.001163264391834673 NSE : 0.6032654047012329 WAPE : 36.08454274457485 Validation Loss: 8.581339352531359e-05\n","Epoch 32: Training Loss: 0.0011241066481488815 NSE : 0.8155015707015991 WAPE : 21.404348460528237 Validation Loss: 3.990687764598988e-05\n","Epoch 33: Training Loss: 0.0011100058468400675 NSE : 0.8759014010429382 WAPE : 13.889440495299244 Validation Loss: 2.684243554540444e-05\n","Epoch 34: Training Loss: 0.0011165037303726422 NSE : 0.8699955940246582 WAPE : 13.738216839340433 Validation Loss: 2.8119849957874976e-05\n","Epoch 35: Training Loss: 0.00112531017521178 NSE : 0.8663113117218018 WAPE : 14.244984469783828 Validation Loss: 2.8916763767483644e-05\n","Epoch 36: Training Loss: 0.001133061835389526 NSE : 0.8798720836639404 WAPE : 13.360376060536575 Validation Loss: 2.5983579689636827e-05\n","Epoch 37: Training Loss: 0.0011346953542670235 NSE : 0.8532888889312744 WAPE : 17.429323966563132 Validation Loss: 3.173349250573665e-05\n","Epoch 38: Training Loss: 0.0011293071183899883 NSE : 0.7238808274269104 WAPE : 28.725133935085783 Validation Loss: 5.97243633819744e-05\n","Epoch 39: Training Loss: 0.001118885884352494 NSE : 0.49286073446273804 WAPE : 41.78415292572596 Validation Loss: 0.00010969383583869785\n","Epoch 40: Training Loss: 0.0011049496933992486 NSE : 0.35892486572265625 WAPE : 47.77342561130683 Validation Loss: 0.0001386640506098047\n","Epoch 41: Training Loss: 0.0010812376076501096 NSE : 0.49625611305236816 WAPE : 41.87479935794543 Validation Loss: 0.00010895940795307979\n","Epoch 42: Training Loss: 0.0010475865828993847 NSE : 0.7439340949058533 WAPE : 27.80695837068229 Validation Loss: 5.538685945793986e-05\n","Epoch 43: Training Loss: 0.0010265215814797557 NSE : 0.8675683736801147 WAPE : 16.553623022242604 Validation Loss: 2.864486668840982e-05\n","Epoch 44: Training Loss: 0.0010272775848534366 NSE : 0.8889279365539551 WAPE : 12.76815263388297 Validation Loss: 2.402480095042847e-05\n","Epoch 45: Training Loss: 0.001038099774177681 NSE : 0.8815985321998596 WAPE : 13.228181609722542 Validation Loss: 2.5610152079025283e-05\n","Epoch 46: Training Loss: 0.0010481583367436542 NSE : 0.8827167749404907 WAPE : 13.276402409789249 Validation Loss: 2.5368273782078177e-05\n","Epoch 47: Training Loss: 0.0010561267508819583 NSE : 0.8897887468338013 WAPE : 13.10063371620354 Validation Loss: 2.3838607376092114e-05\n","Epoch 48: Training Loss: 0.0010584188685243134 NSE : 0.8635063171386719 WAPE : 16.967534552125244 Validation Loss: 2.9523473131121136e-05\n","Epoch 49: Training Loss: 0.0010554518175922567 NSE : 0.7503951191902161 WAPE : 27.22971378541202 Validation Loss: 5.3989340813132e-05\n","Epoch 50: Training Loss: 0.001049712331223418 NSE : 0.5705554485321045 WAPE : 38.15868754038899 Validation Loss: 9.288853470934555e-05\n","Epoch 51: Training Loss: 0.001043116717482917 NSE : 0.4529569149017334 WAPE : 43.888532655145816 Validation Loss: 0.0001183250205940567\n","Epoch 52: Training Loss: 0.0010271443779856781 NSE : 0.535993218421936 WAPE : 40.10178649600801 Validation Loss: 0.0001003643119474873\n","Epoch 53: Training Loss: 0.00100114005681462 NSE : 0.7306398153305054 WAPE : 29.020745867294824 Validation Loss: 5.826241249451414e-05\n","Epoch 54: Training Loss: 0.0009824654371186625 NSE : 0.8601783514022827 WAPE : 17.93884430176565 Validation Loss: 3.0243321816669777e-05\n","Epoch 55: Training Loss: 0.0009815270818762656 NSE : 0.8954314589500427 WAPE : 12.73843155239624 Validation Loss: 2.2618090952164494e-05\n","Epoch 56: Training Loss: 0.000991626988252392 NSE : 0.8882784247398376 WAPE : 12.808761543432489 Validation Loss: 2.416529787296895e-05\n","Epoch 57: Training Loss: 0.0010034067145170411 NSE : 0.8840677738189697 WAPE : 13.346658397781994 Validation Loss: 2.5076049496419728e-05\n","Epoch 58: Training Loss: 0.001013504705952073 NSE : 0.8910190463066101 WAPE : 12.863266348418836 Validation Loss: 2.357250195927918e-05\n","Epoch 59: Training Loss: 0.001020358782625408 NSE : 0.8927674889564514 WAPE : 13.354661149444455 Validation Loss: 2.319431587238796e-05\n","Epoch 60: Training Loss: 0.0010205797698290553 NSE : 0.8425290584564209 WAPE : 19.66784515644869 Validation Loss: 3.4060849429806694e-05\n","Epoch 61: Training Loss: 0.0010170239766011946 NSE : 0.6936428546905518 WAPE : 31.27283150236601 Validation Loss: 6.62648308207281e-05\n","Epoch 62: Training Loss: 0.001014286577628809 NSE : 0.5232732892036438 WAPE : 40.64212128160764 Validation Loss: 0.00010311562800779939\n","Epoch 63: Training Loss: 0.0010083427687277435 NSE : 0.47441214323043823 WAPE : 43.042521523420405 Validation Loss: 0.000113684254756663\n","Epoch 64: Training Loss: 0.000988827890068933 NSE : 0.6306153535842896 WAPE : 35.262037480978094 Validation Loss: 7.989761797944084e-05\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:40:54,775] Trial 13 finished with value: 2.2618090952164494e-05 and parameters: {'lr': 0.0023915530337868785, 'weight_decay': 0.00019406534943825645}. Best is trial 13 with value: 2.2618090952164494e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 65: Training Loss: 0.000964027814916335 NSE : 0.8036901950836182 WAPE : 23.66851639532217 Validation Loss: 4.246166645316407e-05\n","Early stopping!\n","Epoch 1: Training Loss: 0.005814819098304724 NSE : 0.030293405055999756 WAPE : 50.889454045152284 Validation Loss: 0.00020974682411178946\n","Epoch 2: Training Loss: 0.0041809704880506615 NSE : -0.32700181007385254 WAPE : 52.21422943028079 Validation Loss: 0.0002870294265449047\n","Epoch 3: Training Loss: 0.004766945930896327 NSE : -5.125119209289551 WAPE : 152.07397802839216 Validation Loss: 0.0013248586328700185\n","Epoch 4: Training Loss: 0.003789127255004132 NSE : -1.114229440689087 WAPE : 86.31298075920868 Validation Loss: 0.0004573062469717115\n","Epoch 5: Training Loss: 0.003510228212689981 NSE : -0.1123431921005249 WAPE : 59.749911404807065 Validation Loss: 0.00024059900897555053\n","Epoch 6: Training Loss: 0.003511970047838986 NSE : -1.133772611618042 WAPE : 87.72183194013049 Validation Loss: 0.00046153346193023026\n","Epoch 7: Training Loss: 0.003186289242876228 NSE : -0.17057979106903076 WAPE : 62.39113631152154 Validation Loss: 0.00025319549604319036\n","Epoch 8: Training Loss: 0.0031121027568588033 NSE : -0.37067878246307373 WAPE : 68.78774676366972 Validation Loss: 0.0002964767918456346\n","Epoch 9: Training Loss: 0.0029893644677940756 NSE : -0.4776877164840698 WAPE : 72.11467761772738 Validation Loss: 0.00031962274806573987\n","Epoch 10: Training Loss: 0.0028280332662689034 NSE : -0.05247068405151367 WAPE : 59.407283567155154 Validation Loss: 0.00022764864843338728\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:41:12,670] Trial 14 finished with value: 0.00020974682411178946 and parameters: {'lr': 0.002927106637299202, 'weight_decay': 0.0014933969890179434}. Best is trial 13 with value: 2.2618090952164494e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11: Training Loss: 0.002745007339399308 NSE : -0.023638129234313965 WAPE : 58.678993558608326 Validation Loss: 0.0002214121341239661\n","Early stopping!\n","Epoch 1: Training Loss: 0.006337017191981431 NSE : -1.354703664779663 WAPE : 90.86527276896457 Validation Loss: 0.0005093205836601555\n","Epoch 2: Training Loss: 0.0037477931182365865 NSE : -0.560032844543457 WAPE : 63.83021408767797 Validation Loss: 0.0003374338848516345\n","Epoch 3: Training Loss: 0.003807905592111638 NSE : -3.4628734588623047 WAPE : 125.63877342561132 Validation Loss: 0.0009653160814195871\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:41:18,326] Trial 15 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Training Loss: 0.0045940959244035184 NSE : -4.43015718460083 WAPE : 143.5811302662025 Validation Loss: 0.0011745388619601727\n","Epoch 1: Training Loss: 0.005732262301535229 NSE : -0.0623476505279541 WAPE : 52.145467053011195 Validation Loss: 0.00022978498600423336\n","Epoch 2: Training Loss: 0.004541104819509201 NSE : -2.7946228981018066 WAPE : 118.19575993829605 Validation Loss: 0.000820774061139673\n","Epoch 3: Training Loss: 0.0038726188213331625 NSE : -1.9176506996154785 WAPE : 103.05234412457527 Validation Loss: 0.0006310855387710035\n","Epoch 4: Training Loss: 0.0034280927502550185 NSE : 0.47105860710144043 WAPE : 29.482958454066 Validation Loss: 0.00011440963135100901\n","Epoch 5: Training Loss: 0.003518175522913225 NSE : 0.18422085046768188 WAPE : 48.98150549290196 Validation Loss: 0.00017645242041908205\n","Epoch 6: Training Loss: 0.0033462987194070593 NSE : -3.4000463485717773 WAPE : 129.39495945467053 Validation Loss: 0.0009517266880720854\n","Epoch 7: Training Loss: 0.002870288687518041 NSE : 0.037964701652526855 WAPE : 56.29018782180901 Validation Loss: 0.00020808751287404448\n","Epoch 8: Training Loss: 0.0026160281222473714 NSE : 0.6138312816619873 WAPE : 23.22323070188239 Validation Loss: 8.352800796274096e-05\n","Epoch 9: Training Loss: 0.0026374189346825005 NSE : 0.6464322805404663 WAPE : 27.546836630464238 Validation Loss: 7.647643360542133e-05\n","Epoch 10: Training Loss: 0.002604151151899714 NSE : -0.27364420890808105 WAPE : 67.63058514519189 Validation Loss: 0.00027548824436962605\n","Epoch 11: Training Loss: 0.0024160361572285183 NSE : -0.6990076303482056 WAPE : 79.28363386212503 Validation Loss: 0.0003674940671771765\n","Epoch 12: Training Loss: 0.0022315241421893006 NSE : 0.05362039804458618 WAPE : 57.34815617769069 Validation Loss: 0.00020470119488891214\n","Epoch 13: Training Loss: 0.0020850752443948295 NSE : 0.511717677116394 WAPE : 38.052873611140065 Validation Loss: 0.00010561510134721175\n","Epoch 14: Training Loss: 0.0020014017518406035 NSE : 0.6991093158721924 WAPE : 26.047931041671006 Validation Loss: 6.508242222480476e-05\n","Epoch 15: Training Loss: 0.0019505117752487422 NSE : 0.76236891746521 WAPE : 19.87589585374497 Validation Loss: 5.139942004461773e-05\n","Epoch 16: Training Loss: 0.0018985351216542767 NSE : 0.7760849595069885 WAPE : 18.36914385774739 Validation Loss: 4.8432648327434435e-05\n","Epoch 17: Training Loss: 0.001836669096519472 NSE : 0.782541036605835 WAPE : 18.147912280336037 Validation Loss: 4.7036217438289896e-05\n","Epoch 18: Training Loss: 0.001783597124813241 NSE : 0.7671893835067749 WAPE : 20.58451772946155 Validation Loss: 5.035675712861121e-05\n","Epoch 19: Training Loss: 0.0017302205615123967 NSE : 0.6324379444122314 WAPE : 31.92370598903504 Validation Loss: 7.950339204398915e-05\n","Epoch 20: Training Loss: 0.0016713546392566059 NSE : 0.19961684942245483 WAPE : 52.60080882199662 Validation Loss: 0.00017312228737864643\n","Epoch 21: Training Loss: 0.0016019228041841416 NSE : -0.24675416946411133 WAPE : 67.45224406412207 Validation Loss: 0.0002696719893720001\n","Epoch 22: Training Loss: 0.0015194206625892548 NSE : -0.12571179866790771 WAPE : 63.90636843092702 Validation Loss: 0.00024349060549866408\n","Epoch 23: Training Loss: 0.0014328211846077465 NSE : 0.31816577911376953 WAPE : 48.225540430676865 Validation Loss: 0.00014748021203558892\n","Epoch 24: Training Loss: 0.001346437624306418 NSE : 0.7102807760238647 WAPE : 27.832951157991285 Validation Loss: 6.266604032134637e-05\n","Epoch 25: Training Loss: 0.001282009796341299 NSE : 0.8291252851486206 WAPE : 17.642388109482813 Validation Loss: 3.696007843245752e-05\n","Epoch 26: Training Loss: 0.0012378337200971146 NSE : 0.7839242815971375 WAPE : 21.448410498009213 Validation Loss: 4.673700823332183e-05\n","Epoch 27: Training Loss: 0.0012096654836568632 NSE : 0.6188563108444214 WAPE : 32.975501865710534 Validation Loss: 8.24411035864614e-05\n","Epoch 28: Training Loss: 0.0011966768852289533 NSE : 0.4867844581604004 WAPE : 39.762152133580706 Validation Loss: 0.0001110081429942511\n","Epoch 29: Training Loss: 0.001180471164843766 NSE : 0.12813866138458252 WAPE : 54.201921994538374 Validation Loss: 0.00018858294060919434\n","Epoch 30: Training Loss: 0.001193379644973902 NSE : -0.22492730617523193 WAPE : 65.66569385670509 Validation Loss: 0.0002649508533068001\n","Epoch 31: Training Loss: 0.001266092011519504 NSE : -0.1884920597076416 WAPE : 64.83636363636364 Validation Loss: 0.00025706985616125166\n","Epoch 32: Training Loss: 0.0013856171281076968 NSE : 0.7572916746139526 WAPE : 24.46330908257072 Validation Loss: 5.249763125902973e-05\n","Epoch 33: Training Loss: 0.0014327551107271574 NSE : -0.4757958650588989 WAPE : 73.91758354005546 Validation Loss: 0.00031921351910568774\n","Epoch 34: Training Loss: 0.0013609718134830473 NSE : -1.0214195251464844 WAPE : 87.78258531195931 Validation Loss: 0.00043723153066821396\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:42:16,621] Trial 16 finished with value: 3.696007843245752e-05 and parameters: {'lr': 0.0014532780866686377, 'weight_decay': 1.0205734866825408e-05}. Best is trial 13 with value: 2.2618090952164494e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 35: Training Loss: 0.0012910376826766878 NSE : 0.6001059412956238 WAPE : 30.357049050467992 Validation Loss: 8.649678056826815e-05\n","Early stopping!\n","Epoch 1: Training Loss: 0.006648270762525499 NSE : -0.48988258838653564 WAPE : 70.2573388088637 Validation Loss: 0.00032226048642769456\n","Epoch 2: Training Loss: 0.0041183643425029 NSE : 0.35704225301742554 WAPE : 33.37032373725793 Validation Loss: 0.00013907128595747054\n","Epoch 3: Training Loss: 0.0038100620731711388 NSE : 0.006427347660064697 WAPE : 54.821667257301286 Validation Loss: 0.000214909014175646\n","Epoch 4: Training Loss: 0.003221388516976731 NSE : 0.4278644323348999 WAPE : 37.99015238373184 Validation Loss: 0.00012375249934848398\n","Epoch 5: Training Loss: 0.0027960989682469517 NSE : 0.6049273014068604 WAPE : 23.611732088136584 Validation Loss: 8.545391028746963e-05\n","Epoch 6: Training Loss: 0.002520730467949761 NSE : 0.69077467918396 WAPE : 24.512745200225137 Validation Loss: 6.688520079478621e-05\n","Epoch 7: Training Loss: 0.002292810922881472 NSE : 0.727402925491333 WAPE : 20.291023743511705 Validation Loss: 5.896254879189655e-05\n","Epoch 8: Training Loss: 0.0021352239164116327 NSE : 0.7452725172042847 WAPE : 19.944016176439934 Validation Loss: 5.5097360018407926e-05\n","Epoch 9: Training Loss: 0.0020300677824707236 NSE : 0.5571445226669312 WAPE : 35.68345041796085 Validation Loss: 9.578928438713774e-05\n","Epoch 10: Training Loss: 0.001785620319424197 NSE : 0.4998663663864136 WAPE : 39.41119009401513 Validation Loss: 0.00010817852307809517\n","Epoch 11: Training Loss: 0.0016306293509842362 NSE : 0.46734702587127686 WAPE : 41.26866648600196 Validation Loss: 0.00011521243141032755\n","Epoch 12: Training Loss: 0.0015051944646984339 NSE : 0.3563486933708191 WAPE : 46.60353963853161 Validation Loss: 0.00013922128709964454\n","Epoch 13: Training Loss: 0.001393689828546485 NSE : 0.08978796005249023 WAPE : 57.11728335869588 Validation Loss: 0.0001968781725736335\n","Epoch 14: Training Loss: 0.0012772537220371305 NSE : 0.36791056394577026 WAPE : 46.48375059932042 Validation Loss: 0.00013672046770807356\n","Epoch 15: Training Loss: 0.0012001525374216726 NSE : 0.29796743392944336 WAPE : 49.68695253382252 Validation Loss: 0.00015184910444077104\n","Epoch 16: Training Loss: 0.0010998481525348325 NSE : 0.6837646961212158 WAPE : 30.997861207813056 Validation Loss: 6.840144487796351e-05\n","Epoch 17: Training Loss: 0.001035646598211315 NSE : 0.8616721630096436 WAPE : 15.507576452440016 Validation Loss: 2.992021654790733e-05\n","Epoch 18: Training Loss: 0.0010010912510551861 NSE : 0.888573169708252 WAPE : 14.553990952867357 Validation Loss: 2.4101531380438246e-05\n","Epoch 19: Training Loss: 0.0009645032669141074 NSE : 0.7575693130493164 WAPE : 25.812995351358115 Validation Loss: 5.243757550488226e-05\n","Epoch 20: Training Loss: 0.000928811336052604 NSE : 0.5485259890556335 WAPE : 37.98884325946926 Validation Loss: 9.765349386725575e-05\n","Epoch 21: Training Loss: 0.0009347748846266768 NSE : 0.8011897206306458 WAPE : 21.92189864709929 Validation Loss: 4.3002513848477975e-05\n","Epoch 22: Training Loss: 0.0009935479965861305 NSE : 0.2819821834564209 WAPE : 48.86433887140147 Validation Loss: 0.00015530672681052238\n","Epoch 23: Training Loss: 0.0010303194590051135 NSE : 0.11367958784103394 WAPE : 55.33970940776719 Validation Loss: 0.0001917104673339054\n","Epoch 24: Training Loss: 0.0009782592273950286 NSE : 0.7462207078933716 WAPE : 25.04406412207375 Validation Loss: 5.489227260113694e-05\n","Epoch 25: Training Loss: 0.0009164399107248755 NSE : 0.8820837736129761 WAPE : 14.459846573971774 Validation Loss: 2.5505200028419495e-05\n","Epoch 26: Training Loss: 0.0008734459129300376 NSE : 0.3788326382637024 WAPE : 46.146496841841945 Validation Loss: 0.00013435803703032434\n","Epoch 27: Training Loss: 0.0008404774316659314 NSE : 0.43183428049087524 WAPE : 42.79086114527527 Validation Loss: 0.00012289380538277328\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:43:01,668] Trial 17 finished with value: 2.4101531380438246e-05 and parameters: {'lr': 0.005401518272437287, 'weight_decay': 4.373937579104545e-05}. Best is trial 13 with value: 2.2618090952164494e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28: Training Loss: 0.0008689047995176225 NSE : 0.8788590431213379 WAPE : 14.858910591815889 Validation Loss: 2.620270061015617e-05\n","Early stopping!\n","Epoch 1: Training Loss: 0.007181458167906385 NSE : -1.7310073375701904 WAPE : 97.11249713368494 Validation Loss: 0.0005907147424295545\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:43:06,001] Trial 18 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.004550203047983814 NSE : -2.4797251224517822 WAPE : 112.7789372746034 Validation Loss: 0.0007526619010604918\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:43:07,508] Trial 19 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006656157216639258 NSE : -7.827408790588379 WAPE : 182.44270913677013 Validation Loss: 0.0019093615701422095\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:43:08,993] Trial 20 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.0076574454724323004 NSE : -22.378650665283203 WAPE : 299.60354380771713 Validation Loss: 0.005056784488260746\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:43:10,427] Trial 21 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.00850523942062864 NSE : -12.22979736328125 WAPE : 225.24697004440185 Validation Loss: 0.002861595246940851\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:43:11,974] Trial 22 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.00687317008851096 NSE : -4.1580352783203125 WAPE : 138.7118550791103 Validation Loss: 0.0011156790424138308\n","Epoch 1: Training Loss: 0.0057454203324596165 NSE : 0.22804301977157593 WAPE : 39.81671426486836 Validation Loss: 0.0001669737248448655\n","Epoch 2: Training Loss: 0.004249118044754141 NSE : -0.11609780788421631 WAPE : 45.78006712388735 Validation Loss: 0.00024141107860486954\n","Epoch 3: Training Loss: 0.0046971692208899185 NSE : -5.7689642906188965 WAPE : 160.33820433178377 Validation Loss: 0.0014641218585893512\n","Epoch 4: Training Loss: 0.003586206208638032 NSE : -0.20966267585754395 WAPE : 62.381780659148234 Validation Loss: 0.0002616491401568055\n","Epoch 5: Training Loss: 0.003444163034146186 NSE : 0.11674231290817261 WAPE : 51.84461028538075 Validation Loss: 0.0001910479477373883\n","Epoch 6: Training Loss: 0.003322819735330995 NSE : -1.1790506839752197 WAPE : 89.33347230618499 Validation Loss: 0.000471327017294243\n","Epoch 7: Training Loss: 0.00296050839824602 NSE : -0.5634082555770874 WAPE : 74.84036605449126 Validation Loss: 0.00033816404175013304\n","Epoch 8: Training Loss: 0.00266938312597631 NSE : 0.4060918092727661 WAPE : 41.78427800129245 Validation Loss: 0.00012846187746617943\n","Epoch 9: Training Loss: 0.0025499909734207904 NSE : 0.5373384952545166 WAPE : 35.12256988597278 Validation Loss: 0.00010007333912653849\n","Epoch 10: Training Loss: 0.0024689162855793256 NSE : 0.4124426245689392 WAPE : 42.26194992808155 Validation Loss: 0.00012708822032436728\n","Epoch 11: Training Loss: 0.002377226086537121 NSE : -0.07916319370269775 WAPE : 61.65908152842342 Validation Loss: 0.00023342219355981797\n","Epoch 12: Training Loss: 0.002249831402878044 NSE : -0.39606451988220215 WAPE : 71.412061453795 Validation Loss: 0.0003019676951225847\n","Epoch 13: Training Loss: 0.0021021380143793067 NSE : 0.06018078327178955 WAPE : 57.361935335932124 Validation Loss: 0.00020328217942733318\n","Epoch 14: Training Loss: 0.001978028175471991 NSE : 0.4552493095397949 WAPE : 41.40103812720185 Validation Loss: 0.00011782915680669248\n","Epoch 15: Training Loss: 0.0018934832769446075 NSE : 0.6874172687530518 WAPE : 27.983185674678452 Validation Loss: 6.761141412425786e-05\n","Epoch 16: Training Loss: 0.0018404807460683514 NSE : 0.776293933391571 WAPE : 19.92704759125305 Validation Loss: 4.838745735469274e-05\n","Epoch 17: Training Loss: 0.0018027261794486549 NSE : 0.7943257689476013 WAPE : 17.496191449000438 Validation Loss: 4.448718027560972e-05\n","Epoch 18: Training Loss: 0.0017734621815179707 NSE : 0.7998399138450623 WAPE : 17.420160096725105 Validation Loss: 4.329447619966231e-05\n","Epoch 19: Training Loss: 0.001746467705743271 NSE : 0.7754875421524048 WAPE : 21.2574951533218 Validation Loss: 4.856186933466233e-05\n","Epoch 20: Training Loss: 0.0017093866135837743 NSE : 0.5878517627716064 WAPE : 35.44524817076984 Validation Loss: 8.914734644349664e-05\n","Epoch 21: Training Loss: 0.0016548514140595216 NSE : 0.2972344756126404 WAPE : 49.23167747180589 Validation Loss: 0.0001520076475571841\n","Epoch 22: Training Loss: 0.0015971389912010636 NSE : 0.062487125396728516 WAPE : 58.087427820975165 Validation Loss: 0.00020278332522138953\n","Epoch 23: Training Loss: 0.0015304672861020663 NSE : 0.2834952473640442 WAPE : 50.06594817702362 Validation Loss: 0.00015497943968512118\n","Epoch 24: Training Loss: 0.0014578380032617133 NSE : 0.6671113967895508 WAPE : 31.228571428571428 Validation Loss: 7.200356776593253e-05\n","Epoch 25: Training Loss: 0.0014106786043157626 NSE : 0.8090887069702148 WAPE : 19.64494798941027 Validation Loss: 4.129396620555781e-05\n","Epoch 26: Training Loss: 0.0014017432818036468 NSE : 0.8354508876800537 WAPE : 16.499203685560026 Validation Loss: 3.559185643098317e-05\n","Epoch 27: Training Loss: 0.0013979081950310501 NSE : 0.8387222290039062 WAPE : 16.262380396489544 Validation Loss: 3.4884255001088604e-05\n","Epoch 28: Training Loss: 0.0013883954889024608 NSE : 0.8072165846824646 WAPE : 20.40164057451377 Validation Loss: 4.169890598859638e-05\n","Epoch 29: Training Loss: 0.0013705697310797404 NSE : 0.7333165407180786 WAPE : 27.010500093806677 Validation Loss: 5.768342089140788e-05\n","Epoch 30: Training Loss: 0.0013504884573194431 NSE : 0.5959237813949585 WAPE : 35.87089700027099 Validation Loss: 8.740137127460912e-05\n","Epoch 31: Training Loss: 0.001327522126302938 NSE : 0.452003538608551 WAPE : 43.28109899731088 Validation Loss: 0.00011853122123284265\n","Epoch 32: Training Loss: 0.0012987825411983067 NSE : 0.4437662363052368 WAPE : 43.76502053323883 Validation Loss: 0.00012031293590553105\n","Epoch 33: Training Loss: 0.0012649257305383799 NSE : 0.5812718868255615 WAPE : 37.028496383231534 Validation Loss: 9.057056740857661e-05\n","Epoch 34: Training Loss: 0.0012337414209468989 NSE : 0.7369145750999451 WAPE : 27.435675720747955 Validation Loss: 5.690518446499482e-05\n","Epoch 35: Training Loss: 0.0012158652643847745 NSE : 0.8198757171630859 WAPE : 20.35211690396281 Validation Loss: 3.8960752135608345e-05\n","Epoch 36: Training Loss: 0.0012108874766454392 NSE : 0.8490181565284729 WAPE : 16.98653769986033 Validation Loss: 3.26572589983698e-05\n","Epoch 37: Training Loss: 0.001210152035127976 NSE : 0.8541049957275391 WAPE : 16.38087281899481 Validation Loss: 3.155698504997417e-05\n","Epoch 38: Training Loss: 0.0012082571474820725 NSE : 0.8463940024375916 WAPE : 17.481691021658918 Validation Loss: 3.3224860089831054e-05\n","Epoch 39: Training Loss: 0.0012041863783451845 NSE : 0.8210282325744629 WAPE : 20.39696483291989 Validation Loss: 3.871145599987358e-05\n","Epoch 40: Training Loss: 0.0011972421161772218 NSE : 0.7734920978546143 WAPE : 24.76233557774489 Validation Loss: 4.8993490054272115e-05\n","Epoch 41: Training Loss: 0.0011874429001181852 NSE : 0.7040101885795593 WAPE : 29.91263888599362 Validation Loss: 6.402236613212153e-05\n","Epoch 42: Training Loss: 0.0011749078767024912 NSE : 0.6445093154907227 WAPE : 33.70994559212858 Validation Loss: 7.689235644647852e-05\n","Epoch 43: Training Loss: 0.0011602329059314798 NSE : 0.6281128525733948 WAPE : 34.7478163890684 Validation Loss: 8.04389055701904e-05\n","Epoch 44: Training Loss: 0.0011436796485213563 NSE : 0.6736396551132202 WAPE : 32.1003314502512 Validation Loss: 7.05914935679175e-05\n","Epoch 45: Training Loss: 0.0011280811277174507 NSE : 0.7452435493469238 WAPE : 27.37255633611974 Validation Loss: 5.510363189387135e-05\n","Epoch 46: Training Loss: 0.0011167356933583505 NSE : 0.8086137175559998 WAPE : 22.2548310437556 Validation Loss: 4.139671727898531e-05\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:44:28,591] Trial 23 finished with value: 3.155698504997417e-05 and parameters: {'lr': 0.002085312213699206, 'weight_decay': 0.00022500337989435712}. Best is trial 13 with value: 2.2618090952164494e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 47: Training Loss: 0.001111535245399864 NSE : 0.8464665412902832 WAPE : 18.411144232974088 Validation Loss: 3.320917676319368e-05\n","Early stopping!\n","Epoch 1: Training Loss: 0.00939495257625822 NSE : 0.22059452533721924 WAPE : 40.66422630339163 Validation Loss: 0.00016858481103554368\n","Epoch 2: Training Loss: 0.004668894107453525 NSE : 0.2875834107398987 WAPE : 39.12773550686873 Validation Loss: 0.00015409517800435424\n","Epoch 3: Training Loss: 0.0038473283893836197 NSE : 0.28784406185150146 WAPE : 42.519630610160306 Validation Loss: 0.00015403880388475955\n","Epoch 4: Training Loss: 0.0031756486860103905 NSE : 0.5394901037216187 WAPE : 29.47706739488441 Validation Loss: 9.960793977370486e-05\n","Epoch 5: Training Loss: 0.00272866020713991 NSE : 0.23765361309051514 WAPE : 42.95283400387734 Validation Loss: 0.00016489495465066284\n","Epoch 6: Training Loss: 0.0023902017819636967 NSE : -0.09016942977905273 WAPE : 56.71682891747098 Validation Loss: 0.00023580281413160264\n","Epoch 7: Training Loss: 0.0023594583180965856 NSE : -0.6693710088729858 WAPE : 73.19772362468991 Validation Loss: 0.00036108368658460677\n","Epoch 8: Training Loss: 0.002530441523049376 NSE : -1.4161114692687988 WAPE : 92.49889308123657 Validation Loss: 0.0005226031062193215\n","Epoch 9: Training Loss: 0.0020271915218472714 NSE : 0.15938472747802734 WAPE : 49.33070396698005 Validation Loss: 0.00018182444910053164\n","Epoch 10: Training Loss: 0.0016516713540113415 NSE : -0.05165112018585205 WAPE : 57.26378020053783 Validation Loss: 0.00022747134789824486\n","Epoch 11: Training Loss: 0.001566717772220727 NSE : -1.1375055313110352 WAPE : 87.29167205186467 Validation Loss: 0.00046234080218710005\n","Epoch 12: Training Loss: 0.001660925252508605 NSE : -1.8878324031829834 WAPE : 103.42974713889642 Validation Loss: 0.0006246359553188086\n","Epoch 13: Training Loss: 0.002109145210852148 NSE : 0.4263073205947876 WAPE : 37.72615121635988 Validation Loss: 0.00012408931797835976\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:44:52,548] Trial 24 finished with value: 9.960793977370486e-05 and parameters: {'lr': 0.006741245939801204, 'weight_decay': 5.392532052081583e-05}. Best is trial 13 with value: 2.2618090952164494e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14: Training Loss: 0.002075019880066975 NSE : -0.239477276802063 WAPE : 63.84133747472431 Validation Loss: 0.000268097996013239\n","Early stopping!\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:44:54,103] Trial 25 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006098266978369793 NSE : -3.287281036376953 WAPE : 125.44657397177461 Validation Loss: 0.0009273355826735497\n","Epoch 1: Training Loss: 0.005175031985345413 NSE : -0.4158233404159546 WAPE : 67.84238810948281 Validation Loss: 0.0003062415053136647\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:44:58,177] Trial 26 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.003811653894445044 NSE : -2.5524983406066895 WAPE : 108.71573242167143 Validation Loss: 0.0007684027077630162\n","Epoch 1: Training Loss: 0.009103670206968673 NSE : -1.5837047100067139 WAPE : 86.09720873027455 Validation Loss: 0.0005588533240370452\n","Epoch 2: Training Loss: 0.005265000509098172 NSE : 0.26031607389450073 WAPE : 36.8063204852932 Validation Loss: 0.00015999308379832655\n","Epoch 3: Training Loss: 0.004231557984894607 NSE : -0.7314692735671997 WAPE : 76.70772758541618 Validation Loss: 0.0003745154826901853\n","Epoch 4: Training Loss: 0.0034180926104454556 NSE : 0.3933565020561218 WAPE : 33.37121594296554 Validation Loss: 0.00013121654046699405\n","Epoch 5: Training Loss: 0.003107056899352756 NSE : 0.35897380113601685 WAPE : 32.79478434887745 Validation Loss: 0.00013865347136743367\n","Epoch 6: Training Loss: 0.0027883883958566003 NSE : 0.2807880640029907 WAPE : 36.92744366388026 Validation Loss: 0.00015556499420199543\n","Epoch 7: Training Loss: 0.002656986442161724 NSE : 0.5821238160133362 WAPE : 29.89366909174293 Validation Loss: 9.038631105795503e-05\n","Epoch 8: Training Loss: 0.002417112802504562 NSE : 0.3560420274734497 WAPE : 44.79232452940318 Validation Loss: 0.00013928761472925544\n","Epoch 9: Training Loss: 0.0020169429808447603 NSE : 0.272347092628479 WAPE : 49.6524316774718 Validation Loss: 0.0001573907647980377\n","Epoch 10: Training Loss: 0.0017718299277476035 NSE : 0.48304957151412964 WAPE : 40.525646744908386 Validation Loss: 0.00011181599256815389\n","Epoch 11: Training Loss: 0.0016261074561043642 NSE : 0.2718597650527954 WAPE : 50.40586604406829 Validation Loss: 0.00015749619342386723\n","Epoch 12: Training Loss: 0.0014737341007275973 NSE : 0.1645922064781189 WAPE : 54.587830147380714 Validation Loss: 0.0001806980581022799\n","Epoch 13: Training Loss: 0.0013409157181740738 NSE : 0.16268283128738403 WAPE : 54.78574138542036 Validation Loss: 0.0001811110705602914\n","Epoch 14: Training Loss: 0.0012401135318214074 NSE : 0.11749047040939331 WAPE : 56.606908340455696 Validation Loss: 0.0001908861449919641\n","Epoch 15: Training Loss: 0.0011257703454248258 NSE : 0.5297853946685791 WAPE : 39.87570824039524 Validation Loss: 0.000101707068097312\n","Epoch 16: Training Loss: 0.0010370091540607973 NSE : 0.6726589202880859 WAPE : 32.42371432740614 Validation Loss: 7.080363138811663e-05\n","Epoch 17: Training Loss: 0.0009677100520093518 NSE : 0.8629310131072998 WAPE : 17.532611369368993 Validation Loss: 2.9647917472175322e-05\n","Epoch 18: Training Loss: 0.0009237903173016093 NSE : 0.9032485485076904 WAPE : 12.243735798711722 Validation Loss: 2.0927267541992478e-05\n","Epoch 19: Training Loss: 0.0008929496168548212 NSE : 0.834658682346344 WAPE : 19.555798294803107 Validation Loss: 3.5763197956839576e-05\n","Epoch 20: Training Loss: 0.0008794950817900826 NSE : 0.7207406759262085 WAPE : 28.221250338746323 Validation Loss: 6.040358857717365e-05\n","Epoch 21: Training Loss: 0.0008962704096120433 NSE : 0.521274209022522 WAPE : 38.60839257051135 Validation Loss: 0.00010354802361689508\n","Epoch 22: Training Loss: 0.000981145552941598 NSE : 0.17779433727264404 WAPE : 51.62414375351775 Validation Loss: 0.00017784246301744133\n","Epoch 23: Training Loss: 0.001073785039352515 NSE : 0.46831613779067993 WAPE : 38.76989431114632 Validation Loss: 0.00011500282562337816\n","Epoch 24: Training Loss: 0.0011164552097397973 NSE : 0.06237387657165527 WAPE : 55.51223447499532 Validation Loss: 0.00020280780154280365\n","Epoch 25: Training Loss: 0.0010273235047861817 NSE : -0.012369751930236816 WAPE : 59.21187384044527 Validation Loss: 0.00021897480473853648\n","Epoch 26: Training Loss: 0.000898574226994242 NSE : 0.24128293991088867 WAPE : 50.248108232056865 Validation Loss: 0.00016410993703175336\n","Epoch 27: Training Loss: 0.0009481046599830734 NSE : 0.2568433880805969 WAPE : 50.29955598173896 Validation Loss: 0.00016074421000666916\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:45:44,008] Trial 27 finished with value: 2.0927267541992478e-05 and parameters: {'lr': 0.007261370392859548, 'weight_decay': 4.515920437787298e-05}. Best is trial 27 with value: 2.0927267541992478e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28: Training Loss: 0.0009741341091284994 NSE : -0.2063910961151123 WAPE : 64.87957724458528 Validation Loss: 0.00026094148051925004\n","Early stopping!\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:45:45,441] Trial 28 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.015844912850297987 NSE : -17.855134963989258 WAPE : 269.32336620041275 Validation Loss: 0.004078351426869631\n","Epoch 1: Training Loss: 0.005409827062976547 NSE : -0.490128755569458 WAPE : 70.30585145191887 Validation Loss: 0.0003223137464374304\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:45:48,379] Trial 29 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.003907571737727267 NSE : -0.4832998514175415 WAPE : 60.235917533509834 Validation Loss: 0.0003208366397302598\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:45:49,832] Trial 30 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.00574289787618909 NSE : -4.18215799331665 WAPE : 137.40421504659065 Validation Loss: 0.001120896777138114\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:45:51,359] Trial 31 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.010730307214544155 NSE : -13.983593940734863 WAPE : 239.93105834775176 Validation Loss: 0.003240940161049366\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:45:53,480] Trial 32 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006283694892772473 NSE : -6.943201065063477 WAPE : 172.82152967417815 Validation Loss: 0.001718108425848186\n","Epoch 1: Training Loss: 0.013841747611877508 NSE : -0.019400596618652344 WAPE : 39.849684184194615 Validation Loss: 0.00022049559629522264\n","Epoch 2: Training Loss: 0.00514305700926343 NSE : 0.3719278573989868 WAPE : 32.18223510037314 Validation Loss: 0.00013585154374595731\n","Epoch 3: Training Loss: 0.003996223080321215 NSE : 0.4491618275642395 WAPE : 29.740286839965812 Validation Loss: 0.00011914587958017364\n","Epoch 4: Training Loss: 0.0036526743351714686 NSE : 0.40268129110336304 WAPE : 38.947747597506826 Validation Loss: 0.00012919960136059672\n","Epoch 5: Training Loss: 0.0028720966784021584 NSE : 0.6110848188400269 WAPE : 23.076095974651352 Validation Loss: 8.412204624619335e-05\n","Epoch 6: Training Loss: 0.002546226450249378 NSE : 0.6213268637657166 WAPE : 24.50282670780263 Validation Loss: 8.190670632757246e-05\n","Epoch 7: Training Loss: 0.002339821877740178 NSE : 0.28998512029647827 WAPE : 42.08580183861083 Validation Loss: 0.00015357568918261677\n","Epoch 8: Training Loss: 0.0022029966849004268 NSE : 0.4936392307281494 WAPE : 32.51674970294553 Validation Loss: 0.00010952547745546326\n","Epoch 9: Training Loss: 0.0020183749256830197 NSE : 0.7475742101669312 WAPE : 19.674582560296848 Validation Loss: 5.4599513532593846e-05\n","Epoch 10: Training Loss: 0.0018392507918179035 NSE : 0.7805512547492981 WAPE : 19.40226803693898 Validation Loss: 4.7466604883084074e-05\n","Epoch 11: Training Loss: 0.0017001809501380194 NSE : 0.5689924359321594 WAPE : 35.79759437993788 Validation Loss: 9.322661207988858e-05\n","Epoch 12: Training Loss: 0.0015589759532304015 NSE : 0.3523104786872864 WAPE : 46.38169727543725 Validation Loss: 0.0001400947367073968\n","Epoch 13: Training Loss: 0.0013763641381956404 NSE : 0.007314622402191162 WAPE : 59.665097663171494 Validation Loss: 0.00021471711806952953\n","Epoch 14: Training Loss: 0.001260934724996332 NSE : 0.07059603929519653 WAPE : 57.40301432115237 Validation Loss: 0.00020102936832699925\n","Epoch 15: Training Loss: 0.0012374587076919852 NSE : -0.6310974359512329 WAPE : 78.40770465489567 Validation Loss: 0.0003528051311150193\n","Epoch 16: Training Loss: 0.0011113675195701944 NSE : 0.6494801044464111 WAPE : 33.59659794459152 Validation Loss: 7.581718818983063e-05\n","Epoch 17: Training Loss: 0.0009660341411290574 NSE : 0.8954545259475708 WAPE : 12.5016958162223 Validation Loss: 2.2613101464230567e-05\n","Epoch 18: Training Loss: 0.0009067410769603157 NSE : 0.6573425531387329 WAPE : 32.001061057722374 Validation Loss: 7.411653496092185e-05\n","Epoch 19: Training Loss: 0.0009078348866751185 NSE : 0.49457818269729614 WAPE : 40.66705301119426 Validation Loss: 0.0001093223545467481\n","Epoch 20: Training Loss: 0.0009240152448910521 NSE : 0.3701683282852173 WAPE : 46.440399407975654 Validation Loss: 0.00013623210543300956\n","Epoch 21: Training Loss: 0.0009437055298349151 NSE : 0.38954252004623413 WAPE : 45.7585040962248 Validation Loss: 0.0001320415030932054\n","Epoch 22: Training Loss: 0.000939206377552182 NSE : 0.6806629300117493 WAPE : 30.479650205332387 Validation Loss: 6.907235365360975e-05\n","Epoch 23: Training Loss: 0.0009007316093629925 NSE : 0.8797951340675354 WAPE : 15.02711846740739 Validation Loss: 2.6000216166721657e-05\n","Epoch 24: Training Loss: 0.0008770906533754896 NSE : 0.9030766487121582 WAPE : 13.132273665339476 Validation Loss: 2.096444040944334e-05\n","Epoch 25: Training Loss: 0.0008654694047436351 NSE : 0.6042702794075012 WAPE : 36.74205249004607 Validation Loss: 8.559603884350508e-05\n","Epoch 26: Training Loss: 0.0008468808136967709 NSE : 0.05997663736343384 WAPE : 58.88046111192179 Validation Loss: 0.00020332634449005127\n","Epoch 27: Training Loss: 0.0008559480920666829 NSE : -0.1615891456604004 WAPE : 66.42876738029227 Validation Loss: 0.00025125083629973233\n","Epoch 28: Training Loss: 0.0008459330529149156 NSE : 0.15874803066253662 WAPE : 55.887532050613906 Validation Loss: 0.00018196215387433767\n","Epoch 29: Training Loss: 0.0007591485536977416 NSE : 0.9021292328834534 WAPE : 15.100290800692084 Validation Loss: 2.1169371393625624e-05\n","Epoch 30: Training Loss: 0.0006950948077246721 NSE : 0.7385704517364502 WAPE : 28.298834712638886 Validation Loss: 5.6547018175479025e-05\n","Epoch 31: Training Loss: 0.0007016810541244922 NSE : 0.5900928974151611 WAPE : 36.88412999520544 Validation Loss: 8.86626003193669e-05\n","Epoch 32: Training Loss: 0.0007538540303357877 NSE : 0.4361855983734131 WAPE : 44.76918971878843 Validation Loss: 0.00012195262388559058\n","Epoch 33: Training Loss: 0.0007975757393978711 NSE : 0.42092764377593994 WAPE : 43.12365804340122 Validation Loss: 0.00012525291822385043\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:46:47,646] Trial 33 finished with value: 2.096444040944334e-05 and parameters: {'lr': 0.00796642083338778, 'weight_decay': 1.8983499165632718e-05}. Best is trial 27 with value: 2.0927267541992478e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 34: Training Loss: 0.0007937207305985794 NSE : 0.2795199751853943 WAPE : 49.71120051697901 Validation Loss: 0.00015583929780405015\n","Early stopping!\n","Epoch 1: Training Loss: 0.010263790871249512 NSE : -0.3160536289215088 WAPE : 49.17352567176002 Validation Loss: 0.00028466142248362303\n","Epoch 2: Training Loss: 0.0052054888183192816 NSE : 0.3702847957611084 WAPE : 33.05294031810886 Validation Loss: 0.00013620694517157972\n","Epoch 3: Training Loss: 0.00411362786690006 NSE : 0.4437653422355652 WAPE : 29.3904942569469 Validation Loss: 0.00012031313235638663\n","Epoch 4: Training Loss: 0.003527343013047357 NSE : 0.5226619243621826 WAPE : 28.730537199558064 Validation Loss: 0.00010324784670956433\n","Epoch 5: Training Loss: 0.0030872612787788967 NSE : -0.06796920299530029 WAPE : 52.52101061057722 Validation Loss: 0.00023100091493688524\n","Epoch 6: Training Loss: 0.0029582627494164626 NSE : 0.3974754214286804 WAPE : 34.74727231035417 Validation Loss: 0.00013032561400905252\n","Epoch 7: Training Loss: 0.0027472131296235602 NSE : 0.20930784940719604 WAPE : 43.65160200954744 Validation Loss: 0.00017102609854191542\n","Epoch 8: Training Loss: 0.0027838322712341323 NSE : -1.1874141693115234 WAPE : 90.4609534927352 Validation Loss: 0.0004731360240839422\n","Epoch 9: Training Loss: 0.0022329122948576696 NSE : -0.49992549419403076 WAPE : 73.6619124054116 Validation Loss: 0.0003244327672291547\n","Epoch 10: Training Loss: 0.002070195412670728 NSE : -0.6059223413467407 WAPE : 76.88211627858497 Validation Loss: 0.00034735980443656445\n","Epoch 11: Training Loss: 0.001677508528700855 NSE : 0.640807569026947 WAPE : 31.979402138792185 Validation Loss: 7.769304647808895e-05\n","Epoch 12: Training Loss: 0.0014906709120623418 NSE : 0.8038367629051208 WAPE : 17.59846782431052 Validation Loss: 4.242995782988146e-05\n","Epoch 13: Training Loss: 0.0013855807064828696 NSE : 0.8016767501831055 WAPE : 17.985409935169162 Validation Loss: 4.2897176172118634e-05\n","Epoch 14: Training Loss: 0.001304431621065305 NSE : 0.5081138610839844 WAPE : 37.46601905317796 Validation Loss: 0.00010639460379024968\n","Epoch 15: Training Loss: 0.001279857066947443 NSE : 0.6848212480545044 WAPE : 27.22948864939234 Validation Loss: 6.817293615313247e-05\n","Epoch 16: Training Loss: 0.0012837922004109714 NSE : 0.8497155904769897 WAPE : 15.33857122011215 Validation Loss: 3.250639929319732e-05\n","Epoch 17: Training Loss: 0.0012399124243529513 NSE : 0.6290322542190552 WAPE : 33.28393821267016 Validation Loss: 8.024004637263715e-05\n","Epoch 18: Training Loss: 0.0012208790903969202 NSE : -0.6639291048049927 WAPE : 79.28691084196701 Validation Loss: 0.00035990652395412326\n","Epoch 19: Training Loss: 0.0011059161847697396 NSE : 0.3299029469490051 WAPE : 48.44222551124638 Validation Loss: 0.0001449414703529328\n","Epoch 20: Training Loss: 0.000941689909723209 NSE : 0.8795081973075867 WAPE : 15.867326092847762 Validation Loss: 2.606228918011766e-05\n","Epoch 21: Training Loss: 0.0008946462421590695 NSE : 0.45415204763412476 WAPE : 42.476817243751434 Validation Loss: 0.00011806651309598237\n","Epoch 22: Training Loss: 0.0009414510450369562 NSE : 0.35331135988235474 WAPE : 47.1579579329178 Validation Loss: 0.0001398782478645444\n","Epoch 23: Training Loss: 0.0009741502617544029 NSE : 0.8208664655685425 WAPE : 20.96572721018949 Validation Loss: 3.87464533559978e-05\n","Epoch 24: Training Loss: 0.000953409928115434 NSE : 0.8823959231376648 WAPE : 15.505040545329472 Validation Loss: 2.5437675503781065e-05\n","Epoch 25: Training Loss: 0.0009167017160507385 NSE : -0.25286149978637695 WAPE : 68.84325113089159 Validation Loss: 0.0002709930413402617\n","Epoch 26: Training Loss: 0.0008975305613603268 NSE : 0.4208773970603943 WAPE : 44.54791019574326 Validation Loss: 0.0001252637739526108\n","Epoch 27: Training Loss: 0.0008158196178555954 NSE : 0.8753402233123779 WAPE : 16.98473140022097 Validation Loss: 2.6963809432345442e-05\n","Epoch 28: Training Loss: 0.0007757703278912231 NSE : 0.5472592711448669 WAPE : 39.30307894352838 Validation Loss: 9.792746277526021e-05\n","Epoch 29: Training Loss: 0.0008430198204223416 NSE : 0.7280036211013794 WAPE : 27.90033353484397 Validation Loss: 5.883261110284366e-05\n","Epoch 30: Training Loss: 0.0008317549672938185 NSE : 0.8919146060943604 WAPE : 15.785591294740572 Validation Loss: 2.3378785044769756e-05\n","Epoch 31: Training Loss: 0.000820687575469492 NSE : 0.21810537576675415 WAPE : 53.44074961956182 Validation Loss: 0.00016912321734707803\n","Epoch 32: Training Loss: 0.0008755814051255584 NSE : -0.1909393072128296 WAPE : 67.07734673031624 Validation Loss: 0.0002575992839410901\n","Epoch 33: Training Loss: 0.0008221779694395082 NSE : 0.14945483207702637 WAPE : 54.342073335973815 Validation Loss: 0.00018397229723632336\n","Epoch 34: Training Loss: 0.0006928845155016461 NSE : 0.9113661646842957 WAPE : 13.596231056263159 Validation Loss: 1.9171433450537734e-05\n","Epoch 35: Training Loss: 0.0007098261721694143 NSE : 0.5315036773681641 WAPE : 40.551574909841364 Validation Loss: 0.00010133538307854906\n","Epoch 36: Training Loss: 0.0006870204638289579 NSE : 0.6863549947738647 WAPE : 31.92046027808468 Validation Loss: 6.784119614167139e-05\n","Epoch 37: Training Loss: 0.0007252305067595444 NSE : 0.5222916007041931 WAPE : 40.572946154968626 Validation Loss: 0.00010332796955481172\n","Epoch 38: Training Loss: 0.0007980890272847319 NSE : 0.728804886341095 WAPE : 28.459431740009588 Validation Loss: 5.8659294154495e-05\n","Epoch 39: Training Loss: 0.0007813326719769975 NSE : 0.9195438623428345 WAPE : 12.289716703841904 Validation Loss: 1.7402597222826444e-05\n","Epoch 40: Training Loss: 0.0007825464363122592 NSE : 0.35524874925613403 WAPE : 48.28736528319193 Validation Loss: 0.00013945921091362834\n","Epoch 41: Training Loss: 0.0007668933458262472 NSE : -0.011747479438781738 WAPE : 61.27951470680203 Validation Loss: 0.0002188402140745893\n","Epoch 42: Training Loss: 0.0007539315151916526 NSE : 0.4172968864440918 WAPE : 44.83547559984157 Validation Loss: 0.0001260382414329797\n","Epoch 43: Training Loss: 0.0007001449284871342 NSE : 0.9175933599472046 WAPE : 13.923393300118821 Validation Loss: 1.7824495444074273e-05\n","Epoch 44: Training Loss: 0.0006477150432147027 NSE : 0.8298685550689697 WAPE : 22.185266098267704 Validation Loss: 3.679930523503572e-05\n","Epoch 45: Training Loss: 0.0006702685100208328 NSE : 0.9226494431495667 WAPE : 11.839455087448666 Validation Loss: 1.6730864444980398e-05\n","Epoch 46: Training Loss: 0.0007246520981425419 NSE : 0.42126786708831787 WAPE : 45.18404869608722 Validation Loss: 0.00012517932918854058\n","Epoch 47: Training Loss: 0.0007122456136130495 NSE : 0.8769258260726929 WAPE : 17.218479914948613 Validation Loss: 2.6620857170200907e-05\n","Epoch 48: Training Loss: 0.0007033467459223175 NSE : 0.8210970759391785 WAPE : 21.841270767755518 Validation Loss: 3.869656575261615e-05\n","Epoch 49: Training Loss: 0.0007080950658746588 NSE : 0.9246124029159546 WAPE : 11.970112151091286 Validation Loss: 1.6306281395372935e-05\n","Epoch 50: Training Loss: 0.0007044954054435948 NSE : 0.6350764036178589 WAPE : 35.11358946029893 Validation Loss: 7.893268775660545e-05\n","Epoch 51: Training Loss: 0.0007060019070195267 NSE : 0.41586410999298096 WAPE : 46.0230722728315 Validation Loss: 0.00012634815357159823\n","Epoch 52: Training Loss: 0.0007492323788937938 NSE : 0.048565030097961426 WAPE : 59.419707740092974 Validation Loss: 0.00020579465490300208\n","Epoch 53: Training Loss: 0.0007485268579330295 NSE : 0.7540879249572754 WAPE : 26.845902732901127 Validation Loss: 5.3190600738162175e-05\n","Epoch 54: Training Loss: 0.0006801456638640957 NSE : 0.7972265481948853 WAPE : 23.492749786329238 Validation Loss: 4.3859741708729416e-05\n","Epoch 55: Training Loss: 0.0007145211907300109 NSE : 0.45743680000305176 WAPE : 43.38090929936837 Validation Loss: 0.00011735600128304213\n","Epoch 56: Training Loss: 0.0007417196634378342 NSE : 0.1279025673866272 WAPE : 56.53414771424402 Validation Loss: 0.0001886340178316459\n","Epoch 57: Training Loss: 0.000713111218828999 NSE : 0.7832920551300049 WAPE : 25.045106418461156 Validation Loss: 4.687375985668041e-05\n","Epoch 58: Training Loss: 0.0007456471043951751 NSE : 0.9133107662200928 WAPE : 14.29240478622501 Validation Loss: 1.8750813978840597e-05\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:48:24,458] Trial 34 finished with value: 1.6306281395372935e-05 and parameters: {'lr': 0.007240183391544309, 'weight_decay': 1.3647269057168989e-05}. Best is trial 34 with value: 1.6306281395372935e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 59: Training Loss: 0.000748127706174273 NSE : 0.8723931312561035 WAPE : 16.639840737112007 Validation Loss: 2.7601268811849877e-05\n","Early stopping!\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:48:25,962] Trial 35 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.019147287734085694 NSE : -5.648763656616211 WAPE : 158.08779470930355 Validation Loss: 0.0014381224755197763\n","Epoch 1: Training Loss: 0.008381072329939343 NSE : 0.14856266975402832 WAPE : 43.89251839653124 Validation Loss: 0.00018416524108033627\n","Epoch 2: Training Loss: 0.004457828451450041 NSE : 0.25962668657302856 WAPE : 33.67411978070084 Validation Loss: 0.00016014219727367163\n","Epoch 3: Training Loss: 0.003746959460841026 NSE : -0.2482588291168213 WAPE : 52.2823622605324 Validation Loss: 0.00026999739930033684\n","Epoch 4: Training Loss: 0.0035518698659871006 NSE : 0.4899260401725769 WAPE : 32.324227137228746 Validation Loss: 0.00011032860493287444\n","Epoch 5: Training Loss: 0.0031224725462379865 NSE : -0.41505300998687744 WAPE : 70.53600717099914 Validation Loss: 0.00030607491498813033\n","Epoch 6: Training Loss: 0.002606828733405564 NSE : 0.20456963777542114 WAPE : 51.126372183194015 Validation Loss: 0.0001720509899314493\n","Epoch 7: Training Loss: 0.00237521632516291 NSE : -0.2685811519622803 WAPE : 67.37725709282691 Validation Loss: 0.00027439312543720007\n","Epoch 8: Training Loss: 0.002067894023639383 NSE : 0.01731741428375244 WAPE : 58.8336620041275 Validation Loss: 0.00021255349565763026\n","Epoch 9: Training Loss: 0.0018402832210995257 NSE : 0.6344298124313354 WAPE : 31.685741385420357 Validation Loss: 7.907253893790767e-05\n","Epoch 10: Training Loss: 0.0016512219581272802 NSE : 0.8079918622970581 WAPE : 17.375525838527444 Validation Loss: 4.1531216993462294e-05\n","Epoch 11: Training Loss: 0.001487020908825798 NSE : 0.684157133102417 WAPE : 25.913435200433593 Validation Loss: 6.831657810835168e-05\n","Epoch 12: Training Loss: 0.001350172142338124 NSE : 0.650306224822998 WAPE : 28.84009922661608 Validation Loss: 7.563850522274151e-05\n","Epoch 13: Training Loss: 0.0013532919911085628 NSE : 0.2099834680557251 WAPE : 49.83014738070918 Validation Loss: 0.00017087996820919216\n","Epoch 14: Training Loss: 0.0013580974646174582 NSE : 0.300028920173645 WAPE : 45.9302370181985 Validation Loss: 0.00015140321920625865\n","Epoch 15: Training Loss: 0.0011938404541069758 NSE : 0.5787478685379028 WAPE : 34.103254049321464 Validation Loss: 9.111651161219925e-05\n","Epoch 16: Training Loss: 0.001153431639977498 NSE : 0.0949663519859314 WAPE : 55.15466010714807 Validation Loss: 0.00019575809710659087\n","Epoch 17: Training Loss: 0.0012578107462104526 NSE : 0.48959338665008545 WAPE : 38.48853682433136 Validation Loss: 0.00011040057142963633\n","Epoch 18: Training Loss: 0.0012006430747533159 NSE : -0.16040277481079102 WAPE : 63.957290863229865 Validation Loss: 0.00025099425693042576\n","Epoch 19: Training Loss: 0.00107436047437659 NSE : 0.6980413198471069 WAPE : 29.027479101957432 Validation Loss: 6.531343387905508e-05\n","Epoch 20: Training Loss: 0.0009983639520214638 NSE : 0.8399644494056702 WAPE : 17.266294219424235 Validation Loss: 3.4615572076290846e-05\n","Epoch 21: Training Loss: 0.001022332709908369 NSE : -0.04803669452667236 WAPE : 60.77207479518876 Validation Loss: 0.00022668954625260085\n","Epoch 22: Training Loss: 0.0009003551472233084 NSE : 0.8063462972640991 WAPE : 21.595313835442244 Validation Loss: 4.188715320196934e-05\n","Epoch 23: Training Loss: 0.0008667761840115418 NSE : -0.014346599578857422 WAPE : 60.824039523879016 Validation Loss: 0.0002194023982156068\n","Epoch 24: Training Loss: 0.0009146124014023371 NSE : 0.35738635063171387 WAPE : 46.52374142711221 Validation Loss: 0.0001389968383591622\n","Epoch 25: Training Loss: 0.0009778165349416668 NSE : 0.6010109186172485 WAPE : 34.89697525588376 Validation Loss: 8.630103548057377e-05\n","Epoch 26: Training Loss: 0.0009189394586428534 NSE : 0.6156938672065735 WAPE : 34.39805299034834 Validation Loss: 8.312513091368601e-05\n","Epoch 27: Training Loss: 0.0008547121287847403 NSE : 0.8900544047355652 WAPE : 15.218462195910028 Validation Loss: 2.3781149138812907e-05\n","Epoch 28: Training Loss: 0.0008514712462783791 NSE : 0.6006036996841431 WAPE : 35.823526714056406 Validation Loss: 8.63891327753663e-05\n","Epoch 29: Training Loss: 0.0008118240848489222 NSE : 0.9239524602890015 WAPE : 11.810952450438807 Validation Loss: 1.6449026588816196e-05\n","Epoch 30: Training Loss: 0.0007807511847204296 NSE : 0.8568764328956604 WAPE : 19.783594254862315 Validation Loss: 3.09575189021416e-05\n","Epoch 31: Training Loss: 0.00082101816587965 NSE : 0.5576509237289429 WAPE : 39.1890058577057 Validation Loss: 9.567976667312905e-05\n","Epoch 32: Training Loss: 0.0008662397940497613 NSE : -0.3306084871292114 WAPE : 71.05177711534051 Validation Loss: 0.0002878096420317888\n","Epoch 33: Training Loss: 0.0008083430909664457 NSE : 0.6741698980331421 WAPE : 32.30642263033916 Validation Loss: 7.047679537208751e-05\n","Epoch 34: Training Loss: 0.0007041108874545898 NSE : 0.9058710336685181 WAPE : 14.036555418898919 Validation Loss: 2.0360017515486106e-05\n","Epoch 35: Training Loss: 0.0007286628087967983 NSE : 0.49628371000289917 WAPE : 41.707369035458925 Validation Loss: 0.00010895344894379377\n","Epoch 36: Training Loss: 0.0007790163288063923 NSE : 0.7238351702690125 WAPE : 28.964153342644515 Validation Loss: 5.973424777039327e-05\n","Epoch 37: Training Loss: 0.0007577939572911419 NSE : 0.9084553122520447 WAPE : 13.333816263992828 Validation Loss: 1.9801043890765868e-05\n","Epoch 38: Training Loss: 0.0007207272706182266 NSE : 0.9099975824356079 WAPE : 13.325813512330367 Validation Loss: 1.9467455786070786e-05\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:49:29,789] Trial 36 finished with value: 1.6449026588816196e-05 and parameters: {'lr': 0.006735089685746013, 'weight_decay': 1.727263993007296e-05}. Best is trial 34 with value: 1.6306281395372935e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 39: Training Loss: 0.0006973217477934668 NSE : 0.8026918172836304 WAPE : 24.182418544537324 Validation Loss: 4.2677616875153035e-05\n","Early stopping!\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:49:31,333] Trial 37 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.012800656753825024 NSE : -6.955783367156982 WAPE : 173.00155510621 Validation Loss: 0.001720829983241856\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:49:32,834] Trial 38 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.008272196109828656 NSE : -5.782522201538086 WAPE : 159.07489108002753 Validation Loss: 0.0014670543605461717\n","Epoch 1: Training Loss: 0.01836305548204109 NSE : 0.34426599740982056 WAPE : 36.01750224093723 Validation Loss: 0.00014183476741891354\n","Epoch 2: Training Loss: 0.0046825905355945 NSE : -1.048264980316162 WAPE : 76.20300598278126 Validation Loss: 0.0004430381231941283\n","Epoch 3: Training Loss: 0.004073234232237155 NSE : -0.7687942981719971 WAPE : 72.19600175105792 Validation Loss: 0.0003825888561550528\n","Epoch 4: Training Loss: 0.0035006322177650873 NSE : -1.431868553161621 WAPE : 89.67855579412561 Validation Loss: 0.0005260112811811268\n","Epoch 5: Training Loss: 0.0030865722128510242 NSE : -0.9492747783660889 WAPE : 79.08811573659086 Validation Loss: 0.00042162672616541386\n","Epoch 6: Training Loss: 0.003049641547477222 NSE : -1.8123879432678223 WAPE : 98.74083925705114 Validation Loss: 0.000608317437581718\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:49:44,381] Trial 39 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7: Training Loss: 0.0029550113367804443 NSE : -2.359384536743164 WAPE : 109.03970315398888 Validation Loss: 0.0007266323664225638\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:49:45,920] Trial 40 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.012947956449352205 NSE : -19.071123123168945 WAPE : 277.77699026495173 Validation Loss: 0.004341368097811937\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:49:47,392] Trial 41 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.004936750861816108 NSE : -3.357767105102539 WAPE : 127.30165308207042 Validation Loss: 0.0009425817406736314\n","Epoch 1: Training Loss: 0.00929815376730403 NSE : -0.47651708126068115 WAPE : 52.562006212086466 Validation Loss: 0.0003193695447407663\n","Epoch 2: Training Loss: 0.005179032468731748 NSE : 0.32737016677856445 WAPE : 35.46019470096516 Validation Loss: 0.0001454893354093656\n","Epoch 3: Training Loss: 0.003956987198762363 NSE : -0.38373899459838867 WAPE : 55.748372975339265 Validation Loss: 0.0002993016969412565\n","Epoch 4: Training Loss: 0.0037280452033883194 NSE : -0.5582557916641235 WAPE : 65.10801942840466 Validation Loss: 0.0003370495396666229\n","Epoch 5: Training Loss: 0.003308830375317484 NSE : -1.4645817279815674 WAPE : 89.71893852535906 Validation Loss: 0.0005330871790647507\n","Epoch 6: Training Loss: 0.0029914136703155236 NSE : -1.7519619464874268 WAPE : 96.57422192574681 Validation Loss: 0.0005952473147772253\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:49:59,109] Trial 42 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7: Training Loss: 0.0027474126654851716 NSE : -3.5440597534179688 WAPE : 128.72765629234328 Validation Loss: 0.0009828766342252493\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:50:00,530] Trial 43 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.008079645747784525 NSE : -5.350657939910889 WAPE : 154.39964978841383 Validation Loss: 0.0013736425898969173\n","Epoch 1: Training Loss: 0.0097556232358329 NSE : -0.44798529148101807 WAPE : 68.89490733985116 Validation Loss: 0.0003131980774924159\n","Epoch 2: Training Loss: 0.004285619394067908 NSE : 0.24594980478286743 WAPE : 36.115603176919386 Validation Loss: 0.000163100499776192\n","Epoch 3: Training Loss: 0.003482795975287445 NSE : 0.3699995279312134 WAPE : 30.52130036897292 Validation Loss: 0.00013626861618831754\n","Epoch 4: Training Loss: 0.002973417744215112 NSE : 0.24928325414657593 WAPE : 35.15995914198161 Validation Loss: 0.00016237945237662643\n","Epoch 5: Training Loss: 0.0031227459221554454 NSE : -0.10984349250793457 WAPE : 44.803969064643226 Validation Loss: 0.00024005831801332533\n","Epoch 6: Training Loss: 0.0031680326737841824 NSE : -0.3733985424041748 WAPE : 62.48427591669967 Validation Loss: 0.00029706506757065654\n","Epoch 7: Training Loss: 0.0023253253239090554 NSE : 0.6059404015541077 WAPE : 30.884538575389296 Validation Loss: 8.523479482391849e-05\n","Epoch 8: Training Loss: 0.002023944003667566 NSE : 0.7778089046478271 WAPE : 19.256965666756997 Validation Loss: 4.805977368960157e-05\n","Epoch 9: Training Loss: 0.0014815137210462126 NSE : 0.4578871726989746 WAPE : 38.62783348272915 Validation Loss: 0.00011725860531441867\n","Epoch 10: Training Loss: 0.0014149405378702795 NSE : 0.6906297206878662 WAPE : 27.410341664755787 Validation Loss: 6.691656017210335e-05\n","Epoch 11: Training Loss: 0.0013438209161904524 NSE : 0.4569542407989502 WAPE : 39.39534301974109 Validation Loss: 0.00011746038217097521\n","Epoch 12: Training Loss: 0.0011586092027755512 NSE : 0.5006347894668579 WAPE : 37.96043859831982 Validation Loss: 0.00010801231837831438\n","Epoch 13: Training Loss: 0.0012415786586643662 NSE : -0.21825754642486572 WAPE : 63.46671113797919 Validation Loss: 0.0002635082055348903\n","Epoch 14: Training Loss: 0.0013423120417428436 NSE : -0.6531156301498413 WAPE : 73.32059786120782 Validation Loss: 0.00035756765282712877\n","Epoch 15: Training Loss: 0.001565466882311739 NSE : 0.41529273986816406 WAPE : 34.268483041837776 Validation Loss: 0.00012647174298763275\n","Epoch 16: Training Loss: 0.001607000613148557 NSE : -0.9212421178817749 WAPE : 83.04518980217215 Validation Loss: 0.0004155632632318884\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:50:27,689] Trial 44 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17: Training Loss: 0.0012699356811936013 NSE : -0.3463163375854492 WAPE : 67.13473556940652 Validation Loss: 0.0002912072231993079\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:50:29,179] Trial 45 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005857192591065541 NSE : -6.051388740539551 WAPE : 162.96732609284777 Validation Loss: 0.001525210216641426\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:50:30,690] Trial 46 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005216791127168108 NSE : -1.387279748916626 WAPE : 91.99327927289403 Validation Loss: 0.0005163668538443744\n","Epoch 1: Training Loss: 0.00548876463653869 NSE : 0.2865903377532959 WAPE : 36.62922182151716 Validation Loss: 0.00015430997882504016\n","Epoch 2: Training Loss: 0.004266317235305905 NSE : -2.057129383087158 WAPE : 98.1910737737383 Validation Loss: 0.0006612548022530973\n","Epoch 3: Training Loss: 0.005156817351235077 NSE : -8.095739364624023 WAPE : 186.360159262888 Validation Loss: 0.001967401709407568\n","Epoch 4: Training Loss: 0.00370453470168286 NSE : -0.23907041549682617 WAPE : 63.46887077609389 Validation Loss: 0.00026800998602993786\n","Epoch 5: Training Loss: 0.0033596034518268425 NSE : 0.5219711661338806 WAPE : 29.389510329157197 Validation Loss: 0.0001033972657751292\n","Epoch 6: Training Loss: 0.003429429038078524 NSE : -2.010128974914551 WAPE : 106.33876300264744 Validation Loss: 0.0006510886014439166\n","Epoch 7: Training Loss: 0.0029229306855995674 NSE : -0.5721784830093384 WAPE : 75.15568155760772 Validation Loss: 0.0003400610003154725\n","Epoch 8: Training Loss: 0.0025771974396775477 NSE : 0.6690044403076172 WAPE : 23.57492651810469 Validation Loss: 7.15940841473639e-05\n","Epoch 9: Training Loss: 0.0025342726785311243 NSE : 0.5355297327041626 WAPE : 35.62278876821413 Validation Loss: 0.0001004645528155379\n","Epoch 10: Training Loss: 0.0024643936594657134 NSE : -0.16136503219604492 WAPE : 64.23650955785787 Validation Loss: 0.0002512024075258523\n","Epoch 11: Training Loss: 0.0022853041264170315 NSE : -0.19760560989379883 WAPE : 65.55119551395634 Validation Loss: 0.0002590411459095776\n","Epoch 12: Training Loss: 0.002132640656782314 NSE : -0.020239710807800293 WAPE : 60.11769193887974 Validation Loss: 0.00022067705867812037\n","Epoch 13: Training Loss: 0.0019942095614169375 NSE : 0.444156289100647 WAPE : 41.86506847887265 Validation Loss: 0.00012022856390103698\n","Epoch 14: Training Loss: 0.0018926754974017967 NSE : 0.6688389182090759 WAPE : 29.3903045590044 Validation Loss: 7.16298891347833e-05\n","Epoch 15: Training Loss: 0.0018209082818430034 NSE : 0.7702832221984863 WAPE : 20.85005524170853 Validation Loss: 4.96875582030043e-05\n","Epoch 16: Training Loss: 0.0017613295240153093 NSE : 0.7980233430862427 WAPE : 16.999881178211837 Validation Loss: 4.368740337667987e-05\n","Epoch 17: Training Loss: 0.0017224882785740192 NSE : 0.8066774606704712 WAPE : 16.733937170373768 Validation Loss: 4.181551048532128e-05\n","Epoch 18: Training Loss: 0.0016874345146788983 NSE : 0.8092784881591797 WAPE : 17.877405098913925 Validation Loss: 4.125291889067739e-05\n","Epoch 19: Training Loss: 0.0016526720464753453 NSE : 0.6490272283554077 WAPE : 31.917429280190113 Validation Loss: 7.5915151683148e-05\n","Epoch 20: Training Loss: 0.0015938640499371104 NSE : 0.2663848400115967 WAPE : 50.683012653478144 Validation Loss: 0.00015868039918132126\n","Epoch 21: Training Loss: 0.0015319671692850534 NSE : -0.17394208908081055 WAPE : 65.73873381834858 Validation Loss: 0.0002539227716624737\n","Epoch 22: Training Loss: 0.0014553884884662693 NSE : 0.05357128381729126 WAPE : 58.62499426736987 Validation Loss: 0.00020471180323511362\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:51:07,991] Trial 47 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23: Training Loss: 0.0013639677176797704 NSE : 0.6027343273162842 WAPE : 35.5322632423756 Validation Loss: 8.592827362008393e-05\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:51:09,385] Trial 48 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005365648463339312 NSE : -4.166945457458496 WAPE : 137.54495007400305 Validation Loss: 0.0011176062980666757\n","Epoch 1: Training Loss: 0.010872668848605826 NSE : 0.2465415596961975 WAPE : 35.48111984323862 Validation Loss: 0.00016297248657792807\n","Epoch 2: Training Loss: 0.004756902068038471 NSE : -0.2970244884490967 WAPE : 51.49472806487253 Validation Loss: 0.00028054541326127946\n","Epoch 3: Training Loss: 0.004025505048048217 NSE : -1.0294556617736816 WAPE : 77.83632611369369 Validation Loss: 0.00043896978604607284\n","Epoch 4: Training Loss: 0.0037376166474132333 NSE : -1.1064660549163818 WAPE : 80.5133351399804 Validation Loss: 0.00045562698505818844\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:51:17,038] Trial 49 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Training Loss: 0.003531001045303128 NSE : -1.6625478267669678 WAPE : 95.18612495049092 Validation Loss: 0.0005759071209467947\n","Epoch 1: Training Loss: 0.014740639177034609 NSE : -0.4925493001937866 WAPE : 55.43184423922787 Validation Loss: 0.000322837324347347\n","Epoch 2: Training Loss: 0.005189125857214094 NSE : 0.2557350993156433 WAPE : 34.51361447541223 Validation Loss: 0.0001609839528100565\n","Epoch 3: Training Loss: 0.004027671608128003 NSE : -0.40533447265625 WAPE : 56.681807758854305 Validation Loss: 0.0003039727744180709\n","Epoch 4: Training Loss: 0.00397650666309346 NSE : -1.3726277351379395 WAPE : 84.63084780388151 Validation Loss: 0.0005131976213306189\n","Epoch 5: Training Loss: 0.0041294231996289454 NSE : 0.4397094249725342 WAPE : 30.820393571115883 Validation Loss: 0.00012119043822167441\n","Epoch 6: Training Loss: 0.003455984315223759 NSE : 0.4532909393310547 WAPE : 36.13626566050322 Validation Loss: 0.00011825274850707501\n","Epoch 7: Training Loss: 0.0030679213559778873 NSE : 0.523598313331604 WAPE : 34.03409559942465 Validation Loss: 0.00010304531315341592\n","Epoch 8: Training Loss: 0.0028620406228583306 NSE : 0.6206132769584656 WAPE : 27.204313022451064 Validation Loss: 8.206105849239975e-05\n","Epoch 9: Training Loss: 0.002933281102741603 NSE : -0.42025792598724365 WAPE : 70.77852869441953 Validation Loss: 0.0003072007093578577\n","Epoch 10: Training Loss: 0.0026506396679906175 NSE : -0.19918334484100342 WAPE : 65.04062037480978 Validation Loss: 0.0002593824465293437\n","Epoch 11: Training Loss: 0.002294641701155342 NSE : 0.6294451951980591 WAPE : 30.094823956140164 Validation Loss: 8.015072671696544e-05\n","Epoch 12: Training Loss: 0.002238427906377183 NSE : 0.6291983127593994 WAPE : 23.569831773363074 Validation Loss: 8.020412496989593e-05\n","Epoch 13: Training Loss: 0.0022952989829718717 NSE : 0.6367758512496948 WAPE : 24.710808613537345 Validation Loss: 7.85651063779369e-05\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:51:39,539] Trial 50 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14: Training Loss: 0.002330153754883213 NSE : 0.582425594329834 WAPE : 31.962452314940275 Validation Loss: 9.032103844219819e-05\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:51:40,914] Trial 51 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.007291131798410788 NSE : -5.22137451171875 WAPE : 152.58722144629047 Validation Loss: 0.0013456784654408693\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:51:42,291] Trial 52 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006479147472418845 NSE : -1.6064162254333496 WAPE : 96.21159450501344 Validation Loss: 0.0005637658177874982\n","Epoch 1: Training Loss: 0.006128315624664538 NSE : 0.08778202533721924 WAPE : 50.042458985637154 Validation Loss: 0.0001973120670299977\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:51:45,787] Trial 53 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.004051518604683224 NSE : -1.0129163265228271 WAPE : 75.06705301119426 Validation Loss: 0.00043539234320633113\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:51:48,088] Trial 54 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.007391314229607815 NSE : -1.2674150466918945 WAPE : 89.48323778949782 Validation Loss: 0.0004904401721432805\n","Epoch 1: Training Loss: 0.006170892327645561 NSE : -1.7075779438018799 WAPE : 95.6147339017323 Validation Loss: 0.0005856470088474452\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:51:50,535] Trial 55 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006274365863646381 NSE : 0.22283172607421875 WAPE : 39.47015905442871 Validation Loss: 0.00016810090164653957\n","Epoch 2: Training Loss: 0.00434312125435099 NSE : 0.2981610894203186 WAPE : 31.742659106543535 Validation Loss: 0.0001518072240287438\n","Epoch 3: Training Loss: 0.004583002155413851 NSE : -5.610989570617676 WAPE : 158.3486689875133 Validation Loss: 0.0014299519825726748\n","Epoch 4: Training Loss: 0.003564822331100004 NSE : -0.09561800956726074 WAPE : 58.581926580642474 Validation Loss: 0.0002369813300902024\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:51:58,108] Trial 56 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Training Loss: 0.0034780450296238996 NSE : -0.48800814151763916 WAPE : 71.52628046111192 Validation Loss: 0.0003218549827579409\n","Epoch 1: Training Loss: 0.005069867011116003 NSE : -0.49949145317077637 WAPE : 68.3023743511705 Validation Loss: 0.00032433890737593174\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:01,150] Trial 57 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.004434446236700751 NSE : -4.580471515655518 WAPE : 144.60286423047256 Validation Loss: 0.0012070516822859645\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:03,131] Trial 58 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.015133836976019666 NSE : -4.86754035949707 WAPE : 148.08218298555377 Validation Loss: 0.0012691443553194404\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:05,740] Trial 59 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005753600727985031 NSE : -2.301596164703369 WAPE : 109.89926413875048 Validation Loss: 0.000714132736902684\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:07,200] Trial 60 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.007339338617384783 NSE : -3.5276336669921875 WAPE : 129.11326426382607 Validation Loss: 0.000979323755018413\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:08,736] Trial 61 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006100061524193734 NSE : -16.33541488647461 WAPE : 257.78017552271166 Validation Loss: 0.0037496371660381556\n","Epoch 1: Training Loss: 0.008933823730330914 NSE : -0.9093847274780273 WAPE : 80.88655229200977 Validation Loss: 0.0004129984590690583\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:11,599] Trial 62 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.004298128600566997 NSE : -0.9875445365905762 WAPE : 74.37957099080694 Validation Loss: 0.00042990437941625714\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:13,006] Trial 63 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.01643136888742447 NSE : -5.106118202209473 WAPE : 150.6762002042901 Validation Loss: 0.0013207488227635622\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:14,531] Trial 64 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006120528181781992 NSE : -13.497596740722656 WAPE : 234.47607929790917 Validation Loss: 0.0031358192209154367\n","Epoch 1: Training Loss: 0.00579772524361033 NSE : 0.06347453594207764 WAPE : 46.98982718725897 Validation Loss: 0.00020256973220966756\n","Epoch 2: Training Loss: 0.004364434207673185 NSE : 0.29436641931533813 WAPE : 38.81928665235247 Validation Loss: 0.0001526280102552846\n","Epoch 3: Training Loss: 0.0042883830901701 NSE : -4.165281772613525 WAPE : 139.6019428404661 Validation Loss: 0.0011172465747222304\n","Epoch 4: Training Loss: 0.0034035513126582373 NSE : -0.08822345733642578 WAPE : 58.51795042838381 Validation Loss: 0.0002353819290874526\n","Epoch 5: Training Loss: 0.003151928885927191 NSE : 0.5218275189399719 WAPE : 26.015134143545062 Validation Loss: 0.00010342834866605699\n","Epoch 6: Training Loss: 0.0032280709201586433 NSE : -1.081336259841919 WAPE : 87.54171478601656 Validation Loss: 0.0004501914663705975\n","Epoch 7: Training Loss: 0.0029116788027749863 NSE : -1.7612698078155518 WAPE : 102.03912363719748 Validation Loss: 0.0005972606595605612\n","Epoch 8: Training Loss: 0.002522401325222745 NSE : 0.6516070365905762 WAPE : 26.014934022638673 Validation Loss: 7.535713666584343e-05\n","Epoch 9: Training Loss: 0.0024459384731017053 NSE : 0.7053472399711609 WAPE : 22.38363177753226 Validation Loss: 6.373316136887297e-05\n","Epoch 10: Training Loss: 0.0024581662873970345 NSE : 0.41650116443634033 WAPE : 42.39790289966855 Validation Loss: 0.00012621036148630083\n","Epoch 11: Training Loss: 0.0023524372300016694 NSE : -0.20299088954925537 WAPE : 65.79504283838152 Validation Loss: 0.0002602060267236084\n","Epoch 12: Training Loss: 0.0021996269024384674 NSE : -0.41485917568206787 WAPE : 72.05734297804925 Validation Loss: 0.00030603300547227263\n","Epoch 13: Training Loss: 0.002049825675385364 NSE : 0.32162022590637207 WAPE : 47.34824373058723 Validation Loss: 0.0001467330293962732\n","Epoch 14: Training Loss: 0.0019441037147771567 NSE : 0.6394680738449097 WAPE : 31.158253945091825 Validation Loss: 7.798277511028573e-05\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:39,522] Trial 65 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15: Training Loss: 0.0018907522508015973 NSE : 0.6902533769607544 WAPE : 27.790871568239144 Validation Loss: 6.699795630993322e-05\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:40,892] Trial 66 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006205737501659314 NSE : -4.806244850158691 WAPE : 146.69848033186716 Validation Loss: 0.00125588639639318\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:42,321] Trial 67 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.00699254177743569 NSE : -7.980720520019531 WAPE : 183.77903316587106 Validation Loss: 0.0019425231730565429\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:43,748] Trial 68 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.00626983807887882 NSE : -3.6679697036743164 WAPE : 132.18029225990702 Validation Loss: 0.0010096783516928554\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:45,737] Trial 69 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.007389897342363838 NSE : -1.4828290939331055 WAPE : 91.6294761418357 Validation Loss: 0.0005370340077206492\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:47,942] Trial 70 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.012290488608414307 NSE : -13.872633934020996 WAPE : 239.2452273248421 Validation Loss: 0.003216939279809594\n","Epoch 1: Training Loss: 0.006562336020579096 NSE : 0.0014347434043884277 WAPE : 49.74947364032437 Validation Loss: 0.00021598892635665834\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:51,096] Trial 71 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.004656392018659972 NSE : -5.088514804840088 WAPE : 151.29730879072773 Validation Loss: 0.0013169412268325686\n","Epoch 1: Training Loss: 0.005623221019050106 NSE : -0.35063135623931885 WAPE : 63.140430676867275 Validation Loss: 0.00029214052483439445\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:53,992] Trial 72 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.004481742573261727 NSE : -6.053553104400635 WAPE : 163.17203310333326 Validation Loss: 0.0015256782062351704\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:55,533] Trial 73 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.017795564053812996 NSE : -4.960597991943359 WAPE : 149.742719559734 Validation Loss: 0.0012892729137092829\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:56,995] Trial 74 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.00807616562815383 NSE : -4.458481788635254 WAPE : 142.95716995684893 Validation Loss: 0.0011806654511019588\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:52:58,647] Trial 75 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005449432892419281 NSE : -3.440967082977295 WAPE : 128.19863667632526 Validation Loss: 0.0009605778031982481\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:53:00,900] Trial 76 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.007273102142789867 NSE : -2.2991647720336914 WAPE : 101.00902628671489 Validation Loss: 0.0007136068306863308\n","Epoch 1: Training Loss: 0.005366136108932551 NSE : -0.6628631353378296 WAPE : 73.26159554730984 Validation Loss: 0.00035967602161690593\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:53:04,471] Trial 77 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.0045758710912195966 NSE : -4.055078983306885 WAPE : 137.10427549978112 Validation Loss: 0.0010934098390862346\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:53:05,880] Trial 78 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.00525511341948004 NSE : -2.9275197982788086 WAPE : 120.13544850013548 Validation Loss: 0.0008495195652358234\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:53:07,353] Trial 79 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.0055799899273552 NSE : -12.968485832214355 WAPE : 230.09263096454106 Validation Loss: 0.0030213731806725264\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:53:08,860] Trial 80 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006146714711576351 NSE : -1.3516521453857422 WAPE : 91.06405953596965 Validation Loss: 0.0005086605669930577\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:53:10,379] Trial 81 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006271563102927757 NSE : -1.538147211074829 WAPE : 93.21933668257905 Validation Loss: 0.0005489993491210043\n","Epoch 1: Training Loss: 0.005906850437895628 NSE : -0.9019914865493774 WAPE : 79.30133622396865 Validation Loss: 0.00041139934910461307\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:53:13,810] Trial 82 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.004504081953200512 NSE : -4.0533366203308105 WAPE : 137.2481707698401 Validation Loss: 0.0010930330026894808\n","Epoch 1: Training Loss: 0.005917062899243319 NSE : 0.21087515354156494 WAPE : 38.73024952575514 Validation Loss: 0.00017068712622858584\n","Epoch 2: Training Loss: 0.004400216603244189 NSE : -0.7414301633834839 WAPE : 77.1875508119489 Validation Loss: 0.00037667001015506685\n","Epoch 3: Training Loss: 0.0040400461803073995 NSE : -3.0720348358154297 WAPE : 123.14550040649559 Validation Loss: 0.00088077801046893\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:53:20,636] Trial 83 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Training Loss: 0.003485160086711403 NSE : 0.1977958083152771 WAPE : 47.470826124116655 Validation Loss: 0.0001735161495162174\n","Epoch 1: Training Loss: 0.005726112740376266 NSE : 0.030621469020843506 WAPE : 52.008521815263386 Validation Loss: 0.0002096758398693055\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:53:23,610] Trial 84 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.004012687990325503 NSE : -3.2578659057617188 WAPE : 120.5258093431448 Validation Loss: 0.0009209731360897422\n","Epoch 1: Training Loss: 0.0052792066053370945 NSE : -0.06470060348510742 WAPE : 53.367384461445454 Validation Loss: 0.00023029395379126072\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:53:26,501] Trial 85 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.0044409644906409085 NSE : -1.0498771667480469 WAPE : 84.49369827604177 Validation Loss: 0.00044338690349832177\n","Epoch 1: Training Loss: 0.008511658663337585 NSE : 0.29855644702911377 WAPE : 35.00082966792437 Validation Loss: 0.00015172171697486192\n","Epoch 2: Training Loss: 0.0035412463839747943 NSE : 0.30108869075775146 WAPE : 34.414896499968734 Validation Loss: 0.00015117399743758142\n","Epoch 3: Training Loss: 0.003219204500055639 NSE : 0.3956655263900757 WAPE : 41.90498426132455 Validation Loss: 0.00013071708963252604\n","Epoch 4: Training Loss: 0.002449649568006862 NSE : 0.6262850165367126 WAPE : 27.40704175439328 Validation Loss: 8.083428110694513e-05\n","Epoch 5: Training Loss: 0.0023502616186306113 NSE : 0.5881292819976807 WAPE : 28.822861728961247 Validation Loss: 8.908731979317963e-05\n","Epoch 6: Training Loss: 0.0021477314803632908 NSE : 0.32200193405151367 WAPE : 41.67434908590607 Validation Loss: 0.00014665047638118267\n","Epoch 7: Training Loss: 0.0019340968938195147 NSE : -0.6650141477584839 WAPE : 75.43010568885367 Validation Loss: 0.0003601412463467568\n","Epoch 8: Training Loss: 0.0017551536675455282 NSE : -0.03797805309295654 WAPE : 57.11057096996102 Validation Loss: 0.00022451384575106204\n","Epoch 9: Training Loss: 0.0017679941593087278 NSE : -1.3077888488769531 WAPE : 90.48947072189448 Validation Loss: 0.0004991729510948062\n","Epoch 10: Training Loss: 0.0022315836358757224 NSE : -1.1421356201171875 WAPE : 85.897121177378 Validation Loss: 0.00046334235230460763\n","Epoch 11: Training Loss: 0.0019169370534655172 NSE : -0.03127443790435791 WAPE : 56.354009714202334 Validation Loss: 0.00022306389291770756\n","Epoch 12: Training Loss: 0.0013915370691393036 NSE : 0.6473349332809448 WAPE : 29.63838360676242 Validation Loss: 7.628119055880234e-05\n","Epoch 13: Training Loss: 0.0013737471144850133 NSE : 0.31007683277130127 WAPE : 44.75400971420233 Validation Loss: 0.00014922987611498684\n","Epoch 14: Training Loss: 0.0013485749477695208 NSE : -0.4193376302719116 WAPE : 69.60852598444893 Validation Loss: 0.0003070016682613641\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:53:50,657] Trial 86 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15: Training Loss: 0.0018416095335851423 NSE : -2.3193421363830566 WAPE : 110.90582226762002 Validation Loss: 0.0007179711246863008\n","Epoch 1: Training Loss: 0.005214697039264138 NSE : -0.15602803230285645 WAPE : 56.680815492693505 Validation Loss: 0.000250048004090786\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:53:53,902] Trial 87 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.004437689727637917 NSE : -3.9589271545410156 WAPE : 136.2342581976611 Validation Loss: 0.0010726122418418527\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:53:55,641] Trial 88 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.009330289030913264 NSE : -9.418087005615234 WAPE : 199.22604907131392 Validation Loss: 0.0022534243762493134\n","Epoch 1: Training Loss: 0.005934910193900578 NSE : 0.205155611038208 WAPE : 38.57018615413479 Validation Loss: 0.000171924228197895\n","Epoch 2: Training Loss: 0.004514369138632901 NSE : -2.1923704147338867 WAPE : 107.7858205999458 Validation Loss: 0.000690507295075804\n","Epoch 3: Training Loss: 0.003949575824663043 NSE : -1.589012861251831 WAPE : 96.38820120489461 Validation Loss: 0.0005600015283562243\n","Epoch 4: Training Loss: 0.0035620226663013455 NSE : 0.34174126386642456 WAPE : 40.18850555543975 Validation Loss: 0.00014238085714168847\n","Epoch 5: Training Loss: 0.003604229277698323 NSE : -0.934205174446106 WAPE : 83.283967396969 Validation Loss: 0.00041836712625809014\n","Epoch 6: Training Loss: 0.003238795936340466 NSE : -1.7701025009155273 WAPE : 101.75905442871735 Validation Loss: 0.0005991710931994021\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:06,835] Trial 89 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7: Training Loss: 0.002824431358021684 NSE : 0.3513060212135315 WAPE : 43.662095849575785 Validation Loss: 0.00014031201135367155\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:08,344] Trial 90 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005403540562838316 NSE : -6.965981483459473 WAPE : 171.7824352212795 Validation Loss: 0.001723035704344511\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:10,390] Trial 91 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005800510945846327 NSE : -1.898331642150879 WAPE : 102.05487482020388 Validation Loss: 0.0006269068690016866\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:12,556] Trial 92 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005195040263060946 NSE : -6.115495204925537 WAPE : 161.7776073044131 Validation Loss: 0.0015390762127935886\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:14,176] Trial 93 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.00649561952741351 NSE : -8.39834976196289 WAPE : 188.33048300014593 Validation Loss: 0.002032856224104762\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:15,720] Trial 94 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.018294088600669056 NSE : -11.28686809539795 WAPE : 216.98963123553813 Validation Loss: 0.0026576402597129345\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:17,284] Trial 95 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005889756997930817 NSE : -4.40414571762085 WAPE : 142.0777386337579 Validation Loss: 0.0011689126258715987\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:18,793] Trial 96 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.00541374313615961 NSE : -3.723593235015869 WAPE : 130.2520189281024 Validation Loss: 0.0010217097587883472\n","Epoch 1: Training Loss: 0.004979955880116904 NSE : -0.7230455875396729 WAPE : 74.62497759062768 Validation Loss: 0.0003726934373844415\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:21,926] Trial 97 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.004354261764092371 NSE : -3.9186220169067383 WAPE : 135.34866481832773 Validation Loss: 0.001063894247636199\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:23,708] Trial 98 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.010201610857620835 NSE : -10.561326026916504 WAPE : 209.87369869296032 Validation Loss: 0.0025007061194628477\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:25,887] Trial 99 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.015672850582632236 NSE : -15.010345458984375 WAPE : 248.73847949803007 Validation Loss: 0.003463025437667966\n","Epoch 1: Training Loss: 0.020722368179121986 NSE : 0.22970902919769287 WAPE : 41.48707344020345 Validation Loss: 0.000166613346664235\n","Epoch 2: Training Loss: 0.004410923094837926 NSE : 0.4632222652435303 WAPE : 29.89891392716433 Validation Loss: 0.00011610462388489395\n","Epoch 3: Training Loss: 0.003271729168773163 NSE : 0.5698546767234802 WAPE : 27.416386983802717 Validation Loss: 9.304011473432183e-05\n","Epoch 4: Training Loss: 0.00298367568757385 NSE : 0.6120588183403015 WAPE : 26.93395593170874 Validation Loss: 8.391139272134751e-05\n","Epoch 5: Training Loss: 0.002336980100153596 NSE : 0.2914595603942871 WAPE : 42.5490483833983 Validation Loss: 0.00015325675485655665\n","Epoch 6: Training Loss: 0.002145294609363191 NSE : 0.14051836729049683 WAPE : 48.985487065101836 Validation Loss: 0.00018590522813610733\n","Epoch 7: Training Loss: 0.001998854222620139 NSE : 0.2522100806236267 WAPE : 45.332079798211424 Validation Loss: 0.0001617464004084468\n","Epoch 8: Training Loss: 0.0018004917219514027 NSE : -0.419561505317688 WAPE : 67.65820183027246 Validation Loss: 0.0003070500970352441\n","Epoch 9: Training Loss: 0.0017176012770505622 NSE : -0.8761731386184692 WAPE : 80.09397344228805 Validation Loss: 0.0004058147897012532\n","Epoch 10: Training Loss: 0.0017348613146168645 NSE : -1.9416429996490479 WAPE : 102.7349023368285 Validation Loss: 0.0006362751591950655\n","Epoch 11: Training Loss: 0.002128021704265848 NSE : -4.087172031402588 WAPE : 138.6541785662171 Validation Loss: 0.001100351451896131\n","Epoch 12: Training Loss: 0.002310705756826792 NSE : -4.151358604431152 WAPE : 138.75339684392654 Validation Loss: 0.0011142349103465676\n","Epoch 13: Training Loss: 0.0019996711635030806 NSE : 0.14095091819763184 WAPE : 49.07487440328532 Validation Loss: 0.0001858116447692737\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:48,466] Trial 100 finished with value: 8.391139272134751e-05 and parameters: {'lr': 0.00969875980503795, 'weight_decay': 9.27878152142338e-05}. Best is trial 34 with value: 1.6306281395372935e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14: Training Loss: 0.0012561377488964354 NSE : 0.013776838779449463 WAPE : 55.855058264368054 Validation Loss: 0.0002133193629560992\n","Early stopping!\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:49,948] Trial 101 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006680516420601634 NSE : -5.83174991607666 WAPE : 158.175247545392 Validation Loss: 0.001477702520787716\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:51,566] Trial 102 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005731977285904577 NSE : -4.363498210906982 WAPE : 139.71045840195117 Validation Loss: 0.0011601205915212631\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:53,771] Trial 103 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006798044756578747 NSE : -8.507893562316895 WAPE : 188.1503741844031 Validation Loss: 0.0020565504673868418\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:55,873] Trial 104 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005673116655088961 NSE : -7.249457359313965 WAPE : 174.47722999312086 Validation Loss: 0.0017843513051047921\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:57,348] Trial 105 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005214527862335672 NSE : -1.5720818042755127 WAPE : 95.33356402826708 Validation Loss: 0.0005563393351621926\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:54:58,894] Trial 106 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.0057380370126338676 NSE : -14.203360557556152 WAPE : 240.12964499385046 Validation Loss: 0.0032884753309190273\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:00,419] Trial 107 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.00748531141289277 NSE : -1.23056960105896 WAPE : 84.84390986220842 Validation Loss: 0.00048247052473016083\n","Epoch 1: Training Loss: 0.006791963405703427 NSE : 0.020207524299621582 WAPE : 50.9428446353005 Validation Loss: 0.00021192837448325008\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:03,209] Trial 108 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.004088908734047436 NSE : -1.2285754680633545 WAPE : 79.74121031456505 Validation Loss: 0.0004820391477551311\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:04,612] Trial 109 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.008848813482472906 NSE : -12.849517822265625 WAPE : 230.06279627274813 Validation Loss: 0.0029956402722746134\n","Epoch 1: Training Loss: 0.013972473389003426 NSE : -0.4243861436843872 WAPE : 68.10302474411624 Validation Loss: 0.0003080936148762703\n","Epoch 2: Training Loss: 0.0043820561295433436 NSE : 0.2986587882041931 WAPE : 31.01677054887328 Validation Loss: 0.00015169958351179957\n","Epoch 3: Training Loss: 0.0034555292913864832 NSE : 0.2850274443626404 WAPE : 37.182581142773756 Validation Loss: 0.00015464801981579512\n","Epoch 4: Training Loss: 0.0028516259390016785 NSE : 0.6547220945358276 WAPE : 24.111371453586543 Validation Loss: 7.468333933502436e-05\n","Epoch 5: Training Loss: 0.002709847794903908 NSE : -1.1377863883972168 WAPE : 85.12472118571637 Validation Loss: 0.0004624015709850937\n","Epoch 6: Training Loss: 0.0027211740707571153 NSE : -1.7530438899993896 WAPE : 97.77116174355339 Validation Loss: 0.0005954814259894192\n","Epoch 7: Training Loss: 0.0029820937706972472 NSE : -1.779008388519287 WAPE : 98.89835108711513 Validation Loss: 0.0006010974175296724\n","Epoch 8: Training Loss: 0.0026501068023208063 NSE : 0.42538636922836304 WAPE : 37.38759667298993 Validation Loss: 0.00012428849004209042\n","Epoch 9: Training Loss: 0.00170301509251658 NSE : 0.4491131901741028 WAPE : 36.66006545621313 Validation Loss: 0.00011915639333892614\n","Epoch 10: Training Loss: 0.0016779120633145794 NSE : -0.5223785638809204 WAPE : 71.60581184465615 Validation Loss: 0.00032928938162513077\n","Epoch 11: Training Loss: 0.0018275119036843535 NSE : -1.2952213287353516 WAPE : 89.12082716641304 Validation Loss: 0.0004964546533301473\n","Epoch 12: Training Loss: 0.00208583072253532 NSE : -0.5521844625473022 WAPE : 72.40592858185153 Validation Loss: 0.00033573637483641505\n","Epoch 13: Training Loss: 0.0017045971849256603 NSE : -0.050963521003723145 WAPE : 57.3175626941277 Validation Loss: 0.00022732262732461095\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:29,056] Trial 110 finished with value: 7.468333933502436e-05 and parameters: {'lr': 0.009104016669501818, 'weight_decay': 7.38990404057489e-05}. Best is trial 34 with value: 1.6306281395372935e-05.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14: Training Loss: 0.0017280547635891708 NSE : -1.368819236755371 WAPE : 92.74139792791478 Validation Loss: 0.0005123738665133715\n","Early stopping!\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:30,508] Trial 111 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.0070637870630889665 NSE : -1.7345592975616455 WAPE : 96.01423359946635 Validation Loss: 0.0005914831417612731\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:32,055] Trial 112 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006717837957694428 NSE : -6.118349552154541 WAPE : 161.39367534552125 Validation Loss: 0.001539693446829915\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:33,568] Trial 113 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.00895309413317591 NSE : -4.953949928283691 WAPE : 148.01070646849138 Validation Loss: 0.0012878349516540766\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:35,750] Trial 114 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.011400510353269055 NSE : -5.762951374053955 WAPE : 159.36444935481853 Validation Loss: 0.0014628212666139007\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:37,895] Trial 115 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006435833191062557 NSE : -6.80001163482666 WAPE : 171.10897834108107 Validation Loss: 0.0016871365951374173\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:39,435] Trial 116 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.009492934259469621 NSE : -14.393792152404785 WAPE : 243.32959496362386 Validation Loss: 0.0033296658657491207\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:40,970] Trial 117 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005635308945784345 NSE : -13.547073364257812 WAPE : 235.405757645244 Validation Loss: 0.003146521048620343\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:42,576] Trial 118 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005732393176003825 NSE : -6.771098613739014 WAPE : 169.46721977861625 Validation Loss: 0.0016808827640488744\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:44,080] Trial 119 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.004997279385861475 NSE : -0.8843303918838501 WAPE : 80.31298909757979 Validation Loss: 0.00040757920942269266\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:45,606] Trial 120 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.011188016593223438 NSE : -0.7702069282531738 WAPE : 76.84574430385024 Validation Loss: 0.00038289441727101803\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:47,127] Trial 121 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005611879561911337 NSE : -12.465773582458496 WAPE : 225.79475099539303 Validation Loss: 0.002912636613473296\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:49,053] Trial 122 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.007500922711187741 NSE : -1.7885470390319824 WAPE : 97.59704821663088 Validation Loss: 0.0006031605880707502\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:51,262] Trial 123 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006903653276822297 NSE : -6.098304748535156 WAPE : 160.60406495591087 Validation Loss: 0.00153535814024508\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:55:53,036] Trial 124 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.0056045687379082665 NSE : -12.53874683380127 WAPE : 226.02970961622648 Validation Loss: 0.0029284206684678793\n","Epoch 1: Training Loss: 0.0056404426923108986 NSE : 0.09308946132659912 WAPE : 46.25146859560984 Validation Loss: 0.00019616405188571662\n","Epoch 2: Training Loss: 0.00430674220842775 NSE : 0.34800463914871216 WAPE : 35.793850451314334 Validation Loss: 0.00014102610293775797\n","Epoch 3: Training Loss: 0.004337717648013495 NSE : -4.474493980407715 WAPE : 143.5514289883471 Validation Loss: 0.00118412880692631\n","Epoch 4: Training Loss: 0.0035488484827510547 NSE : 0.12604379653930664 WAPE : 49.91150486752413 Validation Loss: 0.00018903607269749045\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:00,594] Trial 125 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Training Loss: 0.0035171615090803243 NSE : -0.35241079330444336 WAPE : 67.6704425590461 Validation Loss: 0.00029252542299218476\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:02,109] Trial 126 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.0051877639489248395 NSE : -10.14965534210205 WAPE : 204.83275312167768 Validation Loss: 0.002411662135273218\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:04,204] Trial 127 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005517291181604378 NSE : -8.30865478515625 WAPE : 186.36262742073336 Validation Loss: 0.0020134549122303724\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:06,245] Trial 128 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006075546698411927 NSE : -2.3711180686950684 WAPE : 108.19383377457214 Validation Loss: 0.0007291703368537128\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:07,778] Trial 129 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.0055932508475962095 NSE : -8.72930908203125 WAPE : 190.4618873902983 Validation Loss: 0.002104442100971937\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:09,238] Trial 130 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.008334153957548551 NSE : -0.9252163171768188 WAPE : 80.66617748222885 Validation Loss: 0.0004164228157605976\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:10,788] Trial 131 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006835302734543802 NSE : -5.935758113861084 WAPE : 159.85664672406244 Validation Loss: 0.0015001994324848056\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:12,325] Trial 132 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006573490158189088 NSE : -18.66417694091797 WAPE : 274.5554272372892 Validation Loss: 0.004253346938639879\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:13,847] Trial 133 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005379992275265977 NSE : -10.444552421569824 WAPE : 207.16292760209294 Validation Loss: 0.002475447952747345\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:15,429] Trial 134 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005544616287807003 NSE : -10.229793548583984 WAPE : 205.64060786725312 Validation Loss: 0.0024289959110319614\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:17,436] Trial 135 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.011349578853696585 NSE : -0.4595651626586914 WAPE : 52.90120697921661 Validation Loss: 0.0003157028113491833\n","Epoch 1: Training Loss: 0.005536297383514466 NSE : -0.36062538623809814 WAPE : 63.588347126388854 Validation Loss: 0.00029430227004922926\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:21,322] Trial 136 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.004447119063115679 NSE : -4.329695701599121 WAPE : 141.28119071939295 Validation Loss: 0.0011528092436492443\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:22,893] Trial 137 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.00694075621504453 NSE : -6.717329025268555 WAPE : 168.4892789393592 Validation Loss: 0.0016692525241523981\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:24,402] Trial 138 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006917902879649773 NSE : -10.357746124267578 WAPE : 207.00860102978882 Validation Loss: 0.002456671791151166\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:25,953] Trial 139 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.00547120956616709 NSE : -3.0248799324035645 WAPE : 120.1543849409018 Validation Loss: 0.0008705784566700459\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:27,492] Trial 140 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006337145699944813 NSE : -9.074248313903809 WAPE : 193.40534906506014 Validation Loss: 0.0021790522150695324\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:29,048] Trial 141 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.01487312640165328 NSE : -3.3873329162597656 WAPE : 121.10425048466782 Validation Loss: 0.0009489766671322286\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:30,813] Trial 142 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.013197236432461068 NSE : -7.503549575805664 WAPE : 179.69951845906903 Validation Loss: 0.0018393110949546099\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:33,358] Trial 143 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.011686875703162514 NSE : -4.220990180969238 WAPE : 134.0042775843739 Validation Loss: 0.0011292961426079273\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:35,183] Trial 144 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.008927192917326465 NSE : -5.868823528289795 WAPE : 160.99023159825728 Validation Loss: 0.0014857212081551552\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:36,620] Trial 145 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.011329135770210996 NSE : -1.3584141731262207 WAPE : 79.89021700610786 Validation Loss: 0.0005101232090964913\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:38,141] Trial 146 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.009719929308630526 NSE : -2.2142622470855713 WAPE : 107.79661879051928 Validation Loss: 0.0006952424882911146\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:39,658] Trial 147 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005461312975967303 NSE : -7.305558204650879 WAPE : 175.1465510412541 Validation Loss: 0.0017964859725907445\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:41,118] Trial 148 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.007510852730774786 NSE : -8.642745971679688 WAPE : 191.38482833378498 Validation Loss: 0.0020857187919318676\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:42,641] Trial 149 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005724415306758601 NSE : -1.9941589832305908 WAPE : 104.41081486731566 Validation Loss: 0.0006476343260146677\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:44,177] Trial 150 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.011514978425111622 NSE : -2.79199481010437 WAPE : 118.3275812470034 Validation Loss: 0.0008202056051231921\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:46,435] Trial 151 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.01476819497111137 NSE : -2.7881975173950195 WAPE : 109.41251172583435 Validation Loss: 0.0008193842368200421\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:48,471] Trial 152 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.01868257521709893 NSE : -3.2144317626953125 WAPE : 117.62035396385315 Validation Loss: 0.0009115784196183085\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:49,875] Trial 153 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005656188528519124 NSE : -0.7589048147201538 WAPE : 75.19220362302225 Validation Loss: 0.00038044978282414377\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:51,412] Trial 154 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.010902494428592036 NSE : -0.905981183052063 WAPE : 70.36966500594109 Validation Loss: 0.0004122623067814857\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:52,956] Trial 155 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.007540005841292441 NSE : -21.9227294921875 WAPE : 296.44737028621455 Validation Loss: 0.0049581690691411495\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:54,427] Trial 156 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.0103992935619317 NSE : -11.895265579223633 WAPE : 222.13724124992186 Validation Loss: 0.0027892363723367453\n","Epoch 1: Training Loss: 0.01044797629583627 NSE : -0.030437231063842773 WAPE : 54.68473035792458 Validation Loss: 0.00022288279433269054\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:57,346] Trial 157 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.004521299902989995 NSE : 0.0606960654258728 WAPE : 38.810952450438805 Validation Loss: 0.00020317074086051434\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:56:59,135] Trial 158 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.014003999951455626 NSE : -1.497318983078003 WAPE : 85.79084863771863 Validation Loss: 0.0005401681992225349\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:01,309] Trial 159 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.016016388311982155 NSE : -7.273647308349609 WAPE : 177.0888078213921 Validation Loss: 0.0017895837081596255\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:03,250] Trial 160 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006643034175795037 NSE : -1.4048659801483154 WAPE : 90.22741239498863 Validation Loss: 0.0005201707244850695\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:04,645] Trial 161 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.008616287261247635 NSE : -5.390744209289551 WAPE : 155.0888328365054 Validation Loss: 0.0013823130866512656\n","Epoch 1: Training Loss: 0.00922761220135726 NSE : -0.3818535804748535 WAPE : 66.34681369994372 Validation Loss: 0.00029889389406889677\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:07,375] Trial 162 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.0041413488834223244 NSE : 0.11631858348846436 WAPE : 36.99442996810573 Validation Loss: 0.00019113959569949657\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:08,867] Trial 163 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.012188786058686674 NSE : -10.087056159973145 WAPE : 205.9223447499531 Validation Loss: 0.0023981216363608837\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:10,277] Trial 164 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.011261454979830887 NSE : -2.9143760204315186 WAPE : 113.2964332617623 Validation Loss: 0.0008466765866614878\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:11,749] Trial 165 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.010867137170862406 NSE : -2.011003255844116 WAPE : 103.80499885347399 Validation Loss: 0.0006512777763418853\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:13,401] Trial 166 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006539969206642127 NSE : -5.217856407165527 WAPE : 150.65143524212544 Validation Loss: 0.0013449174584820867\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:15,510] Trial 167 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006299592219875194 NSE : -5.1957831382751465 WAPE : 152.43099372537574 Validation Loss: 0.001340143266133964\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:17,345] Trial 168 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.010898998560151085 NSE : -7.329343795776367 WAPE : 177.6422088345042 Validation Loss: 0.001801630831323564\n","Epoch 1: Training Loss: 0.005775339606771013 NSE : -0.12051355838775635 WAPE : 56.547931041671006 Validation Loss: 0.00024236622266471386\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:20,123] Trial 169 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.0040526887778469245 NSE : -0.6264375448226929 WAPE : 61.63537137020283 Validation Loss: 0.00035179717815481126\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:21,887] Trial 170 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005109699792228639 NSE : -8.282806396484375 WAPE : 186.0367472014342 Validation Loss: 0.0020078641828149557\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:23,348] Trial 171 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.012543924560304731 NSE : -15.221542358398438 WAPE : 249.19597256675908 Validation Loss: 0.003508707508444786\n","Epoch 1: Training Loss: 0.009895201248582453 NSE : 0.1293647289276123 WAPE : 37.60014175230869 Validation Loss: 0.0001883177610579878\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:26,323] Trial 172 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.005030090298532741 NSE : 0.08663803339004517 WAPE : 37.36341122761668 Validation Loss: 0.00019755950779654086\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:28,290] Trial 173 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.010016927895776462 NSE : -1.7090654373168945 WAPE : 88.9195722415626 Validation Loss: 0.0005859687807969749\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:30,475] Trial 174 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.007292513953871094 NSE : -4.870935440063477 WAPE : 145.5945633820433 Validation Loss: 0.0012698790524154902\n","Epoch 1: Training Loss: 0.006137113694421714 NSE : 0.10684645175933838 WAPE : 44.736269829688766 Validation Loss: 0.00019318841805215925\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:33,598] Trial 175 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.004615000376361422 NSE : -4.824778079986572 WAPE : 147.93948010256196 Validation Loss: 0.0012598951580002904\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:35,089] Trial 176 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.012628326861886308 NSE : -2.497624397277832 WAPE : 113.23411227616684 Validation Loss: 0.0007565335254184902\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:36,571] Trial 177 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.024561644822824746 NSE : -0.4742264747619629 WAPE : 70.0850930770674 Validation Loss: 0.0003188739938195795\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:38,026] Trial 178 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005300679325955571 NSE : -1.2743113040924072 WAPE : 89.06707802630756 Validation Loss: 0.0004919317434541881\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:39,444] Trial 179 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.014609623904107139 NSE : -9.412602424621582 WAPE : 199.64680327697982 Validation Loss: 0.002252238104119897\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:40,986] Trial 180 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005638592319883173 NSE : -4.168582916259766 WAPE : 137.96383648454272 Validation Loss: 0.0011179604334756732\n","Epoch 1: Training Loss: 0.006126636793851503 NSE : 0.10420870780944824 WAPE : 44.52458777177878 Validation Loss: 0.00019375898409634829\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:45,117] Trial 181 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.004580292661557905 NSE : -4.692861557006836 WAPE : 145.99650622250942 Validation Loss: 0.001231361529789865\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:46,592] Trial 182 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005788312435470289 NSE : -0.6027055978775024 WAPE : 72.09839277897063 Validation Loss: 0.00034666404826566577\n","Epoch 1: Training Loss: 0.005385077382015879 NSE : 0.12949931621551514 WAPE : 37.21218236017594 Validation Loss: 0.0001882886135717854\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:49,634] Trial 183 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.00468207779340446 NSE : -3.769130229949951 WAPE : 132.8913802088762 Validation Loss: 0.0010315594263374805\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:51,075] Trial 184 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.006213373912032694 NSE : -4.544619083404541 WAPE : 142.03014321152364 Validation Loss: 0.0011992969084531069\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:52,538] Trial 185 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005685043994162697 NSE : -8.82325267791748 WAPE : 192.44788726522273 Validation Loss: 0.0021247619297355413\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:54,089] Trial 186 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.004899182233202737 NSE : -7.684623718261719 WAPE : 179.42513601967855 Validation Loss: 0.0018784775165840983\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:55,911] Trial 187 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.0050014288717648014 NSE : -3.3077635765075684 WAPE : 126.25903149819682 Validation Loss: 0.0009317661169916391\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:58,053] Trial 188 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.007617114199092612 NSE : -1.5883572101593018 WAPE : 93.87763440411916 Validation Loss: 0.0005598596762865782\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:57:59,888] Trial 189 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.015292358992155641 NSE : -1.1203868389129639 WAPE : 85.92087719663964 Validation Loss: 0.0004586380673572421\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:58:01,449] Trial 190 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.007518559257732704 NSE : -3.79929256439209 WAPE : 133.33258010047737 Validation Loss: 0.001038083340972662\n","Epoch 1: Training Loss: 0.0058345456345705315 NSE : 0.03444647789001465 WAPE : 49.175727001730216 Validation Loss: 0.0002088484907289967\n","Epoch 2: Training Loss: 0.004376253848022316 NSE : 0.3116617798805237 WAPE : 35.105984865856456 Validation Loss: 0.00014888706209603697\n","Epoch 3: Training Loss: 0.004590537530020811 NSE : -6.010298252105713 WAPE : 162.90775676971504 Validation Loss: 0.0015163220232352614\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:58:07,549] Trial 191 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Training Loss: 0.0037319922012102325 NSE : -0.28202879428863525 WAPE : 64.239377957516 Validation Loss: 0.0002773018495645374\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:58:09,222] Trial 192 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005593090450929594 NSE : -4.626079559326172 WAPE : 144.38029642909257 Validation Loss: 0.0012169169494882226\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:58:11,740] Trial 193 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.0148591570177814 NSE : -1.6922869682312012 WAPE : 87.88090304559005 Validation Loss: 0.0005823395331390202\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:58:13,661] Trial 194 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.0076462263241410255 NSE : -6.089507102966309 WAPE : 162.61266181651416 Validation Loss: 0.001533455098979175\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:58:15,238] Trial 195 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005968882469460368 NSE : -15.412176132202148 WAPE : 250.18071751683308 Validation Loss: 0.0035499411169439554\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:58:16,770] Trial 196 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.005028720561313094 NSE : -2.0588278770446777 WAPE : 104.56809322298888 Validation Loss: 0.0006616222090087831\n","Epoch 1: Training Loss: 0.006286640862526838 NSE : -0.00426793098449707 WAPE : 52.50828208709428 Validation Loss: 0.00021722240489907563\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:58:19,693] Trial 197 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Training Loss: 0.003982955922765541 NSE : -1.7529146671295166 WAPE : 91.8699881178212 Validation Loss: 0.0005954533698968589\n","Epoch 1: Training Loss: 0.010060397267807275 NSE : 0.2465943694114685 WAPE : 33.419272060203035 Validation Loss: 0.0001629610633244738\n","Epoch 2: Training Loss: 0.0046199790958780795 NSE : -0.08703482151031494 WAPE : 56.679894102687044 Validation Loss: 0.0002351248258491978\n","Epoch 3: Training Loss: 0.003704337985254824 NSE : 0.32004815340042114 WAPE : 40.786917095745345 Validation Loss: 0.00014707306399941444\n","Epoch 4: Training Loss: 0.0033654498256510124 NSE : 0.004305660724639893 WAPE : 56.914181484646974 Validation Loss: 0.00021536795247811824\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:58:28,347] Trial 198 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Training Loss: 0.0028873950359411538 NSE : 0.33600884675979614 WAPE : 45.06213337224573 Validation Loss: 0.0001436207676306367\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-08-07 19:58:29,803] Trial 199 pruned. \n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Training Loss: 0.0053932967957734945 NSE : -1.4594135284423828 WAPE : 93.51231368952075 Validation Loss: 0.0005319693009369075\n","Number of finished trials: 200\n","Best trial:\n","  Value (Best Validation Loss): 1.6306281395372935e-05\n","  Params:\n","    lr: 0.007240183391544309\n","    weight_decay: 1.3647269057168989e-05\n"]}],"source":["def objective(trial):\n","    # define the search space of the hyper-parameters -- optuna uses\n","    # bayesian optimization to find the optimal values of the hyperparameters.\n","    learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n","    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n","\n","    model = BasicLSTMNetwork(seq_len, pred_len)\n","    model = model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(),\n","                                 lr = learning_rate,\n","                                 weight_decay=weight_decay)\n","\n","    # shoot for 50 next\n","    num_epochs = 300\n","\n","    best_val_loss = float('inf')\n","\n","    # keep this between 5-10\n","    patience = 10\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_loss = []\n","        for batch_idx, (inputs, labels) in enumerate(train_loader):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            outputs = model(inputs)\n","            loss_val = loss(outputs, labels)\n","\n","            # calculate gradients for back propagation\n","            loss_val.backward()\n","\n","            # update the weights based on the gradients\n","            optimizer.step()\n","\n","            # reset the gradients, avoid gradient accumulation\n","            optimizer.zero_grad()\n","            epoch_loss.append(loss_val.item())\n","\n","        avg_train_loss = sum(epoch_loss)/len(epoch_loss)\n","        print(f'Epoch {epoch+1}: Training Loss: {avg_train_loss}', end=' ')\n","        avg_val_loss = evaluate_model(model, val_loader)\n","\n","        # check for improvement\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            epochs_no_improve = 0\n","            # save the best model\n","            torch.save(model.state_dict(), 'best_model_trial.pth')\n","        else:\n","            epochs_no_improve += 1\n","            if epochs_no_improve == patience:\n","                print('Early stopping!')\n","                # load the best model before stopping\n","                model.load_state_dict(torch.load('best_model_trial.pth'))\n","                break\n","\n","        # report intermediate objective value\n","        trial.report(best_val_loss, epoch)\n","\n","        # handle pruning based on the intermediate value\n","        if trial.should_prune():\n","            raise optuna.exceptions.TrialPruned()\n","\n","    return best_val_loss\n","\n","study = optuna.create_study(direction='minimize')\n","\n","study.optimize(objective, n_trials = 200)\n","\n","print('Number of finished trials:', len(study.trials))\n","print('Best trial:')\n","trial = study.best_trial\n","\n","print('  Value (Best Validation Loss):', trial.value)\n","print('  Params:')\n","for key, value in trial.params.items():\n","    print(f'    {key}: {value}')\n","\n","\n"]},{"cell_type":"code","execution_count":31,"id":"f7216e93-a35d-4aa5-83db-2e43ea380f6e","metadata":{"id":"f7216e93-a35d-4aa5-83db-2e43ea380f6e","executionInfo":{"status":"ok","timestamp":1754596711851,"user_tz":300,"elapsed":1611,"user":{"displayName":"Alexi Sommerville","userId":"03709470020468717788"}}},"outputs":[],"source":["import optuna.visualization as vis\n","\n","# optimization history\n","fig1 = vis.plot_optimization_history(study)\n","fig1.write_html(\"optimization_history_lstm.html\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}