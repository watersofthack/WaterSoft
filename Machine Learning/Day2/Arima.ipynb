{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd522ffc-f962-4b13-af22-a119e167e259",
   "metadata": {},
   "source": [
    "# ARIMA Modeling\n",
    "\n",
    "Here we explain ARIMA model by discussing its mathematical functions such as **'AR', 'I'** and **'MA'** in its name.\n",
    "\n",
    "## AR (Autoregressive)\n",
    "The autoregressive (AR) component assumes that the current value of series is a linear combination of its previous values. This means the current value is regressed on past values of the same series.\n",
    "\n",
    "### Equation (1)\n",
    "$$\n",
    "y_t = \\alpha_1 y_{t-1} + \\alpha_2 y_{t-2} + \\dots + \\alpha_p y_{t-p} + e_t\n",
    "$$\n",
    "Where:\n",
    "- $ \\alpha_1, \\alpha_2, \\dots, \\alpha_p $ are the AR parameters.\n",
    "- $ e_t $ is the error term.\n",
    "- $ p $ is the lag order, representing how many past values influence the current value.\n",
    "\n",
    "The integrated (I) component refers to differencing the series to make it stationary, i.e., to remove trends or seasonality. Differencing subtracts the previous observation from the current one.\n",
    "\n",
    "### Equation (2)\n",
    "For first-order differencing (d=1):\n",
    "$$\n",
    "y_t' = y_t - y_{t-1}\n",
    "$$\n",
    "For second-order differencing (d=2):\n",
    "$$\n",
    "y_t'' = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}) = y_t - 2y_{t-1} + y_{t-2}\n",
    "$$\n",
    "The integer $ d $ denotes the number of times differencing is applied until the series becomes stationary.\n",
    "\n",
    "## Moving Average (MA)\n",
    "The moving average component models the dependency between the current value and past errors. It assumes that the current value of the series depends on past forecast errors.\n",
    "\n",
    "### Equation (3)\n",
    "$$\n",
    "y_t = e_t + \\beta_1 e_{t-1} + \\beta_2 e_{t-2} + \\dots + \\beta_q e_{t-q}\n",
    "$$\n",
    "Where:\n",
    "- $ \\beta_1, \\beta_2, \\dots, \\beta_q $ are the MA parameters.\n",
    "- $ e_t $ is the error at time $ t $.\n",
    "- $ q $ is the order of the moving average, indicating how many past errors influence the current value.\n",
    "\n",
    "## ARIMA (p,d,q) Model\n",
    "The ARIMA model combines the AR, I, and MA components into a single model.\n",
    "\n",
    "### Equation (4)\n",
    "$$\n",
    "d_t = \\alpha_1 d_{t-1} + \\alpha_2 d_{t-2} + \\dots + \\alpha_p d_{t-p} + e_t + \\beta_1 e_{t-1} + \\dots + \\beta_q e_{t-q}\n",
    "$$\n",
    "Where:\n",
    "- $ d_t $ is the differenced series after applying differencing $ d $ times.\n",
    "- $ \\alpha_1, \\dots, \\alpha_p $ capture the relationship with past values (AR part).\n",
    "- $ \\beta_1, \\dots, \\beta_q $ capture the relationship with past errors (MA part).\n",
    "\n",
    "## Fitting an ARIMA Model\n",
    "To fit an ARIMA model:\n",
    "1. **Determine $ p $, $ d $, and $ q $:** Use ACF (Auto-correlation Function) and PACF (Partial Auto-correlation Function) plots to decide on the AR and MA orders.\n",
    "2. **Estimate parameters:** Use Maximum Likelihood Estimation (MLE) to find the best-fitting parameters ($ \\alpha $ and $ \\beta $).\n",
    "3. **Model validation:** Perform diagnostic checks, such as residual analysis, to ensure the model fits well.\n",
    "4. **Forecast:** Use the fitted model to forecast future values.\n",
    "\n",
    "ARIMA modeling is slow, so we are applying only to monthly downsampled data\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE) basic:**\n",
    "\n",
    "Maximum Likelihood Estimation (MLE) is a method of estimating the parameters of a statistical model by maximizing the likelihood function, which represents the probability of the observed data given the parameters. It doesn’t aim to minimize the error (like in least squares), but rather to maximize the probability of the data. It's done using iterative method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a88f527-08eb-448d-9883-ef8cfc1fe7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f48a1de7-6968-45b6-9ba4-890570392cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_data.csv')\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce023f8b-f2a2-4b6e-a067-f7e4dfd6a7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'DATE', 'Precip', 'WetBulbTemp', 'DryBulbTemp', 'RelHumidity',\n",
       "       'WindSpeed', 'StationPressure', 'gauge_height'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8173b91d-8b4e-4b50-96df-81e79ecb8d21",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "The standard ARIMA model is use for univariate forecasting. i.e. it uses only one column of data as input. \n",
    "We will implement ARIMA to make prediction for future values of gauge height based on past values.\n",
    "\n",
    "We will fit ARIMA model for 1 year of data. ARIMA is a small model and there are few parameters to learn.\n",
    "One year of data is normally a lot of data. Also note that ARIMA can be slow.\n",
    "\n",
    "Select data for year 2023 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e072d8b-3c68-4d73-bdd0-bb09c3afef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "\n",
    "df_23 = df[df.DATE.dt.year == 2023]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065f0b99-c03f-43c7-a20e-07c81f5d7e1c",
   "metadata": {},
   "source": [
    "Next select only the gauge height data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5da92e5-9d4f-411a-a0b3-79f80d7bd8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_23_gh = df_23[['DATE', 'gauge_height']]\n",
    "\n",
    "df_23_gh.set_index('DATE', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c73d316-87ce-4b08-9d8a-3a5710e67cf3",
   "metadata": {},
   "source": [
    "**Split the data such that, 10 months data is used for training and 2 months data is used for testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2b2685d-1f61-4304-ae4b-e3a49ecdf697",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_23_gh[:'2023-11-01 00:00:00'], df_23_gh['2023-11-01 01:00:00':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7dc1d30-4aa1-40d7-aebb-a010e7c931dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ARIMA model\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8c3b7-712b-49ff-b5f2-5b9865b2dbec",
   "metadata": {},
   "source": [
    "Note that p, d and q are the hyperparameters in ARIMA model\n",
    "- p stands for \"sequence length or lookback window or lag\"\n",
    "- q stands for \"number of past errors\"\n",
    "- d stands for differencing\n",
    "\n",
    "Let's train the model with lookback window of 6 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ad0bbaa-32a8-4686-b230-ea4aaa45ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 5 hours because it's computationally expensive and slow to use more. But you can try.\n",
    "\n",
    "ORDER = (5, 1, 5) #lag p=5; difference d=1; error dependency q=5\n",
    "model = ARIMA(train, order=ORDER, freq='1h') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47b1d1cc-9a7b-4b32-bc7f-8c994bb02191",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78950147-c1cf-4bd0-93e7-3b75d823078b",
   "metadata": {},
   "source": [
    "We have fit the ARIMA model, which means we have calculated p(6) parameters that approximate how previous observations affect the next output.\n",
    "Similary, we have calculated q(6) parameters that approximate how previous errors (predictions) affect the next output.\n",
    "\n",
    "So, to make new prediction we need 6 previous observations and 6 previous errors.\n",
    "\n",
    "we trained model for training data, it has a bunch of information it has saved. They can be observed by printing following fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f0fe905-f6f7-4d5c-87ab-3771237da075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data saved in the model\n",
    "\n",
    "# model_fit.resid  - Pandas Series with date as index : Saved residuals\n",
    "# model_fit.model.endog - Numpy array 2D array : Saved training data\n",
    "# model_fit.fittedvalues - Pandas Series with date as index : Fitted (predicted) values from training data\n",
    "# model_fit.arparams - Numpy array : calculated parameters that determine effect of past values \n",
    "# model_fit.maparams - Numpy array : calculated parameters that dtermine effect of past errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f4253-ca04-4123-94be-da3de0bd90b3",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Performing forecasting and evaluating the model and outputs.\n",
    "\n",
    "There are two ways we normally make forecasts in ARIMA.\n",
    "\n",
    "Forecastd values depend on past observed values, past residuals, and model parameters.\n",
    "When model is trained, it forecasts values for training data, calculates residuals based on observations and updates model parameters so as to minimize residuals.\n",
    "For training dataset, we can observe the corresponding training data, fitted values, residuals and computed model parameters in following objects\n",
    "\n",
    "`model_fit` -- Fitted Model\n",
    "\n",
    "    - `model_fit.resid`  - Pandas Series with date as index : Saved residuals\n",
    "    - `model_fit.model.endog` - Numpy array 2D array : Saved training data\n",
    "    - `model_fit.fittedvalues` - Pandas Series with date as index : Fitted (predicted) values from training data\n",
    "    - `model_fit.arparams` - Numpy array : calculated parameters that determine effect of past values \n",
    "    - `model_fit.maparams` - Numpy array : calculated parameters that dtermine effect of past errors\n",
    "\n",
    "### Making predictions using the trained model\n",
    "1. Prediction with refitting\n",
    "\n",
    "In this method, we make forecast for next step using the trained model. Then, when we get the observation for next step, we update the training data with this new observation.\n",
    "And finally we retrain the model to update the parameters and make prediction for next step. \n",
    "\n",
    "\n",
    "2. Rolling foreast\n",
    "\n",
    "In this method, given the test data for next steps, the model makes next prediction, then it dynamically uses the corresponding observaed values from test data, calculates residuals and uses this information to make further predictions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af056e1-85a3-4662-8708-3b0bc1de2708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/20 done\n",
      "1/20 done\n",
      "2/20 done\n",
      "3/20 done\n",
      "4/20 done\n",
      "5/20 done\n",
      "6/20 done\n",
      "7/20 done\n",
      "8/20 done\n",
      "9/20 done\n",
      "10/20 done\n",
      "11/20 done\n",
      "12/20 done\n",
      "13/20 done\n",
      "14/20 done\n",
      "15/20 done\n",
      "16/20 done\n",
      "17/20 done\n",
      "18/20 done\n",
      "19/20 done\n"
     ]
    }
   ],
   "source": [
    "# Method 1\n",
    "predicted_values = []\n",
    "observed_values = []\n",
    "\n",
    "history = train.copy()\n",
    "\n",
    "# This method is slow, we will make prediction for next 20 steps only\n",
    "test_subset = test[:20]\n",
    "\n",
    "model_fit1 = copy.deepcopy(model_fit)\n",
    "\n",
    "for i in range(len(test_subset)):\n",
    "    # make prediction for next timestep\n",
    "    \n",
    "    prediction = model_fit1.forecast(steps=1)[0]\n",
    "\n",
    "    idx = test_subset.index[i]\n",
    "\n",
    "    # observed value at next timestep\n",
    "    observation = test_subset.iloc[i]\n",
    "\n",
    "    # update training data\n",
    "    history.loc[idx] = observation\n",
    "\n",
    "    model = ARIMA(history, order=ORDER, freq='1h')\n",
    "    model_fit1 = model.fit()\n",
    "    \n",
    "    predicted_values.append(prediction)\n",
    "    observed_values.append(observation)\n",
    "    print(f'{i}/{len(test_subset)} done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8779cad1-f4d2-443c-a1fd-bc7f6c38cfae",
   "metadata": {},
   "source": [
    "Next, we will perform rolling forecast. In this approach we won't retrain the model each time but provide last observed <br>value to the model in each step.\n",
    "It updates the previous vaues and residuals but parameters are not updated.\n",
    "\n",
    "`append` method \n",
    "\n",
    "\"Recreate the results object with new data appended to the original data.\n",
    "Creates a new result object <br>applied to a dataset that is created by appending new data to the end of the model’s original data. <br>The new results can then be used for analysis or forecasting.\" **-- from documentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fab832-1811-40c8-8261-a2c32ffc8202",
   "metadata": {},
   "source": [
    "### Method 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a78635e-c6b9-4bcd-8286-5b6bf55fb27c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m predicted_values2 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m observed_values2 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 12\u001b[0m test_subset \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m[:\u001b[38;5;241m20\u001b[39m]\n\u001b[1;32m     14\u001b[0m model_fit2 \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(model_fit)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_subset)):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# make prediction for next timestep\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "# to compare between the results obtained above, we will just compute for 20 steps \n",
    "# rolling evaluation\n",
    "\n",
    "# append method documentation\n",
    "# url: https://www.statsmodels.org/stable/generated/statsmodels.tsa.arima.model.ARIMAResults.append.html\n",
    "\n",
    "\n",
    "# Method 2\n",
    "predicted_values2 = []\n",
    "observed_values2 = []\n",
    "\n",
    "test_subset = test[:20]\n",
    "\n",
    "model_fit2 = copy.deepcopy(model_fit)\n",
    "\n",
    "for i in range(len(test_subset)):\n",
    "    # make prediction for next timestep\n",
    "    \n",
    "    prediction = model_fit2.forecast(steps=1)[0]\n",
    "\n",
    "    idx = test_subset.index[i]\n",
    "\n",
    "    # observed value at next timestep\n",
    "    observation = test_subset.iloc[i]\n",
    "\n",
    "    # saved the observed value to the model\n",
    "    model_fit2 = model_fit2.append([observation], refit=False)\n",
    "    \n",
    "    predicted_values2.append(prediction)\n",
    "    observed_values2.append(observation)\n",
    "\n",
    "\n",
    "\n",
    "#prediction_subset = model_fit.predict(start=test_subset.index[0], end=test_subset.index[-1], dynamic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd33fbf0-9e9e-41f8-911f-573f085f0571",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Let's calculate how the model did by calculating the metrics. We will compute \n",
    "\n",
    "- Mean Absolute Error(MAE): Measures the average of absolute differences between predicted and observed values.\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{t=1}^{n} |y_t - \\hat{y}_t|\n",
    "$$\n",
    "\n",
    "- Nash Sutcliffe Efficiency (NSE) Coefficent: Quantifies the average absolute difference between predicted and observed values.\n",
    "$$\n",
    "\\text{NSE} = 1 - \\frac{\\sum_{t=1}^{n} (y_t - \\hat{y}_t)^2}{\\sum_{t=1}^{n} (y_t - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "- Weighted Percentage Error (WAPE): Measures prediction accuracy in percentage form, weighted by total observed values.\n",
    "$$\n",
    "\\text{WAPE} = \\frac{\\sum |y_t - \\hat{y}_t|}{\\sum |y_t|} \\times 100\\%\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e94859e4-23b9-4733-bf0b-6953ffef484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "epsilon = np.finfo(float).eps\n",
    "\n",
    "def WAPE(y, y_pred):\n",
    "    \"\"\"Weighted Average Percentage Error metric in the interval [0; 100]\"\"\"\n",
    "    nominator = np.sum(np.abs(np.subtract(y, y_pred)))\n",
    "    denominator = np.add(np.sum(np.abs(y)), epsilon)\n",
    "    wape = np.divide(nominator, denominator)*100.0\n",
    "    return wape\n",
    "\n",
    "def NSE(y, y_pred):\n",
    "    return (1-(np.sum((y_pred-y)**2)/np.sum((y-np.mean(y))**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d7d3f-e411-4bed-9dc7-74a7f95bbee7",
   "metadata": {},
   "source": [
    "Following method executes the above functions and print results.\n",
    "\n",
    "Before executing, it's important to make sure that input vectors are 1D numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc08b5c6-7e08-4bf6-9ee8-1d1168f8caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_subset, prediction_subset):\n",
    "    \n",
    "    test_subset = np.array(test_subset).flatten()\n",
    "    prediction_subset = np.array(prediction_subset).flatten()\n",
    "    \n",
    "    mae = mean_absolute_error(test_subset, prediction_subset)\n",
    "    nse = NSE(test_subset, prediction_subset)\n",
    "    wape = WAPE(test_subset, prediction_subset)\n",
    "    \n",
    "    print(f'MAE: {mae:.2f}\\nNSE: {nse}\\nWAPE: {wape}')\n",
    "    return (nse, mae, wape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2409844c-3d7f-4bd2-968d-a88fc817e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rolling evaluation\")\n",
    "evaluate(observed_values2, predicted_values2)\n",
    "\n",
    "print(\"\\nRetraining each time\")\n",
    "evaluate(observed_values, predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c31906-0cf0-474b-9f24-19ef8ee54bb8",
   "metadata": {},
   "source": [
    "**Here we evaluate the model for all of the test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb97cb-8304-4ac1-911a-fdf0a97de6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(test, model):\n",
    "    predicted_values = []\n",
    "    \n",
    "    for i in range(len(test)):\n",
    "        # make prediction for next timestep\n",
    "        \n",
    "        prediction = model.forecast(steps=1)[0]\n",
    "    \n",
    "        idx = test.index[i]\n",
    "    \n",
    "        # observed value at next timestep\n",
    "        observation = test.iloc[i]\n",
    "    \n",
    "        model = model.append([observation], refit=False)\n",
    "        \n",
    "        predicted_values.append(prediction)\n",
    "        if(i % 100 == 0):\n",
    "            print(f'{i}/{len(test)} done')\n",
    "        \n",
    "    return predicted_values\n",
    "\n",
    "\n",
    "predictions = make_predictions(test, copy.deepcopy(model_fit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5ad02-f4a2-417b-b664-38054ca47495",
   "metadata": {},
   "outputs": [],
   "source": [
    "nse, mae, wape = evaluate(test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84dbc5c-84a3-43ad-8444-ee70ef76a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(test))\n",
    "\n",
    "# add evaluation results in the graph itself\n",
    "\n",
    "plt.plot(np.array(test[:100]), color='green', label='observations')\n",
    "plt.plot(np.array(predictions[:100]), color='red', linestyle='-', label='predictions')\n",
    "metrics_text = f\"NSE: {nse:.3f}\\nMAE: {mae:.3f}\\nWAPE: {wape:.3f}\" \n",
    "plt.text(0.28, 0.97, metrics_text, transform=plt.gca().transAxes, \n",
    "         fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c3fa3-7d3f-4a94-822b-5915ac161d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
